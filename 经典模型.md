
# 图像

一些可供使用的库：

1. [huggingface/pytorch-image-models: The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more](https://github.com/huggingface/pytorch-image-models)



## Resnet

Resnet 的基本模块为 Res Block，基本结构如下（右图为加入输入输出尺寸不同时的结构）

![](https://img-blog.csdnimg.cn/20210515235414917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NhemFzcw==,size_16,color_FFFFFF,t_70)

输入尺寸为 224 × 224。

## Res2net

将特征图（$B\times C \times H\times W$）先经过一个 1 x 1 卷积+ BN + Relu，之后在通道维度分割为 s 个特征子图（$B\times (C/S) \times H\times W$），对于 s 个通道子图进行如下处理
$$
{y_i} = \left\{ \matrix{
  {x_i}\quad i = 1 \hfill \cr 
  {K_i}({x_i})\quad i = 2 \hfill \cr 
  {K_i}({x_i} + {y_{i - 1}})\quad 2 < i \le s \hfill \cr}  \right.
$$
这里的 $K_i$​ 可以是简单的 3 × 3 卷积 + BN + Relu，也可以是分组卷积（减少计算量），最后经过一个 1 × 1 的卷积 + BN，再经过一个可选的 SE block 与输入相加连接，最后再经过一个 Relu。

## ResNeXt

将 `[256, 3×3, 256]` 卷积改为 `[256, 1×1, 128]+[128, 3×3, 128, groups=32]+[128, 1×1, 256]`

下面三种网络架构等效

![](https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/ResNeXt.jpg)



## SENet

用于图像分类，在 Resnet 的基础上加入了 SE Block，SE Block 的架构如下：

<img src="https://i-blog.csdnimg.cn/blog_migrate/16d6a483fe3f04bbc9a01adc6151804d.png" style="zoom:67%;" />

注意此处的 FC 可以是二维卷积，一维卷积等。SENet 将 SE Block 加在每个小卷积块之后（整个模块可能有几十个SE Block，r=16），一些更加简单的模型将 SE Block 加在模块之后（可能整个模型中只有3、4个 SE Block），控制好r的大小，对于模型的计算量影响很小。

## VIT

将二维图片 $x\in R^{H\times W\times C}$ 重整为 $x_p\in R^{N\times(P^2\cdot C)}$，P 为一个图片块的长宽，$N=HW/P^2$​。

输入 $x\in R^{B\times C \times H \times W}$ 经过一个二维卷积 `[3, 768]`（`kernel_size` 和 `stride_size` 相等，均为 16），对于一个 3 × 224 × 224 的图片，可以得到 768 × 14 × 14 的特征，然后摊平最后两个维度得到 196 × 768 的特征，196 相当于文本中的token长度，768 为token的特征维度。

然后进行位置编码，可以设置一个参数进行学习

```python
self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)
```

然后将分类或者回归token（可选）连接到第一个维度，即token长度上

```python
self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
self.reg_token = nn.Parameter(torch.zeros(1, reg_tokens, embed_dim))
```

之后便是若干个 Block，Block 中包含了常规的多头自注意力和前馈层。

$$
\eqalign{
  & {z_0} = \left[ {{x_{class}};x_p^1E;x_p^2E; \cdots ;x_p^NE} \right] + {E_{pos}}\quad E \in {R^{({P^2} \cdot C) \times D}},{E_p} \in {R^{\left( {N + 1} \right) \times D}}  \cr 
  & {z_l}' = MSA(LN({z_{l - 1}})) + {z_{l - 1}}\quad l = 1...L  \cr 
  & {z_l} = MLP(LN({z_l}')) + {z_l}'\quad l = 1...L  \cr 
  & y = LN(z_L^0) \cr} 
$$

计算多头自注意力（MSA）时，代码如下

```python
class Attention(nn.Module):  
    def __init__(  
            self,  
            dim: int,  
            num_heads: int = 8,  
            qkv_bias: bool = False,  
            qk_norm: bool = False,  
            proj_bias: bool = True,  
            attn_drop: float = 0.,  
            proj_drop: float = 0.,  
            norm_layer: nn.Module = nn.LayerNorm,  
    ) -> None:  
        super().__init__()  
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'  
        self.num_heads = num_heads  
        self.head_dim = dim // num_heads  
        self.scale = self.head_dim ** -0.5  
  
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()  
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()  
        self.attn_drop = nn.Dropout(attn_drop)  
        self.proj = nn.Linear(dim, dim, bias=proj_bias)  
        self.proj_drop = nn.Dropout(proj_drop)  
  
    def forward(self, x: torch.Tensor) -> torch.Tensor:  
        B, N, C = x.shape  
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  
        q, k, v = qkv.unbind(0)  
        q, k = self.q_norm(q), self.k_norm(k)  
  
		x = F.scaled_dot_product_attention(  
			q, k, v, dropout_p=self.attn_drop.p if self.training else 0.,  
		)  
		
        x = x.transpose(1, 2).reshape(B, N, C)  
        x = self.proj(x)  
        x = self.proj_drop(x)  
        return x
```

`MLP` 为两个全连接层，中间加一个激活函数，全连接层的中间维度为 4 倍输入维度。

在经过若干个 Block 提取特征后，经过 norm 和其它层进行分类等操作。


## Swin Transformer

视觉模态的信息与文本信息的差距体现在视觉信息会存在快速变化，此外图像的分辨率更高。VIT 中计算自注意力时对于整张图进行计算，Swin Transformer 在局部窗口中计算自注意力，假设每个窗口包含了 $M\times M$个图像块，窗口之间不重叠。为了加入窗口间的连接，引入了 Shifted Window 多头自注意力。Shifted window 在计算时

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220211104213844-333037308.png)

Shifted Window MSA 计算时先对图片进行不均匀的划分，再进行类似围绕中心旋转的操作，再进行均匀的分割，对每个小块计算MSA，最后再倒回来。

输入 $x\in R^{B\times C \times H \times W}$ 经过一个二维卷积 `[3, 96]`（`kernel_size` 和 `stride_size` 相等，均为 4），对于一个 3 × 224 × 224 的图片，可以得到 96 × 56 × 56 的特征，与 VIT 不同，Swin Transformer不会摊平最后两个维度。然后经过若干个 Block，单个 Block 中包含了若干个 Swin Transformer Block。Swin Transformer Block 与 VIT 中的 Block 类似，但是注意力计算方式不同。对于单个样本 $x_i\in R^{H'\times W'\times C’}$ 分割成多个patch，单个patch 的大小为 `7 × 7`，对于 `56 × 56` 大小的图像，可以分割成 `8 × 8` 个 patch（放在批次这个维度上），$x_i$ 转为 $R^{64B \times 49 \times 96}$ ，这时就可以参考 VIT 中的注意力进行计算，计算完注意力后，将特征图重新翻转过来。

上面是普通的 MSA 计算方式，每一个普通的 MSA 之后的 Block 中的注意力为 Shifted Window MSA，即在计算注意力前先进行 shift

```python
shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))
```

计算完注意力之后再 Shift 回来

```python
x = torch.roll(shifted_x, shifts=self.shift_size, dims=(1, 2))
```

在输入 MLP 之前，将中间两个维度展平，变成三维，经过 MLP 后再 reshape 回来。

每隔几个 Block 需要融合图像块，先将 $x\in R^{N\times H\times W \times C}$ 转为 $x\in R^{N\times (H/2) \times 2\times(W/2)\times 2\times C}$，再转为$x\in R^{N\times (H/2) \times(W/2)\times 4C}$，最后经过一个全连接层，输入为 $4\times C$，输出为 $C$ 得到融合后的特征图。

>相比于 VIT，Swin Transformer 的适用范围更广。


## ConvNeXt

ConvNeXt 改进了Block的设计

![](https://img-blog.csdnimg.cn/821b2d2d77a1471f9797b41da3dd29e8.png)


LayerScale 的代码如下

```python
class LayerScale(nn.Module):  
    def __init__(  
            self,  
            dim: int,  
            init_values: float = 1e-5,  
            inplace: bool = False,  
    ) -> None:  
        super().__init__()  
        self.inplace = inplace  
        self.gamma = nn.Parameter(init_values * torch.ones(dim))  
  
    def forward(self, x: torch.Tensor) -> torch.Tensor:  
        return x.mul_(self.gamma) if self.inplace else x * self.gamma
```

GRN 的代码如下

```python
class GlobalResponseNorm(nn.Module):  
    """ Global Response Normalization layer  
    """    def __init__(self, dim, eps=1e-6, channels_last=True):  
        super().__init__()  
        self.eps = eps  
        if channels_last:  
            self.spatial_dim = (1, 2)  
            self.channel_dim = -1  
            self.wb_shape = (1, 1, 1, -1)  
        else:  
            self.spatial_dim = (2, 3)  
            self.channel_dim = 1  
            self.wb_shape = (1, -1, 1, 1)  
  
        self.weight = nn.Parameter(torch.zeros(dim))  
        self.bias = nn.Parameter(torch.zeros(dim))  
  
    def forward(self, x):  
        x_g = x.norm(p=2, dim=self.spatial_dim, keepdim=True)  
        x_n = x_g / (x_g.mean(dim=self.channel_dim, keepdim=True) + self.eps)  
        return x + torch.addcmul(self.bias.view(self.wb_shape), self.weight.view(self.wb_shape), x * x_n)
```




## CRNN

[meijieru/crnn.pytorch: Convolutional recurrent network in pytorch](https://github.com/meijieru/crnn.pytorch)

用于OCR

CRNN 由两部分网络组成，首先是CNN，，CNN 总共有 7 层（记为 0 - 6），每一层由 Conv2d +Relu + BatchNorm（BatchNorm只在2、4，6三层有），0-1，1-2，3-4，5-6之间有 MaxPool2d。通过 Cnn 将高转为1。第二部分为两个双向LSTM层，每个双向LSTM层包括一个双向LSTM和Linear。


## EdgeNeXt



![](https://img-blog.csdnimg.cn/cc19caf14948420db3e60e237ec3de60.png)
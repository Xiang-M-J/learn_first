
## 语音活动检测（VAD）

语音活动检测是检测一段声音中是否存在语音，并给出语音的起点和终点。传统的 VAD 包括基于能量（计算一小段音频的能量，如果能量超过了界限，就认为存在语音），基于跨零率（一般认为噪声的跨零率较高），还有基于频谱特征（一般认为语音的频谱特征更加规范），对语音计算频谱，然后对于给定的帧计算各个频点的熵

$$
H = -\sum_{i} P(f_i)\log (P(f_i))
$$
$P(f_i)$ 是频点 $f_i$ 的归一化能量。低熵代表语音，高熵代表噪声。

还有基于频谱变动，即计算

$$
F(t) = \sum_{f}(|X(f,t)|-|X(f,t-1)|)^2
$$
$X(f,t)$ 表示时间 $t$ 和频率 $f$ 时的频谱幅度。高的 $F(t)$ 表示噪声，低的 $F(t)$ 表示语音。

下面介绍两种深度学习算法 silero-vad 和 fsmn-vad

### silero-vad

对于一段音频，将其按照512（采样率为16000Hz）和 256（采样率为 8000Hz）的帧长分成若干帧（不重叠），使用模型预测这512个点的语音存在概率 $p_i$，将所有段的语音概率存在一个列表 $L$ 中。基于所有段的语音存在概率预测语音的起点和终点，预测算法分为端到端（较为准确）和实时（只根据一帧和之前的历史状态判断）。

#### 算法流程


令语音存在概率列表 $L = \{p_1, p_2,\cdots, p_n\}$，默认情况下判断语音存在的依据是 $p_i\ge 0.5$，当 $p_i<0.35$ 时认为语音不存在，这里为了避免将小的间隔识别为静音段，会在 $p_i <0.35$ 暂时保存当前时间（采样点），只有当间隔大于100ms时才会保存这段语音。（真实的更加复杂）。

默认情况对于保存的语音会判断这些片段是否小于250ms，并且对于语音段两端进行补零（默认30ms）


#### 训练过程

训练的损失函数为 BCELoss，训练时，将每个样本分割成若干个段（512个点），对每个段计算概率（0-1之间），然后将所有段的概率与标签计算BCE损失。



### fsmn-vad

fsmn-vad 需要先提取音频的fbank特征，并且应用 `lfr_cmvn` 进一步处理。fsmn-vad 同样支持端到端和实时处理。

#### 算法流程

fsmn-vad 的输入为音频的特征和波形，波形用于计算每一帧的分贝（注意分帧时有重叠），音频特征则送入模型计算每一帧的分数（语音概率）。后面通过状态机来检测帧（比较复杂）



## 语音识别


### CTC Loss

[Sequence Modeling with CTC](https://distill.pub/2017/ctc/)

CTC loss 用于对齐网络输出和目标值，便于计算损失。除了语音识别，CTC 损失还能应用于手写文字识别等。

对于给定的输入 $X$，训练模型最大化接近正确答案 $Y$ 的概率，因此需要计算可微分的条件概率 $p(Y|X)$。训练好模型后，推理的目标为

$$
{Y^*} = \mathop {\arg \max }\limits_Y p(Y|X)
$$

CTC 引入特殊的空白token $\varepsilon$ 用来分割不同的token，便于处理重复的token，两个 $\varepsilon$ 之间的所有重复token都会被去除。 

![](https://distill.pub/2017/ctc/assets/ctc_alignment_steps.svg)

模型的输出为 $N \times C$，N 为tokens长度，C 为可能的tokens类别数，

$$
p(Y|X) = \sum\limits_{A \in {A_{X,Y}}} {\prod\limits_{t = 1}^T {{p_t}({a_t}|X)} } 
$$
$A_{X,Y}$ 为所有可能的对齐路径，计算这些路径的概率。这些路径的数量可能太大以至于无法计算，因此需要一定的策略，如动态规划。ctc loss的目标是最小化

$$
\sum\limits_{\left( {X,Y} \right) \in D} { - \log p(Y|X)} 
$$

pytorch 中有 ctc loss 的实现


### Whisper

Whisper 训练数据为多任务学习数据，包括英语识别数据，其它语言到英语的语言翻译数据和非英语识别数据等，语音对应的文本来自人类或者其它ASR模型，其中部分音频中不包含语音（作为VAD的训练数据）。

将音频文件分成 30 s 的片段（长的截断，短的补零），并且重采样为16000Hz，计算80通道的对数梅尔谱图（hann窗，窗口大小为25ms，步长为10ms），将输入变换为-1到1之间，零均值。

对文本进行清洗（英文文本替换缩略词，将一些数字和特殊符号转写为文本，替换口语和方言，去掉括号中的文本）。每个token对应 320 个采样点（两倍步长，两倍是因为模型输入时有一个步长为2的卷积）。whisper 使用 Byte Pair Encoder （BPE）的方法进行分词，可以处理多种语言，实现时使用了 tiktoken 库。先将文本拆分成一个个字节，然后通过动态地合并频繁出现的字符对（byte pair），形成新的“子词”单元，从而生成一个可用于文本处理的更小的词汇表，自动地学习和切分语音中的音节和子词。token中除了常规的文本，还有一些特殊的token，包括 `<|startoftranscript|>`、`<|translate|>`、`<|nospeech|>`等标记功能，`<|en|>` 等用来标记语言，还有用来标记时间戳的（总共30s，每隔20ms有一个时间戳，总共1501个时间戳token），这些特殊token对应的值直接依次递增即可。

模型为编码器-解码器结构，音频的对数梅尔谱图作为编码器的输入，文字的tokens作为解码器的输入。音频编码器包括两个一维卷积，激活函数为gelu，若干个Transformer层（仅有自注意力），最后为layernorm，在输入 Transformer 之前需要先进行位置编码，位置编码的过程如下

```python
def sinusoids(length, channels, max_timescale=10000):  
    """Returns sinusoids for positional embedding"""  
    assert channels % 2 == 0  
    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)  
    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))  # [c // 2]  
    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]  # [l, c//2]  
    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)  # [l, c]
```


文本解码器包括一个token的embedding，一个需要学习的位置编码，若干个Transformer层（自注意力和跨注意力），文本解码器的输入包括token以及音频编码器的输出（作为计算注意力时的 k 和 v）。经过若干个Transformer层后再经过一个LayerNorm层乘上embedding的权重得到输出的$logits\in R^{B\times T\times C}$，T 为输入的token长度，C 为词表长度。


在推理时，如果不确定输入的语音语言，会在末尾补30s的零，然后取前30s用于检测语言，提取前30s的对数梅尔谱图，token为单个特殊token `<|startoftranscript|>`，输出为 $logits\in R^{B\times 1\times C}$，然后对非语言token进行遮蔽，然后做softmax，得到检测的语言。

得到检测的语言后就不需要补零的数据了，不过由于模型的输入要求30s，因此如果音频不满30s，还需要补零到30s。语音转写一开始的token为 `[50258, 50259, 50359]`，50258表示 `<|startoftranscript|>`，50259 表示语言，50359表示 `<|transcribe|>`，如果是语音翻译，那么第三个位置就是 50358 表示 `<|translate|>`。将对数梅尔谱送入编码器得到特征，特征和tokens送入解码器得到预测的logits，解码是一个循环的过程（循环有一个最大次数，并且通过判断最后一个token是否为 `<|endoftranscript|>` 来决定是否停止），每次循环时找到概率最大的token（贪心算法），将这个token加入tokens。最后还需要加入一个特殊token `<|endoftranscript|>`。token还需要进行一些额外处理，然后再解码就得到了转写后的文本，此外还有添加时间戳和标点符号的内容。

添加时间戳实现方式为：设置decoder的输入tokens为 `[50258, 50259, 50359, 50363, decode_text_tokens, 50257]`，其中 50363 表示tokens中不包含时间戳，50257 表示 `<|endoftranscript|>` ，然后通过预先设置的 hook 获得跨注意力层的注意力，这里的注意力大小为 `[L, T]`，L 为 tokens 长度，而 T 则为时间长度，由于可能存在补零，需要将补零帧对应的权重去除，同时只保留文字对应的tokens的权重，最终得到文字和时间的注意力权重矩阵，对该矩阵进行动态时间规整找到token对应的帧，即时间戳。




### Distil-Whisper

假设数据集中样本为 $(X_{1:T}, y_{1:N})$ 是音频—文本对，使用标准的交叉熵损失训练，

$$
{L_{CE}} =  - \sum\limits_{i = 1}^N {P({y_i}|{y_{ < i}},{H_{1:M}})} 
$$

知识蒸馏（KD）是一项模型压缩技术来训练一个学生模型来尽可能接近教师模型的输出，KD让模型可以学习给定情况下下一个可能的token的概率分布。

使用教师模型的参数初始化学生模型的参数，如从教师模型的第一层和最后一层复制到学生模型。

使用教师模型的输出 $\hat y_{1:N}$ 替代目标值 $y_{1:N}$，伪标签损失为

$$
{L_{PL}} =  - \sum\limits_{i = 1}^N {P({y_i}|{{\hat y}_{ < i}},{H_{1:M}})} 
$$
除此之外，需要缩小学生模型的概率分布 $P_i$ 和教师模型的概率分布 $Q_i$ 之间的 KL 损失

$$
{L_{KL}} = \sum\limits_{i = 1}^N {KL({Q_i},{P_i})} 
$$

为了只在准确的样本上训练，需要过滤掉一些数据，即去除一些WER过高的样本。

训练集包括了 9 个公共数据集，学生模型的音频编码器直接使用教师模型，并且在训练时固定参数，文字解码器只使用教师模型中的第一层和最后一层，其余层全部丢弃。

**训练细节**

先给数据标上伪标签，将短的音频连接起来变成30s，使用预训练的whisper模型对音频进行标记，标记时设置语言和返回时间戳。

然后初始化学生模型，使用教师模型的全部编码器和解码器的第一层和最后一层。




## 语音合成


文字到音频的映射通常通过训练一个神经网络来实现

**文本编码**：首先将输入的文本转换为一个向量表示。通常，文本会经过词嵌入（Word Embedding）或者字符级别的编码，将每个词或字符转换为一个固定长度的向量。

**声学模型**：声学模型负责根据文本的输入生成语音特征（如梅尔频率倒谱系数MFCC或梅尔谱图）。传统方法使用像隐马尔可夫模型（HMM）这样的算法，但深度学习方法通常使用卷积神经网络（CNN）、循环神经网络（RNN）或者变换器（Transformer）来生成这些特征。

**解码生成音频**：将声学模型生成的音频特征传递给声码器（Vocoder）。声码器是一个将音频特征转换成实际波形的模块。常见的声码器包括WaveNet、WaveGlow、HiFi-GAN等。

### Tacotron2

每次输入数据时需要对不等长的数据进行补零， 模型的输入为`补零后的文本`，`输入的长度`，`补零后的梅尔谱图`，`补零后的门限（1表示补零）`和 `输出长度（未补零时的梅尔谱图长度）`。


先将文字进行embedding，然后输入编码器，编码器为若干个一维卷积，最后为一个双向LSTM。编码器之后是解码器。将编码器输出 $e\in R^{B \times C \times T}$ 转为 $e\in R^{T \times B \times C}$ ，将 $e$ 与全零帧连接得到解码器的输入。在解码时，对于每个时间步的输入进行解码，解码时会得到预测的梅尔谱图和预测的门限（门限用来标记什么时候结束），Tacotron2 的损失是梅尔谱图的MSE损失和预测门限的分类损失。


### FastSpeech2

训练时，输入除了文字，还有目标语音的音频时间，音高和能量信息。推理时，使用模型对于音频时间、音高等信息的预测值。

FastSpeech2 首先对音素进行embedding，然后添加位置编码，经过一个encoder 提取信息，再经过一个Variance Adaptor，这个Adaptor用来添加时长、音高和能量等信息，注意这里添加的都是预测器（卷积网络）预测的信息，这些预测器则通过MSE损失进行优化。之后经过解码器得到梅尔谱图或者波形。

音素时长通过 Montreal forced alignment （MFA） 工具提取；音高提取时先使用连续小波变换来分解连续音高序列为音高谱，音高谱为训练的目标；能量则计算短时傅里叶变换，将每一帧的幅度作为能量，并将其量化为256个可能的值，编码为能量 embedding。


### Parler-TTS

提出了一种高效标记数据的方法，完全使用自动标注的标签，这样可以使用大规模的数据进行训练。

输入分为原始文本和描述文本，原始文本编码为token，描述文本经过预训练的 T5 模型提取特征，token 输入Transformer 架构的Decoder，描述文本特征作为跨注意力输入，Decoder 后面是RVQ Decoder 得到语音。

描述文本包括说话人性别、重音、录制品质、音高和说话速度等。




## 回声消除（AEC）

回声指声音信号经过一系列反射之后，又听到了自己说话的声音。常见的场景：远端讲话者的声音被远端麦克风采集并传入通信设备，经过通信传输之后达到近端的通信设备，并通过近端扬声器播放，这个声音又会被近端麦克风采集形成声学回声，经传输又返回到远端的通信设备，并通过远端扬声器播放出来，从而远端讲话者就听到了自己的回声。

回声消除的目标是消除近端说话时远端的声音，不要将远端的声音再传回远端。

一个完整的回声消除系统，包括以下几个模块：

1. **时延估计（Time Delay Estimation, TDE）** 模块
2. **线性回声消除（Linear Acoustic  Echo Cancellation, AEC）** 模块
3. **双讲检测（Double-Talk Detect, DTD）** 模块
4. **非线性残余声学回声抑制（Residual Acoustic Echo Suppression, RAES）** 模块，也常称为非线性处理技术(Nonlinear Processing, NLP)

消除时使用自适应滤波器，滤波器有两种状态

滤波：$\hat y(n)=x(n)*\hat w(n), e(n) = d(n)-\hat y(n)$

自适应滤波器系数更新（NLMS）：$\hat w(n+1)=\hat w(n)+\mu e(n)\frac{x(n)}{x^T(n)x(n)}$

三种工作模式（通过DTD双讲检测）

- **远端语音存在，近端语音不存在(单讲)**：滤波、自适应滤波器系数更新
- **远端语音存在，近端语音存在(双讲)**：滤波，滤波器系数不更新
- **远端语音不存在**：什么都不用做







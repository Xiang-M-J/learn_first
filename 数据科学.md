
## 小知识

1. numpy 的数据按行存储，pandas 的数据按列存储，pandas的按行读取 iloc 比按列读取慢很多，而numpy按列和按行读取都很快
2. parquet 一种按列存储的二进制数据格式，可以使用 pandas 的 `to_parquet` 转换

3. NoSQL（Not only SQL）是一种非关联型数据库，主要包括文档模型和图模型。文档模型的目标用例一般数据来自自包含的文档，并且文档之间的联系很少。图模型则相反，目标是数据项之间的关系常见且重要的用例。


## 训练数据

### 采样数据

水塘采样：从流数据中采样 k 个数据，具体方法先将前 k 个数据放在水塘（采样的数据）中，为对于第 n 个到来的数据 `x`，随机生成一个 `[1, n]` 的数，如果这个数 `i` 位于 `[1, k]` 之间，那么就将水塘中第 `i` 个数替换为 `x`。


### 标记数据

当给数据做标记时，需要注意跟踪数据的来源，防止某一批标记不准确的数据干扰整体数据集。

下面介绍在没有高质量标签的情况下的几种技术：

**弱监督**：Snorkel 是一种基于标记函数（labeling function，LF）的开源工具，可以为无标注数据添加低质量的标注，LF 通过数据中的一些关键信息进行标注，每个数据样本都会被多个LF进行标注，然后结合这些LF的标注来获得最可能的标签。为了观察LF的效果，推荐包含一部分人工标注的数据。如果LF可以完美标注数据，那么还需要机器学习模型吗，答案是需要，因为需要模型泛化到其它没有被LF覆盖到的数据。

**半监督**：半监督基于一小部分有标签数据来生成新的标签。一种经典的半监督方法是自训练，先在一小部分数据上训练模型，然后用这个模型来在未标注的数据预测，将最大概率的数据加入训练中，这样重复操作直到模型达到满意的结果。另一种方法是假设类似结构的数据有着类似的标签，可以使用kmeans聚类来对样本进行聚类。还有一种流行的方法是对比学习。

**迁移学习**

**主动学习**：不是随机的标记样本，而是选择性的标记对模型最有帮助的样本，最直接的方法是标记模型最不确定的样本。先随机标记一些样本，训练模型，在未标记的样本进行测试，标记那些模型最不确定的样本。还有一种方法是训练一组模型，每个模型都对样本进行一次标记，然后人工标记那些争议最大的样本。主动学习可以用在实时数据上。



### 类别不平衡


下面介绍三种解决类别不平衡的方法：

**使用合适的评价标准**：如为每个类都计算一次准确率，使用召回率等


**重采样**：从数量较小的类别中采样更多的数据，从数量较多的类别中采样更少的数据。但是这种方法容易欠拟合数量较多的类别，而过拟合数量较少的类别。一种方法是，先在重采样的数据上（对于数量较多的类别，只选取一部分样本）进行训练，再在原始数据上微调。还有一种动态采样方法，[Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/8396983)

**修改算法（损失函数）**：1. 设置一个cost矩阵 $C_{ij}$ 来衡量第 i 个类被判成第 j 个类的损失加权值，其中 $C_{ii} = 0$。2. 类别平衡损失，为每个类设置一个权重 $W_i$，$W_i$ 为样本总数 / 类别样本数，3. focal loss，即将原本的交叉熵损失 $L=-\log(p_t)$ 改为 $L=-(1-p_t)^{\gamma}\log(p_t)$。


**数据增强**：


## 特征工程



### 常用特征工程操作



**缺失值处理**


**特征归一化**：例如将每个特征都进行归一化到 `[0, 1]` 区间，或者归一化到 `[a, b]` 区间

$$
\bar x = a + {{\left( {x - \min (x)} \right)(b - a)} \over {\max (x) - \min (x)}}
$$

**离散化**：将连续值量化，例如将连续值分成一个个小区间。

**编码类别特征**：为每个类别生成一个哈希值（Vowpal Wabbit），但是可能会遇到两个类别用着相同的哈希值

**特征交叉**：将两个和多个特征结合生成新的特征

**离散和连续位置嵌入**：
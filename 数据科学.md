

# 理论学习


## 小知识

1. numpy 的数据按行存储，pandas 的数据按列存储，pandas的按行读取 iloc 比按列读取慢很多，而numpy按列和按行读取都很快
2. parquet 一种按列存储的二进制数据格式，可以使用 pandas 的 `to_parquet` 转换

3. NoSQL（Not only SQL）是一种非关联型数据库，主要包括文档模型和图模型。文档模型的目标用例一般数据来自自包含的文档，并且文档之间的联系很少。图模型则相反，目标是数据项之间的关系常见且重要的用例。
4. 32bit表示一个float数（单精度），16bit表示一个float（半精度），8bit表示一个int（定点）
5. 在浏览器上运行模型可以选择 WASM，但是相比于原生代码还是比较慢



## 训练数据


### 常用数据操作（pandas）

#### 行（列）索引

默认情况pandas为每一行分配一个整数索引，可以通过 `set_index` 来设置行索引

```python
# 将 'Name' 列设置为行索引 
data.set_index('Name', inplace=True)
```

列可以直接通过列名索引

```python
df['column_A']
```

行可以通过 `loc` 或者 `iloc`

```python
df.loc[1]   # 通过行名索引，只不过行名默认为整数
df.iloc[1]  # iloc 通过整数索引
```

行和列结合索引

```python
df.loc[0, 'column_A']
df.iloc[0, 0]
```


#### 某一列补零缺失值

```python
# 字符串类型
df['holiday_name'] = df['holiday_name'].fillna("Unknown")

# 数值类型
median_total_orders = df['total_orders'].median()
df['total_orders'] = df['total_orders'].fillna(median_total_orders)
```

#### 删除列（行）

如果某一列只在训练集中出现而不在测试集中出现，那么就要删除

```python
train = train.drop(labels='availability', axis=1)
```

**根据索引删除行**

```python
df = df.drop(index=1) # 删除索引为 1 的行
df.drop(index=1, inplace=True)   # 直接修改原始对象
```

**根据条件删除行**

```python
# 删除列 'A' 值为 2 的行 
df = df[df['A'] != 2]
```


> [!NOTE] Title
> 在删除行之后，可以使用 reset_index 来重新排列索引


#### 添加列（行）

**添加列**

```python
data.insert(data.shape[1], 'new_column', 0)
data['new_column'] = 0
```

**添加一行**

```python
df.loc[len(df)] = [1,2,3,4]
```

**添加多行（数据表）**

```python
df = pd.concat([df, ndf])
```

#### 删除部分缺失行

一般是目标值缺失

```python
train = train.dropna(subset=['sales'])
```


#### 修改行或列索引

```python
pred = pred.set_axis(new_axes, axis="index")  # 行索引
pred = pred.set_axis(new_axes, axis="columns")  # 列索引
```

#### 数据集合并

使用 `pandas.merge` 合并两个存在关联的数据集，on 表示按照这些列合并，how 表示

```python
train = pd.read_csv(os.path.join(base_path, "sales_train.csv"))  

inventory = pd.read_csv(os.path.join(base_path, "inventory.csv"))  
calender = pd.read_csv(os.path.join(base_path, "calendar.csv"))  

train = train.merge(inventory, how='left', on=['warehouse', 'unique_id'])  
train = train.merge(calender, how='left', on=['warehouse', 'date'])
```


#### 统计某一列的不重复元素

```python
test_ids = test['unique_id'].unique()
```

#### 统计某一列元素出现次数

```python
values = train['unique_id'].value_counts().to_dict()
```

`to_dict` 可以获得元素和其出现次数的字典。

#### 按照某一列排序

```python
train['date'] = pd.to_datetime(train['date'])
train.sort_values('date', ascending=True, inplace=True)
```


#### 日期操作

[Time series / date functionality — pandas 2.2.3 documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview)

**字符串转为日期**

```python
date = pd.to_datetime("2024-06-03")
```

**生成一定范围的日期**

```python
pd.date_range("2018-01-01", periods=3, freq="d")  # 以天为间隔生成
```


**日期加减**

可以使用 `pd.offsets` 或者 `pd.Timedelta` 来表示偏移

```python
date + 2* pd.offsets.Day()   # 加两天
date - pd.Timedelta(weeks=4)   # 减四周
```


### 采样数据

水塘采样：从流数据中采样 k 个数据，具体方法先将前 k 个数据放在水塘（采样的数据）中，为对于第 n 个到来的数据 `x`，随机生成一个 `[1, n]` 的数，如果这个数 `i` 位于 `[1, k]` 之间，那么就将水塘中第 `i` 个数替换为 `x`。


### 标记数据

当给数据做标记时，需要注意跟踪数据的来源，防止某一批标记不准确的数据干扰整体数据集。

下面介绍在没有高质量标签的情况下的几种技术：

**弱监督**：Snorkel 是一种基于标记函数（labeling function，LF）的开源工具，可以为无标注数据添加低质量的标注，LF 通过数据中的一些关键信息进行标注，每个数据样本都会被多个LF进行标注，然后结合这些LF的标注来获得最可能的标签。为了观察LF的效果，推荐包含一部分人工标注的数据。如果LF可以完美标注数据，那么还需要机器学习模型吗，答案是需要，因为需要模型泛化到其它没有被LF覆盖到的数据。

**半监督**：半监督基于一小部分有标签数据来生成新的标签。一种经典的半监督方法是自训练，先在一小部分数据上训练模型，然后用这个模型来在未标注的数据预测，将最大概率的数据加入训练中，这样重复操作直到模型达到满意的结果。另一种方法是假设类似结构的数据有着类似的标签，可以使用kmeans聚类来对样本进行聚类。还有一种流行的方法是对比学习。

**迁移学习**

**主动学习**：不是随机的标记样本，而是选择性的标记对模型最有帮助的样本，最直接的方法是标记模型最不确定的样本。先随机标记一些样本，训练模型，在未标记的样本进行测试，标记那些模型最不确定的样本。还有一种方法是训练一组模型，每个模型都对样本进行一次标记，然后人工标记那些争议最大的样本。主动学习可以用在实时数据上。



### 类别不平衡


下面介绍三种解决类别不平衡的方法：

**使用合适的评价标准**：如为每个类都计算一次准确率，使用召回率等


**重采样**：从数量较小的类别中采样更多的数据，从数量较多的类别中采样更少的数据。但是这种方法容易欠拟合数量较多的类别，而过拟合数量较少的类别。一种方法是，先在重采样的数据上（对于数量较多的类别，只选取一部分样本）进行训练，再在原始数据上微调。还有一种动态采样方法，[Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/8396983)

**修改算法（损失函数）**：1. 设置一个cost矩阵 $C_{ij}$ 来衡量第 i 个类被判成第 j 个类的损失加权值，其中 $C_{ii} = 0$。2. 类别平衡损失，为每个类设置一个权重 $W_i$，$W_i$ 为样本总数 / 类别样本数，3. focal loss，即将原本的交叉熵损失 $L=-\log(p_t)$ 改为 $L=-(1-p_t)^{\gamma}\log(p_t)$。


**数据增强**：


## 特征工程



### 常用特征工程操作



#### 缺失值处理

对于类别，可以填入 "Unknown" 作为一类

对于数值，可以填入中位数或者均值等。

#### 目标值处理

如果目标值差异过大，可以选择取 $\log(1+x)$，numpy 中提供了 `np.log1p` 函数，在预测完成后取 $\exp(x)-1$，对应的函数为 `np.expm1` 


#### 特征归一化

例如将每个特征都进行归一化到 `[0, 1]` 区间，或者归一化到 `[a, b]` 区间

$$
\bar x = a + {{\left( {x - \min (x)} \right)(b - a)} \over {\max (x) - \min (x)}}
$$

#### 离散化

将连续值量化，例如将连续值分成一个个小区间。

#### 编码类别特征

为每个类别生成一个哈希值（Vowpal Wabbit），但是可能会遇到两个类别用着相同的哈希值；用类别出现次数替代类别；用 one-hot

#### 编码时间特征

将时间中的年月日分别提取出来，还可以加入年月日的正弦余弦信息，甚至是第几周的信息。下面是一个例子

```python
def date(df):  
    df['date'] = pd.to_datetime(df['date'])  
    df['year'] = df['date'].dt.year  
    df['day'] = df['date'].dt.day  
    df['month'] = df['date'].dt.month  
    df['month_name'] = df['date'].dt.month_name()  
    df['day_of_week'] = df['date'].dt.day_name()  
    df['week'] = df['date'].dt.isocalendar().week  
    df['year_sin'] = np.sin(2 * np.pi * df['year'])  
    df['year_cos'] = np.cos(2 * np.pi * df['year'])  
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)  
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)  
    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)  
    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)  
    df['group'] = (df['year'] - 2020) * 48 + df['month'] * 4 + df['day'] // 7  
  
    df.drop('date', axis=1, inplace=True)  
    return df
```


#### 特征交叉

将两个和多个特征结合生成新的特征

#### 离散和连续位置嵌入

参考Transformer中的位置嵌入

### 时序数据分割

如果一组数据与时间有关，如存在 `item_id` 和 `timestamp` 这两个列，每个 `item_id` 对应一些 `timestamp`。时序数据分割训练集和验证集时，需要按照时间分割。下面给出了基于 `sklearn.model_selection.TimeSeriesSplit` 的实现方式（该方法类似于kfold）

```python
from sklearn.model_selection import TimeSeriesSplit
import datetime as dt
kf = TimeSeriesSplit(n_splits=5, test_size=dt.timedelta(weeks=2).days)
for fold, (trx_idx, val_idx) in enumerate(kf.split(X, y, groups=X['warehouse'])):  
    X_train, y_train = X.iloc[trx_idx], y.iloc[trx_idx]  
    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

```

该方法有一个缺点，只能单纯按照整体序列划分，如一个数据集中包含10个物体在100天内的价格情况，整个数据集按照物体编号排列，前100行对应第1个物体在100天内的价格，假设 TimeSeriesSplit 的 test_size 为 14 天， 那么 `val_idx` 可能为 984 - 999，这样只会覆盖第10个物体的价格情况。

另外一种更加合适的方法

```python
# 设置 K 折分割  
n_splits = 5  
tscv = TimeSeriesSplit(n_splits=n_splits, test_size=dt.timedelta(days=1).days)  
  
train_idx = [[] for _ in range(n_splits)]  
test_idx = [[] for _ in range(n_splits)]  
  
# 对每个 unique_id 分组处理  
for unique_id, group in train.groupby('unique_id'):  
    # 按时间排序  
    group = group.sort_values('date').reset_index()  
    original_indices = group['index']  
      
    # 为每一折保存训练集和验证集  
    for fold_idx, (tidx, vidx) in enumerate(tscv.split(group)):  
        train_idx[fold_idx].extend(original_indices[tidx].tolist())  
        test_idx[fold_idx].extend(original_indices[vidx].tolist())
```


如果部分物体的时间过小，可以考虑分割出来单独处理


### 数据泄露

数据泄露不仅指在训练时使用了测试集的数据，还有另一种更难发现的情况，如一个模型训练的数据来自一个更先进的机器的测量结果，而实际上模型面对的数据可能来自另一个普通机器的测量结果，而模型没有这个普通机器的信息，因此模型训练时，标签实际上泄露了数据的信息。

数据泄露的一些原因：
1. 随机分割时序数据（应该按照时间分割数据），如预测股票股价，将前6天的数据作为训练集，第7天的数据用来验证。
2. 在分割之前进行scaling（应该在分割之后再进行scaling）
3. 使用测试集中的信息填补空缺数据（同样在分割之后再进行填补）
4. 在分割前没有处理好重复数据
5. 组泄露：如果一组数据有着强相关的标签，但是被分到不同的数据集中。
6. 数据收集时的泄露，如只收集一台机器产生的数据来训练和测试，解决这种泄露需要随机的收集数据。

归一化数据可以使得不同数据源的数据有着相同的均值和方差，一定程度上避免了数据泄露。

检查数据泄露：衡量每个特征与标签的相关程度，甚至是不同特征组合与标签的相关程度。做消融分析来衡量每个特征的重要程度

### 好的特征工程

特征泛化：如果一个特征只出现在很少的一部分数据中，那么这可能不是一个好的特征。此外如果一个特征值只出现在训练集而不出现在测试集，那么也不是一个好的特征（如训练集只收集周一到周六的数据，测试集是周日的数据，那么周几这个特征就不是一个好的特征，因为对于测试集，只有周日，而没有周一到周六）

## 模型开发


### 模型建模


如果一个模型有两个目标函数（并且这两个目标函数均有实际的意义），如

$$
L = \alpha L_1+\beta L_2
$$
如果直接用这个目标函数训练网络，那么如果 $\alpha$ 和 $\beta$ 有变动，就需要重新训练模型。为了避免这一问题，可以分别用 $L_1$ 和 $L_2$ 训练模型，然后将两个模型的输出相结合得到最终结果。


### 模型开发和训练

**模型集成**：训练多个模型（一般这些模型的结构不同）集成得到结果。有三种方式来创建一个模型集成

1. Bagging：对于一个数据集，有放回地采样来创建多个数据集，对于每个数据集都训练一个模型
2. Boosting：Boosting是一类迭代集成算法，Boosting中的样本被分配不同的权重，下一个学习器更加关注上一个学习器错分的样本。首先在数据集训练一个弱分类器，根据分类准确率来为样本设置权重（错分的样本设置更高的权重），在加权的数据集上训练第二个分类器，此时集成有两个分类器，根据这两个分类器的分类结果来为数据集重新分配权重，再用重分配权重的数据集训练第三个分类器，重复下去，最后的分类器为这些分类器的结合。算法有：Gradient Boosting Machine，xgboost等
3. Stacking：训练多个模型，将模型预测结果作为一个元学习器（可以是一个简单的逻辑回归模型）的输入，进行再一次学习。



### 模型评估

**模型校准（calibration）**：模型校准是为了将模型预测概率（模型预测的概率往往只表示大小，如分类模型中的概率）转为真实概率。

如何评价一个模型是否校准：先在训练集上训练好一个模型，在验证集上进行预测得到预测标签 $\hat y_i$，预测标签 $\hat y_i$ 和真实标签 $y_i$ 相互拼接成 $(\hat y_i, y_i)$，按照 $\hat y_i$ 升序排列，将这些数据等间距分组（如每10个分成一组），分别计算每组 $\hat y_i$ 和 $y_i$ 的均值，对均值画图，看是不是近似一条45度的折线。

实现模型校准的一种方法为 Platt scaling。


**切片评估**：将数据切成多组数据，并且使用模型对每组数据进行评估，使用模型对于测试集中的所有数据进行评估，有时会出现一些问题。分组的方法有按照经验分的，还有专门的算法：[Slice Finder: Automated Data Slicing for Model Validation | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/8731353)。




# 机器学习算法


## GBDT（梯度提升树）

[GBDT算法原理以及实例理解_gbdt算法实例 csdn-CSDN博客](https://blog.csdn.net/zpalyq110/article/details/79527653)

### CART回归树

GBDT 中使用了许多的决策树，这些决策树基本上都是CART回归树。由于GBDT每次迭代需要拟合梯度（连续值），所以不使用分类树。对于回归树算法来说最重要的是寻找最佳的划分点，回归树中的可划分点包含了所有特征的所有可取的值。在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以使用熵之类的指标不合适，取而代之的是平方误差。

**生成算法**

输入：训练数据集 D
输出：回归树 T

1. 计算各个特征各个值划分的两部分区域 $R_1$ 和 $R_2$ 的误差平方和，选择误差平方和最小的作为最优特征 j 和最优切分点 s

$$
\mathop {\min }\limits_{j,s} \left[ {\mathop {\min }\limits_{{c_1}} \sum\limits_{{x_i} \in {R_1}(j,s)} {{{\left( {{y_i} - {c_1}} \right)}^2}}  + \mathop {\min }\limits_{{c_2}} \sum\limits_{{x_i} \in {R_2}(j,s)} {{{\left( {{y_i} - {c_2}} \right)}^2}} } \right]
$$
其中，$c_1$ 是 $R_1$ 的样本输出均值，$c_2$ 是 $R_2$ 的样本输出均值。在实现时，直接用内外两个循环，外循环遍历所有的特征，内循环则遍历该特征对应的所有值，对于每个特征和值都计算一次误差平方和，最终得到最优特征和最优切分点。

2. 根据最优特征 j 和最优切分点 s，将数据集划分为两个子集合 $R_1$ 和 $R_2$

$$
\eqalign{
  & {R_1}(j,s) = D(x|{x^{(j)}} \le s),\quad {R_2}(j,s) = D(x|{x^{(j)}} > s)  \cr 
  & {c_m} = {1 \over {\left| {{R_m}(j,s)} \right|}}\sum\limits_{{x_i} \in {R_m}(j,s)} {{y_i}}  \cr} 
$$
3. 对于两个子集合分别执行步骤 1 和 2，直至满足条件。条件有3个：树的深度达到最大深度，子集合的节点无法再分了（如子集合只剩一个元素），子集合的标签值相同（说明这个子集合已经切分好了）

对生成的CART回归树做预测时，用叶子节点的均值来作为预测的输出结果。

### Gradient Boosting

假设对 x = 30 拟合，如果 $\hat x_1 = 20$，那么误差就是 10，再对误差进行拟合假设损失变为 5，继续对损失拟合，这样迭代下去，拟合的误差就会逐步减小，所有的拟合结果加起来就是最终结果。

**梯度提升算法**

1. 初始化 $f_0(x) = 0$
2. 迭代 M 次，对于 m = 1，2，...，M
	a. 对N个样本分别计算残差：${r_{mi}} = {y_i} - {f_{m - 1}}(x),\;i = 1,2,...,N$
	b. 拟合残差 $r_{mi}$ 学习一个回归树 $h_m(x)$
	c. 更新 $f_m(x) = f_{m-1}(x)+h_m(x)$
3. 回归问题提升树
$$
{f_M}(x) = {f_0}(x) + \sum\limits_{m = 1}^M {{h_m}(x)}  = \sum\limits_{m = 1}^M {{h_m}(x)} 
$$
这里拟合的残差就是负梯度，假设前一轮迭代得到的学习器为 $f_{t-1}(x)$，那么本轮目标是找到一个回归树 $h_t(x)$ 让本轮的损失最小，如果使用平方损失函数

$$
\eqalign{
  & L(y,{f_t}(x)) = L(y,{f_{t - 1}}(x) + {h_t}(x))  \cr 
  &  = {\left( {y - {f_{t - 1}}(x) - {h_t}(x)} \right)^2} = {\left( {r - {h_t}(x)} \right)^2} \cr} 
$$
为了最小化损失，计算梯度，可以得到负梯度就是残差（忽略乘数）

$$
- {{\partial L} \over {\partial f}} =  - {{\partial {{\left( {y - f} \right)}^2}} \over {\partial f}} = y - f(x)
$$

### GBDT算法

1. 初始化回归树

$$
{f_0}(x) = \arg \mathop {\min }\limits_c \sum\limits_{i = 1}^N {L\left( {{y_i},c} \right)} 
$$
一般来说，由于 $L$ 是平方损失，因此通过对 $f_0(x)$ 求导，令导数为0，求解出来的 $c = {1 \over N}\sum\limits_{i = 1}^N {{y_i}}$，即 $f_0(x)$ 为所有 $y$ 的均值。

2. 迭代 M 次，对于 m = 1, 2, ..., M
	a. 对于 N 个样本计算负梯度（残差）$r_{mi}=y_i-f_{m-1}(x)$
	b. 将上步得到的残差作为样本新的标签值，即将训练对 $(x_i,y_i)$ 换成 $(x_i, r_i)$，再使用一个新回归树训练，得到回归树 $h_m(x)$ 及其对应的叶子节点区域 $R_{jm}, j=1,2,\cdots, J$，$J$ 是叶子节点的个数
	c. 对叶子区域 $j=1,2,\cdots, J$ 计算最佳拟合值（对损失求导，令导数为0，求解 $\Upsilon_{jm}$，对于平方损失实际上就是残差的均值）
$$
{\Upsilon _{jm}} = \arg \mathop {\min }\limits_\Upsilon  \sum\limits_{{x_i} \in {R_{jm}}} {L({y_i},{f_{m - 1}}({x_i}) + \Upsilon )} 
$$

​	d. 更新强学习器（$l_r$ 为学习率）

$$
{f_m}(x) = {f_{m - 1}}(x) + l_r \sum\limits_{j = 1}^J {{\Upsilon _{jm}}I\left( {x \in {R_{jm}}} \right)} 
$$
3. 最终的学习器

$$
f(x) = {f_0}(x) + \sum\limits_{m = 1}^M {\sum\limits_{j = 1}^J {{\Upsilon _{jm}}I\left( {x \in {R_{jm}}} \right)} } 
$$

一般分析数据时不使用原始的GBDT算法，而是使用 XGBoost、LightGB、CatBoost等算法。对于二分类问题，一般使用 BinomialDeviance 损失函数，多分类则一般使用 MultinomialDeviance 损失函数。

### XGBoost

xgboost 在 GBDT 的基础上引入了 L1 和 L2 正则化（Lasso 和 Ridge 正则化）；在优化过程中使用二阶泰勒展开（Hessian矩阵），不仅考虑一阶梯度（导数信息），还加入二阶导数优化；引入最大增益剪枝，在每次树的分裂时计算增益，决定是否继续分裂。若分裂增益低于某个阈值，则停止分裂；此外支持分布式训练等



### LightGBM

LightGBM 采用了直方图算法来构建决策树。每个特征的值首先被分成多个离散的区间（bins），然后根据这些离散区间来选择最优的分裂点；采用了叶子-wise（leaf-wise）策略，即在每一轮构建树时，它会选择一个最优的叶节点进行分裂，从而在每次分裂时优化增益。（XGBoost使用的层次-wise，每一层同时分裂多个节点）；LightGBM 支持处理类别特征；支持自动处理缺失值

适合大规模数据集


### CatBoost

可以自动处理类别特征；对于高维稀疏数据有特有的稀疏优化技术；能够处理缺失值。






## 自动机器学习库

### AutoGluon


[autogluon/autogluon: Fast and Accurate ML in 3 Lines of Code](https://github.com/autogluon/autogluon)

可以直接在原始数据上训练模型，不需要知道关于数据和模型的细节，可以自动处理分类和回归问题；支持多模态数据，可以自动选择最佳模型和超参数。

需要一定的调参得到较为理想的结果。


Autogluon 可以通过 presets 参数来简单控制训练时使用的模型，excluded_model_types 参数用于排除一些模型

```python
predictor = TabularPredictor(label='target', path='model_output').fit(
    train_data,
    presets='best_quality',  # 更注重性能
    excluded_model_types=['NN', 'RF']  # 排除神经网络和随机森林模型
)
```

 部分 preset 的设置如下

| Preset         | Model Quality                                                       | Use Cases                                                                                                                                               | Fit Time (Ideal) |
| -------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- |
| best_quality   | State-of-the-art (SOTA), much better than high_quality              | When accuracy is what matters                                                                                                                           | 16x+             |
| high_quality   | Better than good_quality                                            | When a very powerful, portable solution with fast inference is required: Large-scale batch inference                                                    | 16x+             |
| good_quality   | Stronger than any other AutoML Framework                            | When a powerful, highly portable solution with very fast inference is required: Billion-scale batch inference, sub-100ms online-inference, edge-devices | 16x              |
| medium_quality | Competitive with other top AutoML Frameworks                        | Initial prototyping, establishing a performance baseline                                                                                                | 1x               |
| fast_training  | Fit simple statistical and baseline models + fast tree-based models | Fast to train but may not be very accurate                                                                                                              | 0.5x             |
| ...            |                                                                     |                                                                                                                                                         |                  |

如果需要精准控制模型，可以使用 hyperparameters 参数

```python
predictor = TimeSeriesPredictor(...)

predictor.fit(
    ...
    hyperparameters={
        "DeepAR": {},
        "Theta": [
            {"decomposition_type": "additive"},
            {"seasonal_period": 1},
        ],
    }
)
```



#### 预测表格数据

AutoGluon中使用 TabularDataset 加载数据，可以直接输入csv或者Parquet路径，或者 pd.Dataframe、np.ndarray 等

```python
>>> import pandas as pd
>>> from autogluon.common import TabularDataset
>>> train_data = TabularDataset("https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv")
>>> train_data_pd = pd.read_csv("https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv")
>>> assert isinstance(train_data, pd.DataFrame)  # True
>>> assert train_data.equals(train_data_pd)  # True
>>> assert type(train_data) == type(train_data_pd)  # True
```

加载后的 TabularDataset 实际上和 pd.DataFrame 相同，可以直接用pandas的方法

预测时可以使用 TabularPredictor，基础用法如下

```python
from autogluon.tabular import TabularPredictor

predictor = TabularPredictor(label="xxx", eval_metric='rmse').fit(train_data, presets='best_quality', excluded_model_types=['KNN', 'NN_TORCH', 'FASTAI'])
```


#### 预测时序数据

[Forecasting Time Series - In Depth - AutoGluon 1.2.0 documentation](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-indepth.html#manually-configuring-models)

预测时序数据较为复杂，时序数据使用 TimeSeriesDataFrame 类，需要指定 id 和 timestamp 列

```python
train_series = TimeSeriesDataFrame.from_data_frame(train, id_column="unique_id", timestamp_column="date")
```

预测器为 TimeSeriesPredictor，需要指定timestamp的间隔，如以小时或者天作为间隔，此外指定预测的时间长度

```python
predictor = TimeSeriesPredictor(
    freq='D',
    prediction_length=14,
    path="autogluon-m4-hourly",
    verbosity=2,
    target="sales",
    eval_metric="MASE",
)

predictor.fit(
    train_series,
    presets="fast_training",
    # time_limit=600,
    excluded_model_types=['KNN', 'NN_TORCH', 'FASTAI']
)
```

在预测时，有一个与表格预测不同的地方，predict传入的数据仍为训练集，预测其实是预测每个id对应的之后 `prediction_length` 长的目标值

```python
pred = predictor.predict(train_series)
```

为了方便索引预测的结果，可以重新设置索引

```python
axes = pred.axes
new_axes = []
for ax in axes[0]:
    new_axes.append(f"{ax[0]}_" + str(ax[1]).split(" ")[0])
pred = pred.set_axis(new_axes, axis="index")
```

由于predict预测之后 `prediction_length` 长的目标值，可能需要补充不同时间的数据。


### FLAML

FLAML 是一个轻量级的自动机器学习库

- 使用高效的搜索算法（如基于预算感知的搜索）。
- 能够在短时间内找到接近最佳的模型和超参数组合。
- 支持资源受限的任务，如时间或内存受限

>FLAML 可以直接输入未经过编码的类别特征。


**自动选择最优模型**

```python
from flaml import AutoML

automl = AutoML()
automl.fit(X_train, y_train, task="classification", time_budget=60, max_iter=6)
```

task 用于指定任务，如分类（classification）、回归（regression），时间序列预测（ts_forecast），时间序列预测分类（ts_forecast_classification）等等

time_budget 为训练的约束时间（s），max_iter 约束最多尝试的模型

metric_constraints 可以用来约束指标，如

```python
metric_constraints = [("train_loss", "<=", 0.1), ("val_loss", "<=", 0.1)]
automl.fit(  
	X_train,  
	y_train,  
	max_iter=100,
	metric_constraints=metric_constraints,  
)
```



**自定义优化指标，学习器等**

```python
automl.add_learner("mylgbm", MyLGBMEstimator)
automl.fit(
    X_train,
    y_train,
    task="classification",
    metric=custom_metric,
    estimator_list=["mylgbm"],
    time_budget=60,
)
```

内置的 metric 见 [Task Oriented AutoML | FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#optimization-metric)

自定义 metric 的示例为

```python
def custom_metric(
    X_val,
    y_val,
    estimator,
    labels,
    X_train,
    y_train,
    weight_val=None,
    weight_train=None,
    *args,
):
    from sklearn.metrics import log_loss
    import time

    start = time.time()
    y_pred = estimator.predict_proba(X_val)
    pred_time = (time.time() - start) / len(X_val)
    val_loss = log_loss(y_val, y_pred, labels=labels, sample_weight=weight_val)
    y_pred = estimator.predict_proba(X_train)
    train_loss = log_loss(y_train, y_pred, labels=labels, sample_weight=weight_train)
    alpha = 0.5
    return val_loss * (1 + alpha) - alpha * train_loss, {
        "val_loss": val_loss,
        "train_loss": train_loss,
        "pred_time": pred_time,
    }
```

内置 estimator 和自定义 estimator 见 [Task Oriented AutoML | FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML/#estimator-and-search-space)


**自定义训练**

```python
from flaml import tune
from flaml.automl.model import LGBMEstimator


def train_lgbm(config: dict) -> dict:
    # convert config dict to lgbm params
    params = LGBMEstimator(**config).params
    # train the model
    train_set = lightgbm.Dataset(csv_file_name)
    model = lightgbm.train(params, train_set)
    # evaluate the model
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test, pred)
    # return eval results as a dictionary
    return {"mse": mse}


# load a built-in search space from flaml
flaml_lgbm_search_space = LGBMEstimator.search_space(X_train.shape)
# specify the search space as a dict from hp name to domain; you can define your own search space same way
config_search_space = {
    hp: space["domain"] for hp, space in flaml_lgbm_search_space.items()
}
# give guidance about hp values corresponding to low training cost, i.e., {"n_estimators": 4, "num_leaves": 4}
low_cost_partial_config = {
    hp: space["low_cost_init_value"]
    for hp, space in flaml_lgbm_search_space.items()
    if "low_cost_init_value" in space
}
# run the tuning, minimizing mse, with total time budget 3 seconds
analysis = tune.run(
    train_lgbm,
    metric="mse",
    mode="min",
    config=config_search_space,
    low_cost_partial_config=low_cost_partial_config,
    time_budget_s=3,
    num_samples=-1,
)
```





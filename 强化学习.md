
## 引言

强化学习：智能体在复杂环境下如何最大化获得的奖励，强化学习由两部分组成：智能体和环境。一个智能体可能有以下几个组成部分：


1. **策略**（policy）：智能体会用策略来选取下一步的动作。

2. **价值函数**（value function）：价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。

3. **模型**（model）：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。


==策略==可以分为随机性策略和确定性策略，其中随机性策略指 $\pi$ 函数，即 $\pi(a|s)=p(a_{t}=a|s_{t}=s)$，用来描述当前状态下采取某个动作的概率。确定性策略直接从所有的动作中取最可能的动作，即${a^*} = \mathop {\arg \max }\limits_a p({a_t} = a|{s_t} = s)$ ，一般使用随机性策略。

==价值函数==的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个**折扣因子**（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。价值函数的定义为

$$
{V_\pi }(s) = {E_\pi }\left[ {{G_t}|{s_t} = s} \right] = {E_\pi }\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}{r_{t + k + 1}}|{s_t} = s} } \right]\quad for\;s \in S
$$
期望的下标为 $\pi$，用来反映使用策略 $\pi$ 时可以得到的奖励，这里的 $r_{t+k+1}$ 表示未来的奖励。

另外一种价值函数：Q函数，Q函数中包含了两个变量：状态和动作，定义为

$$
{Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}{r_{t + k + 1}}|{s_t} = s,{a_t} = a} } \right]\quad for\;s \in S
$$
表示未来获得的奖励取决于当前状态和动作。

==模型==决定了下一步的状态，下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率为

$$
p_{ss'}^a = p({s_{t + 1}} = s'|{s_t} = s,{a_t} = a)
$$
奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即

$$
R(s,a) = E\left[ {{r_{t + 1}}|{s_t} = s,{a_t} = a} \right]
$$

根据智能体学习的事物不同，可以把智能体分为：基于价值的智能体（value-based agent），显式地学习价值函数，隐式地学习它的策略，只能应用于离散环境。策略是其从学到的价值函数里面推算出来的。基于策略的智能体（policy-based agent），直接学习策略，我们给它一个状态，它就会输出对应动作的概率，基于策略的智能体并没有学习价值函数，可以用于连续环境。

把基于价值的智能体和基于策略的智能体结合起来就有了演员-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。


## 马尔可夫决策过程


### 马尔可夫奖励过程

马尔可夫奖励过程（Markov reward process, MRP）是马尔可夫链加上奖励函数。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数（reward function）。奖励函数 R 是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子 $\gamma$。如果状态数是有限的，那么 R 可以是一个向量。

MRP过程的回报可以定义为奖励的逐步叠加，

$$
{G_t} = {r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}{r_{t + 3}} +  \cdots  + {\gamma ^{T - t - 1}}{r_T}
$$
因此MRP过程的状态价值函数定义为

$$
{V^t}(s) = E\left[ {{G_t}|{s_t} = s} \right] = E\left[ {{r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}{r_{t + 3}} +  \cdots  + {\gamma ^{T - t - 1}}{r_T}|{s_t} = s} \right]
$$

**贝尔曼方程**：

$$
V(s) = R(s) + \gamma \sum\limits_{s' \in S} {p(s'|s)V(s')} 
$$
其中，第一项 $R(s)$ 是即时奖励，第二项则是未来奖励的折扣总和，$p(s'|s)$ 是从当前状态到未来状态的概率，$V(s')$ 表示未来某一个状态的价值。

贝尔曼方程的推导如下

$$
\eqalign{
  & V(s) = E\left[ {{G_t}|{s_t} = s} \right] = E\left[ {{r_{t + 1}} + \gamma {G_{t + 1}}|{s_t} = s} \right]  \cr 
  &  = E\left[ {{r_{t + 1}}|{s_t} = s} \right] + \gamma E\left[ {{G_{t + 1}}|{s_t} = s} \right]  \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s',{s_t} = s} \right]p({s_{t + 1}} = s'|{s_t} = s)}   \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s'} \right]p(s'|s)}   \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {p(s'|s)V(s')}  \cr} 
$$
计算状态价值函数有两种方法，一种是蒙特卡洛方法，直接从状态 s 和时刻 t 随机产生一些轨迹，对每个轨迹计算回报 $g = \sum\nolimits_{i = t}^{H - 1} {{\gamma ^{i - t}}{r_i}}$，所有轨迹的回报求平均就是 $V_t(s)$。

另一种是动态规划的方法，一直迭代贝尔曼方程，直至所有状态的 $V_t(s)$ 变化小于一个阈值。



### 马尔可夫决策过程

相对于MRP，马尔可夫决策过程多了决策，即多了动作，其他的类似。策略定义了某一个状态应该采取什么样的动作，知道当前状态后，可以把当前状态带入策略函数得到概率，即

$$
\pi (a|s) = p({a_t} = a|{s_t} = s)
$$
假设概率函数是平稳的，不同时间点，采取的动作其实都是在对策略函数进行采样。

已知马尔可夫决策过程和策略 $\pi$，可以把马尔可夫决策过程转换成马尔可夫奖励过程，如状态转移函数 $P(s'|s,a)$

$$
{P_\pi }(s'|s) = \sum\limits_{a \in A} {\pi (a|s)p(s'|s,a)} 
$$
奖励函数

$$
{r_\pi }(s) = \sum\limits_{a \in A} {\pi (a|s)R(s,a)} 
$$

马尔可夫决策过程的价值函数可以定义为

$$
{V_\pi }(s) = {E_\pi }\left[ {{G_t}|{s_t} = s} \right]
$$

另外引入一个 Q 函数（动作价值函数），定义的是某个状态采取某个动作，可能得到的回报的一个期望，即

$$
{Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right]
$$

价值函数可以写成

$$
{V_\pi }(s) = \sum\limits_{a \in A} {\pi (a|s){Q_\pi }(s,a)} 
$$
Q 函数的贝尔曼方程可以推导得到

$$
\eqalign{
  & {Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {{r_{t + 1}} + \gamma {r_{t + 2}} +  \cdots |{s_t} = s,{a_t} = a} \right]  \cr 
  &  = {E_\pi }\left[ {{r_{t + 1}} + \gamma {G_{t + 1}}|{s_t} = s,{a_t} = a} \right]  \cr 
  &  = {E_\pi }\left[ {{r_{t + 1}}|{s_t} = s,{a_t} = a} \right] + \gamma E\left[ {{G_{t + 1}}|{s_t} = s,{a_t} = a} \right]  \cr 
  &  = R(s,a) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s',{s_t} = s,{a_t} = a} \right]p(s'|s,a)}   \cr 
  &  = R(s,a) + \gamma \sum\limits_{s' \in S} {{V_\pi }(s')p(s'|s,a)}  \cr} 
$$
由此可得价值函数的贝尔曼方程

$$
{V_\pi }(s) = \sum\limits_{a \in A} {\pi (a|s){Q_\pi }(s,a)}  = \sum\limits_{a \in A} {\pi (a|s)\left( {R(s,a) + \gamma \sum\limits_{s' \in S} {{V_\pi }(s')p(s'|s,a)} } \right)} 
$$

**策略迭代**由两个步骤组成：策略评估和策略改进（policy improvement）。第一个步骤是策略评估，当前我们在优化策略 $\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。第二个步骤是策略改进，得到状态价值函数后，我们可以进一步推算出它的 Q 函数。得到 Q 函数后，我们直接对 Q 函数进行最大化，通过在 Q 函数做一个贪心的搜索来进一步改进策略。

**贝尔曼最优方程**：${V_\pi }(s) = \mathop {\max }\limits_{a \in A} {Q_\pi }(s,a)$

**价值迭代算法**：对于 k = 1 : H，对于所有状态 s

$$
\eqalign{
  & {Q_{k + 1}}(s,a) = R(s,a) + \gamma \sum\limits_{s' \in S} {p(s'|s,a){V_k}(s')}   \cr 
  & {V_{k + 1}}(s) = \mathop {\max }\limits_a {Q_{k + 1}}(s,a) \cr} 
$$
迭代完成后提取最优策略

$$
\pi (s) = \mathop {\arg \max }\limits_a \left[ {R(s,a) + \gamma \sum\limits_{s' \in S} {p(s'|s,a){V_{H + 1}}(s')} } \right]
$$

## 表格型方法


### 免模型预测


当无法获取马尔可夫决策过程的模型的情况下，可以通过蒙特卡洛方法和时序差分方法来估计给定策略的价值函数

蒙特卡洛方法指在给定的策略下，进行蒙特卡洛仿真，采样大量的轨迹，计算所有轨迹的真实回报，然后计算平均值。在每个回合中，如果在时间步 t 状态 s 被访问了，那么
+ 状态 s 的访问数 $N(s)$ 增加 1，$N(s) = N(s) + 1$。
+ 状态 s 的总的回报 $S(s)$ 增加 $G_t$，$S(s) = S(s) + G_t$。
（2）状态 s 的价值可以通过回报的平均来估计，即 $V (s) = S(s)/N(s)$。

根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。$N(s)\to \infty$，$V(s)\to V_{\pi}(s)$


时序差分方法的目的是对于某个给定的策略$\pi$ ，在线地计算价值函数 $V_{\pi}$，最简单的是一步时序差分，每往前走一步，就做一步自举，用得到的估计回报（estimated return）$r_{t+1} + \gamma V (s_{t+1})$ 来更
新上一时刻的值 $V(s_t)$：

$$
V({s_t}) \leftarrow V({s_t}) + \alpha \left( {{r_{t + 1}} + \gamma V({s_{t + 1}}) - V({s_t})} \right)
$$
估计回报 ${{r_{t + 1}} + \gamma V({s_{t + 1}})}$ 被称为时序差分目标。时序差分目标由两部分组成：到达 $s_{t+1}$ 状态的即时奖励 $r_{t+1}$ 和估计的 $V(s_{t+1})$。

上面是一步时序差分，调整步数可以变成 n 步时序差分，不同的步数影响状态的价值

$$
\eqalign{
  & n = 1\quad G_t^{(1)} = {r_{t + 1}} + \gamma V({s_{t + 1}})  \cr 
  & n = 2\quad G_t^{(2)} = {r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}V({s_{t + 2}})  \cr 
  &  \cdots  \cr} 
$$




### 免模型控制

在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有蒙特卡洛方法和时序差分方法的广义策略迭代（generalized policy iteration，GPI）。

在原本的策略迭代中，先根据给定的当前策略 $\pi$ 来估计价值函数，得到价值函数，通过贪心的方法来改进策略。当我们不知道奖励函数和状态转移时，如何进行策略的优化？

使用蒙特卡洛方法代替原本的动态规划的方法估计 Q 函数，然后再通过贪心的方法来进行改进。算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。为了保证蒙特卡洛方法有足够的探索，可以使用 $\varepsilon$-贪心搜索， $\varepsilon$-贪心是指有 $1-\varepsilon$ 的概率按照 Q 函数来决定动作，但是有 $\varepsilon$ 的概率随机选择，一般 $\varepsilon$ 随着时间递减。

蒙特卡洛方法的算法如下
随机对所有的 $s$ 和 $a$ 初始化 $Q(s,a)\in R$，将 $R(s,a)$ 置为0
在一个回合中，随机选择 $s_0\in S$，$a_0\in A(S_0)$，根据初始状态 $s_0$ 和 $a_0$ 产生 $\pi$：$s_0,a_0,r_1$，$s_1,a_1,r_2$，...，$s_{T-1}, a_{T-1}, r_{T}$。初始化回报 $G$ 为0，对 $\pi$ 进行反向遍历，即 $t=T-1,T-2,\cdots, 0$，每一步计算
$$
G \leftarrow \gamma G + {r_{t + 1}}
$$
如果 $(s_t,a_t)$ 出现在 $s_0,a_0$，$s_1,a_1$，...，$s_{t-1}, a_{t-1}$ 中，将 $G$ 追加到 $R(s_t, a_t)$ 中，$R(s_t, a_t)$ 的平均为 $Q(s_t,a_t)$，然后根据 Q 函数优化策略：$\pi ({s_t}) \leftarrow \mathop {\arg \max }\limits_a Q({s_t},{a_t})$


时序差分是给定一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么使用时序差分方
法的框架来估计 Q 函数，也就是 Sarsa 算法。Sarsa算法很简单，就是把时序差分算法中的价值函数换成 Q 函数。

$$
Q({s_t},{a_t}) \leftarrow Q({s_t},{a_t}) + \alpha \left( {{r_{t + 1}} + \gamma Q({s_{t + 1}},{a_{t + 1}}) - Q({s_t},{a_t})} \right)
$$

${r_{t + 1}} + \gamma Q({s_{t + 1}},{a_{t + 1}}) - Q({s_t},{a_t})$ 是时序差分误差，将 ${r_{t + 1}} + \gamma Q({s_{t + 1}},{a_{t + 1}})$ 作为目标值。


Sarsa 算法属于同策略算法，同策略在学习的过程中只存在一种策略，它用一种策略来做动作的选取和优化。Q学习是一种异策略算法，在学习的过程中，有两种不同的策略：目标策略和行为策略。目标策略是我们需要去学习的策略，一般用 $\pi$ 来表示。行为策略是探索环境的策略，一般用 $\mu$ 来表示。行为策略会尽量地探索环境中可能的轨迹，采集轨迹和数据，将数据提供给目标策略学习。目标策略直接使用贪心策略选择最大的 Q 函数。行为策略 $\mu$ 是一个随机的策略，但是我们采取 $\varepsilon$-贪心策略，基于Q表格逐步改进。

下面是一个 Q 学习的示例

```python
import gym
import numpy as np
import random
env = gym.make("CliffWalking-v0")

n_states = env.observation_space.n
n_actions = env.action_space.n

state = env.reset()

class QLearning:
    def __init__(self, n_states, n_actions, max_epoch):
        self.QTabel = np.random.random((n_states, n_actions))
        self.actions = [i for i in range(n_actions)]
        self.epsilon = 0.9
        self.max_epoch = max_epoch
        self.gamma = 0.9
        self.alpha = 0.9
    
    def sample_action(self, state, _iter):
        if _iter <= 100:
            epsilon = self.epsilon + (1-self.epsilon) * (_iter / 100)
        else:
            epsilon = 1
        rd = random.random()
        if rd < epsilon:
            action = np.argmax(self.QTabel[state])
        else:
            action = random.sample(self.actions, 1)[0]
        return action
    
    def update(self, state, action, next_state, r, done):
        if done:
            Q_target = r
        else:
            Q_target = r + self.gamma * np.max(self.QTabel[next_state])
        
        self.QTabel[state][action] += self.alpha * (Q_target - self.QTabel[state][action])

max_epoch = 50
agent = QLearning(n_states, n_actions, max_epoch)

rewards = []
for i in range(max_epoch):
    ep_reward = 0
    state = env.reset()[0]
    _iter = 0
    while True:
        action = agent.sample_action(state, _iter)
        next_state, reward, done, _, _ = env.step(action)
        agent.update(state, action, next_state, reward, done)
        state = next_state
        ep_reward += reward
        if done:
            break
    rewards.append(ep_reward)

import matplotlib.pyplot as plt

plt.plot(rewards)
plt.show()
```



## 策略梯度


### 策略梯度算法


策略梯度算法是基于策略的方法，其对策略进行了参数化。假设参数为 $\theta$ 的策略为 $\pi_{\theta}$，该策略为随机性策略，其输入某个状态，输出一个动作的概率分布。策略梯度算法不需要在动作空间中最大化价值，因此较为适合解决具有高维或者连续动作空间的问题。

强化学习有 3 个组成部分：演员（actor）、环境和奖励函数，其中环境和奖励函数是我们不能控制的，我们唯一能决定的是演员的策略 $\pi$，策略是一个网络，用 $\theta$ 来代表 $\pi$ 的参数。

把一个回合中环境输出的 s 和演员输出的动作 a 全部组合起来，便可以得到一个轨迹 $\tau$

$$
\tau  = \{ {s_1},{a_1},{s_2},{a_2}, \cdots ,{s_t},{a_t}\} 
$$

给定演员的参数 $\theta$，我们可以计算某个轨迹 $\tau$ 发生的概率

$$
\eqalign{
  & {p_\theta }(\tau ) = p({s_1}){p_\theta }({a_1}|{s_1})p({s_2}|{s_1},{a_1}){p_\theta }({a_2}|{s_2}) \cdots   \cr 
  &  = p({s_1})\prod\limits_{t = 1}^T {{p_\theta }({a_t}|{s_t})p({s_{t + 1}}|{s_t},{a_t})}  \cr} 
$$
收集一个轨迹的奖励求和可以得到 $R(\tau)$，我们的目标就是要调整 $\theta$ 使得 $R(\tau)$ 越大越好，具体来说，使得下式最大

$$
{{\bar R}_\theta } = \sum\limits_\tau  {R(\tau ){p_\theta }(\tau )} 
$$
因为我们要让奖励越大越好，所以可以使用梯度上升（gradient ascent）来最大化期望奖励。我们先要计算期望奖励 ${{\bar R}_\theta }$ 的梯度：

$$
\nabla {{\bar R}_\theta } = \sum\limits_\tau  {R(\tau )\nabla {p_\theta }(\tau )} 
$$
这里的 $R(\tau)$ 不需要可微分。由于 $\nabla {p_\theta }(\tau ) = {p_\theta }(\tau )\nabla \log {p_\theta }(\tau )$

所以可以得到

$$
\eqalign{
  & \nabla {{\bar R}_\theta } = \sum\limits_\tau  {R(\tau )\nabla {p_\theta }(\tau )}  = \sum\limits_\tau  {R(\tau ){p_\theta }(\tau )\nabla \log {p_\theta }(\tau )}   \cr 
  &  = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {R(\tau )\nabla \log {p_\theta }(\tau )} \right] \cr} 
$$
该式无法直接计算，所以使用采样的方式采样 N 个 $\tau$ 并计算每个轨迹的值，把每个值加起来得到梯度。

$$
\eqalign{
  & {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {R(\tau )\nabla \log {p_\theta }(\tau )} \right] \approx {1 \over N}\sum\limits_{n = 1}^N {R({\tau ^n})\nabla \log {p_\theta }({\tau ^n})}   \cr 
  &  = {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {R({\tau ^n})\nabla \log {p_\theta }(a_t^n|s_t^n)} }  \cr} 
$$
其中
$$
\eqalign{
  & \nabla \log {p_\theta }({\tau}) = \nabla \log p({s_1}) + \nabla \sum\limits_{t = 1}^T {\log } {p_\theta }({a_t}|{s_t}) + \nabla \sum\limits_{t = 1}^T {\log } p({s_{t + 1}}|{s_t},{a_t})  \cr 
  &  = \sum\limits_{t = 1}^T {\nabla \log } {p_\theta }({a_t}|{s_t}) \cr} 
$$

为了收集数据，先用参数为 $\theta$ 的智能体与环境交互，也就是拿智能体先与环境交互，得到一些数据和奖励，拿这些数据计算梯度，然后用奖励作为权重，更新模型。




### 实现技巧


技巧1：添加基线。由于在学习时，需要采样动作，但是很有可能一些动作没有采样到，如果全部奖励都是正的，就会导致没有被采样到的动作的概率下降。为了解决这一问题，需要引入基线，即将奖励减b，即

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {R({\tau ^n}) - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$
这样如果 $R(\tau^n)$ 比较小，也会降低采样这个动作的概率。


技巧2：为每一个动作分配合适的分数。一个做法是计算某个状态-动作对的奖励的时候，不把整场游戏得到的奖励全部加起来，只计算从这个动作执行以后得到的奖励。即

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {\sum\limits_{t' = t}^{{T_n}} {r_{t'}^n}  - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$
原来的权重是整场游戏的奖励的总和，现在改成从某个时刻 t 开始，假设这个动作是在 t 开始执行的，从 t 一直到游戏结束所有奖励的总和才能代表这个动作的好坏。接下来更进一步，我们把未来的奖励做一个折扣，即

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {\sum\limits_{t' = t}^{{T_n}} {{\gamma ^{t' - t}}r_{t'}^n}  - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$

### REINFORCE：蒙特卡洛策略梯度

蒙特卡洛方法可以理解为算法完成一个回合之后，再利用这个回合的数据去学习，做一次更新。因为我们已经获得了整个回合的数据，所以也能够获得每一个步骤的奖励，我们可以很方便地计算每个步骤的未来总奖励，即回报 $G_t$ 。$G_t$ 是未来总奖励，代表从这个步骤开始，我们能获得的奖励之和。$G_1$ 代表我们从第一步开始，往后能够获得的总奖励。$G_2$ 代表从第二步开始，往后能够获得的总奖励。相比蒙特卡洛方法一个回合更新一次，时序差分方法是每个步骤更新一次，即每走一步，更新一次，时序差分方法的更新频率更高。时序差分方法使用 Q 函数来近似地表示未来总奖励 $G_t$。

策略梯度中最简单也是最经典的一个算法：REINFORCE，先获取每个步骤的奖励，然后计算每个步骤的未来总奖励 $G_t$，将每个 $G_t$ 代入

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {G_t^n\nabla \log {\pi _\theta }(a_t^n|s_t^n)} } 
$$
优化每一个动作的输出。所以我们在编写代码时会设计一个函数，这个函数的输入是每个步骤获取的奖励，输出是每一个步骤的未来总奖励。因为未来总奖励可写为

$$
{G_t} = \sum\limits_{k = t + 1}^T {{\gamma ^{k - t - 1}}{r_k}}  = {r_{t + 1}} + \gamma {G_{t + 1}}
$$
先产生一个回合的数据，比如 $(s_1, a_1, G_1), (s_2, a_2, G_2), \cdots , (s_T , a_T , G_T)$，然后针对每个动作计算梯度$\nabla \log \pi ({a_t}|{s_t},\theta )$ 。在代码上计算时，我们要获取神经网络的输出。神经网络会输出每个动作对应的概率值（比如 0.2、0.5、0.3），然后我们还可以获取实际的动作 $a_t$，把动作转成独热
（one-hot）向量（比如 `[0,1,0]`）与 `log[0.2, 0.5, 0.3]` 相乘就可以得到 $\nabla \log \pi ({a_t}|{s_t},\theta )$ 。


## 近端策略优化

近端策略优化（proximal policy optimization，PPO）
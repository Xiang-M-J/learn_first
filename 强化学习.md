
## 引言

强化学习：智能体在复杂环境下如何最大化获得的奖励，强化学习由两部分组成：智能体和环境。一个智能体可能有以下几个组成部分：


1. **策略**（policy）：智能体会用策略来选取下一步的动作。

2. **价值函数**（value function）：价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。

3. **模型**（model）：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。


==策略==可以分为随机性策略和确定性策略，其中随机性策略指 $\pi$ 函数，即 $\pi(a|s)=p(a_{t}=a|s_{t}=s)$，用来描述当前状态下采取某个动作的概率。确定性策略直接从所有的动作中取最可能的动作，即${a^*} = \mathop {\arg \max }\limits_a p({a_t} = a|{s_t} = s)$ ，一般使用随机性策略。

==价值函数==的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个**折扣因子**（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。价值函数的定义为

$$
{V_\pi }(s) = {E_\pi }\left[ {{G_t}|{s_t} = s} \right] = {E_\pi }\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}{r_{t + k + 1}}|{s_t} = s} } \right]\quad for\;s \in S
$$
期望的下标为 $\pi$，用来反映使用策略 $\pi$ 时可以得到的奖励，这里的 $r_{t+k+1}$ 表示未来的奖励。

另外一种价值函数：Q函数，Q函数中包含了两个变量：状态和动作，定义为

$$
{Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}{r_{t + k + 1}}|{s_t} = s,{a_t} = a} } \right]\quad for\;s \in S
$$
表示未来获得的奖励取决于当前状态和动作。

==模型==决定了下一步的状态，下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率为

$$
p_{ss'}^a = p({s_{t + 1}} = s'|{s_t} = s,{a_t} = a)
$$
奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即

$$
R(s,a) = E\left[ {{r_{t + 1}}|{s_t} = s,{a_t} = a} \right]
$$

根据智能体学习的事物不同，可以把智能体分为：基于价值的智能体（value-based agent），显式地学习价值函数，隐式地学习它的策略，只能应用于离散环境。策略是其从学到的价值函数里面推算出来的。基于策略的智能体（policy-based agent），直接学习策略，我们给它一个状态，它就会输出对应动作的概率，基于策略的智能体并没有学习价值函数，可以用于连续环境。

把基于价值的智能体和基于策略的智能体结合起来就有了演员-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。


## 马尔可夫决策过程


### 马尔可夫奖励过程

马尔可夫奖励过程（Markov reward process, MRP）是马尔可夫链加上奖励函数。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数（reward function）。奖励函数 R 是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子 $\gamma$。如果状态数是有限的，那么 R 可以是一个向量。

MRP过程的回报可以定义为奖励的逐步叠加，

$$
{G_t} = {r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}{r_{t + 3}} +  \cdots  + {\gamma ^{T - t - 1}}{r_T}
$$
因此MRP过程的状态价值函数定义为

$$
{V^t}(s) = E\left[ {{G_t}|{s_t} = s} \right] = E\left[ {{r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}{r_{t + 3}} +  \cdots  + {\gamma ^{T - t - 1}}{r_T}|{s_t} = s} \right]
$$

**贝尔曼方程**：

$$
V(s) = R(s) + \gamma \sum\limits_{s' \in S} {p(s'|s)V(s')} 
$$
其中，第一项 $R(s)$ 是即时奖励，第二项则是未来奖励的折扣总和，$p(s'|s)$ 是从当前状态到未来状态的概率，$V(s')$ 表示未来某一个状态的价值。

贝尔曼方程的推导如下

$$
\eqalign{
  & V(s) = E\left[ {{G_t}|{s_t} = s} \right] = E\left[ {{r_{t + 1}} + \gamma {G_{t + 1}}|{s_t} = s} \right]  \cr 
  &  = E\left[ {{r_{t + 1}}|{s_t} = s} \right] + \gamma E\left[ {{G_{t + 1}}|{s_t} = s} \right]  \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s',{s_t} = s} \right]p({s_{t + 1}} = s'|{s_t} = s)}   \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s'} \right]p(s'|s)}   \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {p(s'|s)V(s')}  \cr} 
$$
计算状态价值函数有两种方法，一种是蒙特卡洛方法，直接从状态 s 和时刻 t 随机产生一些轨迹，对每个轨迹计算回报 $g = \sum\nolimits_{i = t}^{H - 1} {{\gamma ^{i - t}}{r_i}}$，所有轨迹的回报求平均就是 $V_t(s)$。

另一种是动态规划的方法，一直迭代贝尔曼方程，直至所有状态的 $V_t(s)$ 变化小于一个阈值。





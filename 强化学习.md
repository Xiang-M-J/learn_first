
## 引言

强化学习：智能体在复杂环境下如何最大化获得的奖励，强化学习由两部分组成：智能体和环境。一个智能体可能有以下几个组成部分：


1. **策略**（policy）：智能体会用策略来选取下一步的动作。

2. **价值函数**（value function）：价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。

3. **模型**（model）：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。


==策略==可以分为随机性策略和确定性策略，其中随机性策略指 $\pi$ 函数，即 $\pi(a|s)=p(a_{t}=a|s_{t}=s)$，用来描述当前状态下采取某个动作的概率。确定性策略直接从所有的动作中取最可能的动作，即${a^*} = \mathop {\arg \max }\limits_a p({a_t} = a|{s_t} = s)$ ，一般使用随机性策略。

==价值函数==的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个**折扣因子**（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。价值函数的定义为

$$
{V_\pi }(s) = {E_\pi }\left[ {{G_t}|{s_t} = s} \right] = {E_\pi }\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}{r_{t + k + 1}}|{s_t} = s} } \right]\quad for\;s \in S
$$
期望的下标为 $\pi$，用来反映使用策略 $\pi$ 时可以得到的奖励，这里的 $r_{t+k+1}$ 表示未来的奖励。

另外一种价值函数：Q函数，Q函数中包含了两个变量：状态和动作，定义为

$$
{Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}{r_{t + k + 1}}|{s_t} = s,{a_t} = a} } \right]\quad for\;s \in S
$$
表示未来获得的奖励取决于当前状态和动作。

==模型==决定了下一步的状态，下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率为

$$
p_{ss'}^a = p({s_{t + 1}} = s'|{s_t} = s,{a_t} = a)
$$
奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即

$$
R(s,a) = E\left[ {{r_{t + 1}}|{s_t} = s,{a_t} = a} \right]
$$

根据智能体学习的事物不同，可以把智能体分为：基于价值的智能体（value-based agent），显式地学习价值函数，隐式地学习它的策略，只能应用于离散环境。策略是其从学到的价值函数里面推算出来的。基于策略的智能体（policy-based agent），直接学习策略，我们给它一个状态，它就会输出对应动作的概率，基于策略的智能体并没有学习价值函数，可以用于连续环境。

把基于价值的智能体和基于策略的智能体结合起来就有了演员-评论员智能体（actor-critic agent）。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。


## 马尔可夫决策过程


### 马尔可夫奖励过程

马尔可夫奖励过程（Markov reward process, MRP）是马尔可夫链加上奖励函数。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数（reward function）。奖励函数 R 是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子 $\gamma$。如果状态数是有限的，那么 R 可以是一个向量。

MRP过程的回报可以定义为奖励的逐步叠加，

$$
{G_t} = {r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}{r_{t + 3}} +  \cdots  + {\gamma ^{T - t - 1}}{r_T}
$$
因此MRP过程的状态价值函数定义为

$$
{V^t}(s) = E\left[ {{G_t}|{s_t} = s} \right] = E\left[ {{r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}{r_{t + 3}} +  \cdots  + {\gamma ^{T - t - 1}}{r_T}|{s_t} = s} \right]
$$

**贝尔曼方程**：

$$
V(s) = R(s) + \gamma \sum\limits_{s' \in S} {p(s'|s)V(s')} 
$$
其中，第一项 $R(s)$ 是即时奖励，第二项则是未来奖励的折扣总和，$p(s'|s)$ 是从当前状态到未来状态的概率，$V(s')$ 表示未来某一个状态的价值。

贝尔曼方程的推导如下

$$
\eqalign{
  & V(s) = E\left[ {{G_t}|{s_t} = s} \right] = E\left[ {{r_{t + 1}} + \gamma {G_{t + 1}}|{s_t} = s} \right]  \cr 
  &  = E\left[ {{r_{t + 1}}|{s_t} = s} \right] + \gamma E\left[ {{G_{t + 1}}|{s_t} = s} \right]  \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s',{s_t} = s} \right]p({s_{t + 1}} = s'|{s_t} = s)}   \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s'} \right]p(s'|s)}   \cr 
  &  = R(s) + \gamma \sum\limits_{s' \in S} {p(s'|s)V(s')}  \cr} 
$$
计算状态价值函数有两种方法，一种是蒙特卡洛方法，直接从状态 s 和时刻 t 随机产生一些轨迹，对每个轨迹计算回报 $g = \sum\nolimits_{i = t}^{H - 1} {{\gamma ^{i - t}}{r_i}}$，所有轨迹的回报求平均就是 $V_t(s)$。

另一种是动态规划的方法，一直迭代贝尔曼方程，直至所有状态的 $V_t(s)$ 变化小于一个阈值。



### 马尔可夫决策过程

相对于MRP，马尔可夫决策过程多了决策，即多了动作，其他的类似。策略定义了某一个状态应该采取什么样的动作，知道当前状态后，可以把当前状态带入策略函数得到概率，即

$$
\pi (a|s) = p({a_t} = a|{s_t} = s)
$$
假设概率函数是平稳的，不同时间点，采取的动作其实都是在对策略函数进行采样。

已知马尔可夫决策过程和策略 $\pi$，可以把马尔可夫决策过程转换成马尔可夫奖励过程，如状态转移函数 $P(s'|s,a)$

$$
{P_\pi }(s'|s) = \sum\limits_{a \in A} {\pi (a|s)p(s'|s,a)} 
$$
奖励函数

$$
{r_\pi }(s) = \sum\limits_{a \in A} {\pi (a|s)R(s,a)} 
$$

马尔可夫决策过程的价值函数可以定义为

$$
{V_\pi }(s) = {E_\pi }\left[ {{G_t}|{s_t} = s} \right]
$$

另外引入一个 Q 函数（动作价值函数），定义的是某个状态采取某个动作，可能得到的回报的一个期望，即

$$
{Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right]
$$

价值函数可以写成

$$
{V_\pi }(s) = \sum\limits_{a \in A} {\pi (a|s){Q_\pi }(s,a)} 
$$
Q 函数的贝尔曼方程可以推导得到

$$
\eqalign{
  & {Q_\pi }(s,a) = {E_\pi }\left[ {{G_t}|{s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {{r_{t + 1}} + \gamma {r_{t + 2}} +  \cdots |{s_t} = s,{a_t} = a} \right]  \cr 
  &  = {E_\pi }\left[ {{r_{t + 1}} + \gamma {G_{t + 1}}|{s_t} = s,{a_t} = a} \right]  \cr 
  &  = {E_\pi }\left[ {{r_{t + 1}}|{s_t} = s,{a_t} = a} \right] + \gamma E\left[ {{G_{t + 1}}|{s_t} = s,{a_t} = a} \right]  \cr 
  &  = R(s,a) + \gamma \sum\limits_{s' \in S} {E\left[ {{G_{t + 1}}|{s_{t + 1}} = s',{s_t} = s,{a_t} = a} \right]p(s'|s,a)}   \cr 
  &  = R(s,a) + \gamma \sum\limits_{s' \in S} {{V_\pi }(s')p(s'|s,a)}  \cr} 
$$
由此可得价值函数的贝尔曼方程

$$
{V_\pi }(s) = \sum\limits_{a \in A} {\pi (a|s){Q_\pi }(s,a)}  = \sum\limits_{a \in A} {\pi (a|s)\left( {R(s,a) + \gamma \sum\limits_{s' \in S} {{V_\pi }(s')p(s'|s,a)} } \right)} 
$$

**策略迭代**由两个步骤组成：策略评估和策略改进（policy improvement）。第一个步骤是策略评估，当前我们在优化策略 $\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。第二个步骤是策略改进，得到状态价值函数后，我们可以进一步推算出它的 Q 函数。得到 Q 函数后，我们直接对 Q 函数进行最大化，通过在 Q 函数做一个贪心的搜索来进一步改进策略。

**贝尔曼最优方程**：${V_\pi }(s) = \mathop {\max }\limits_{a \in A} {Q_\pi }(s,a)$

**价值迭代算法**：对于 k = 1 : H，对于所有状态 s

$$
\eqalign{
  & {Q_{k + 1}}(s,a) = R(s,a) + \gamma \sum\limits_{s' \in S} {p(s'|s,a){V_k}(s')}   \cr 
  & {V_{k + 1}}(s) = \mathop {\max }\limits_a {Q_{k + 1}}(s,a) \cr} 
$$
迭代完成后提取最优策略

$$
\pi (s) = \mathop {\arg \max }\limits_a \left[ {R(s,a) + \gamma \sum\limits_{s' \in S} {p(s'|s,a){V_{H + 1}}(s')} } \right]
$$


## 表格型方法


### 免模型预测


当无法获取马尔可夫决策过程的模型的情况下，可以通过蒙特卡洛方法和时序差分方法来估计给定策略的价值函数

蒙特卡洛方法指在给定的策略下，进行蒙特卡洛仿真，采样大量的轨迹，计算所有轨迹的真实回报，然后计算平均值。在每个回合中，如果在时间步 t 状态 s 被访问了，那么
+ 状态 s 的访问数 $N(s)$ 增加 1，$N(s) = N(s) + 1$。
+ 状态 s 的总的回报 $S(s)$ 增加 $G_t$，$S(s) = S(s) + G_t$。
（2）状态 s 的价值可以通过回报的平均来估计，即 $V (s) = S(s)/N(s)$。

根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。$N(s)\to \infty$，$V(s)\to V_{\pi}(s)$


时序差分方法的目的是对于某个给定的策略$\pi$ ，在线地计算价值函数 $V_{\pi}$，最简单的是一步时序差分，每往前走一步，就做一步自举，用得到的估计回报（estimated return）$r_{t+1} + \gamma V (s_{t+1})$ 来更新上一时刻的值 $V(s_t)$：

$$
V({s_t}) \leftarrow V({s_t}) + \alpha \left( {{r_{t + 1}} + \gamma V({s_{t + 1}}) - V({s_t})} \right)
$$
估计回报 ${{r_{t + 1}} + \gamma V({s_{t + 1}})}$ 被称为时序差分目标。时序差分目标由两部分组成：到达 $s_{t+1}$ 状态的即时奖励 $r_{t+1}$ 和估计的 $V(s_{t+1})$。

上面是一步时序差分，调整步数可以变成 n 步时序差分，不同的步数影响状态的价值

$$
\eqalign{
  & n = 1\quad G_t^{(1)} = {r_{t + 1}} + \gamma V({s_{t + 1}})  \cr 
  & n = 2\quad G_t^{(2)} = {r_{t + 1}} + \gamma {r_{t + 2}} + {\gamma ^2}V({s_{t + 2}})  \cr 
  &  \cdots  \cr} 
$$




### 免模型控制

在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有蒙特卡洛方法和时序差分方法的广义策略迭代（generalized policy iteration，GPI）。

在原本的策略迭代中，先根据给定的当前策略 $\pi$ 来估计价值函数，得到价值函数，通过贪心的方法来改进策略。当我们不知道奖励函数和状态转移时，如何进行策略的优化？

使用蒙特卡洛方法代替原本的动态规划的方法估计 Q 函数，然后再通过贪心的方法来进行改进。算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。为了保证蒙特卡洛方法有足够的探索，可以使用 $\varepsilon$-贪心搜索， $\varepsilon$-贪心是指有 $1-\varepsilon$ 的概率按照 Q 函数来决定动作，但是有 $\varepsilon$ 的概率随机选择，一般 $\varepsilon$ 随着时间递减。

蒙特卡洛方法的算法如下
随机对所有的 $s$ 和 $a$ 初始化 $Q(s,a)\in R$，将 $R(s,a)$ 置为0
在一个回合中，随机选择 $s_0\in S$，$a_0\in A(S_0)$，根据初始状态 $s_0$ 和 $a_0$ 产生 $\pi$：$s_0,a_0,r_1$，$s_1,a_1,r_2$，...，$s_{T-1}, a_{T-1}, r_{T}$。初始化回报 $G$ 为0，对 $\pi$ 进行反向遍历，即 $t=T-1,T-2,\cdots, 0$，每一步计算
$$
G \leftarrow \gamma G + {r_{t + 1}}
$$
如果 $(s_t,a_t)$ 出现在 $s_0,a_0$，$s_1,a_1$，...，$s_{t-1}, a_{t-1}$ 中，将 $G$ 追加到 $R(s_t, a_t)$ 中，$R(s_t, a_t)$ 的平均为 $Q(s_t,a_t)$，然后根据 Q 函数优化策略：$\pi ({s_t}) \leftarrow \mathop {\arg \max }\limits_a Q({s_t},{a_t})$


时序差分是给定一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么使用时序差分方法的框架来估计 Q 函数，也就是 Sarsa 算法。Sarsa算法很简单，就是把时序差分算法中的价值函数换成 Q 函数。

$$
Q({s_t},{a_t}) \leftarrow Q({s_t},{a_t}) + \alpha \left( {{r_{t + 1}} + \gamma Q({s_{t + 1}},{a_{t + 1}}) - Q({s_t},{a_t})} \right)
$$

${r_{t + 1}} + \gamma Q({s_{t + 1}},{a_{t + 1}}) - Q({s_t},{a_t})$ 是时序差分误差，将 ${r_{t + 1}} + \gamma Q({s_{t + 1}},{a_{t + 1}})$ 作为目标值。


Sarsa 算法属于同策略算法，同策略在学习的过程中只存在一种策略，它用一种策略来做动作的选取和优化。Q学习是一种异策略算法，在学习的过程中，有两种不同的策略：目标策略和行为策略。目标策略是我们需要去学习的策略，一般用 $\pi$ 来表示。行为策略是探索环境的策略，一般用 $\mu$ 来表示。行为策略会尽量地探索环境中可能的轨迹，采集轨迹和数据，将数据提供给目标策略学习。目标策略直接使用贪心策略选择最大的 Q 函数。行为策略 $\mu$ 是一个随机的策略，但是我们采取 $\varepsilon$-贪心策略，基于Q表格逐步改进。

下面是一个 Q 学习的示例

```python
import gym
import numpy as np
import random
env = gym.make("CliffWalking-v0")

n_states = env.observation_space.n
n_actions = env.action_space.n

state = env.reset()

class QLearning:
    def __init__(self, n_states, n_actions, max_epoch):
        self.QTabel = np.random.random((n_states, n_actions))
        self.actions = [i for i in range(n_actions)]
        self.epsilon = 0.9
        self.max_epoch = max_epoch
        self.gamma = 0.9
        self.alpha = 0.9
    
    def sample_action(self, state, _iter):
        if _iter <= 100:
            epsilon = self.epsilon + (1-self.epsilon) * (_iter / 100)
        else:
            epsilon = 1
        rd = random.random()
        if rd < epsilon:
            action = np.argmax(self.QTabel[state])
        else:
            action = random.sample(self.actions, 1)[0]
        return action
    
    def update(self, state, action, next_state, r, done):
        if done:
            Q_target = r
        else:
            Q_target = r + self.gamma * np.max(self.QTabel[next_state])
        
        self.QTabel[state][action] += self.alpha * (Q_target - self.QTabel[state][action])

max_epoch = 50
agent = QLearning(n_states, n_actions, max_epoch)

rewards = []
for i in range(max_epoch):
    ep_reward = 0
    state = env.reset()[0]
    _iter = 0
    while True:
        action = agent.sample_action(state, _iter)
        next_state, reward, done, _, _ = env.step(action)
        agent.update(state, action, next_state, reward, done)
        state = next_state
        ep_reward += reward
        if done:
            break
    rewards.append(ep_reward)

import matplotlib.pyplot as plt

plt.plot(rewards)
plt.show()
```



## 策略梯度


### 策略梯度算法


策略梯度算法是基于策略的方法，其对策略进行了参数化。假设参数为 $\theta$ 的策略为 $\pi_{\theta}$，该策略为随机性策略，其输入某个状态，输出一个动作的概率分布。策略梯度算法不需要在动作空间中最大化价值，因此较为适合解决具有高维或者连续动作空间的问题。

强化学习有 3 个组成部分：演员（actor）、环境和奖励函数，其中环境和奖励函数是我们不能控制的，我们唯一能决定的是演员的策略 $\pi$，策略是一个网络，用 $\theta$ 来代表 $\pi$ 的参数。

把一个回合中环境输出的 s 和演员输出的动作 a 全部组合起来，便可以得到一个轨迹 $\tau$

$$
\tau  = \{ {s_1},{a_1},{s_2},{a_2}, \cdots ,{s_t},{a_t}\} 
$$

给定演员的参数 $\theta$，我们可以计算某个轨迹 $\tau$ 发生的概率

$$
\eqalign{
  & {p_\theta }(\tau ) = p({s_1}){p_\theta }({a_1}|{s_1})p({s_2}|{s_1},{a_1}){p_\theta }({a_2}|{s_2}) \cdots   \cr 
  &  = p({s_1})\prod\limits_{t = 1}^T {{p_\theta }({a_t}|{s_t})p({s_{t + 1}}|{s_t},{a_t})}  \cr} 
$$
收集一个轨迹的奖励求和可以得到 $R(\tau)$，我们的目标就是要调整 $\theta$ 使得 $R(\tau)$ 越大越好，具体来说，使得下式最大

$$
{{\bar R}_\theta } = \sum\limits_\tau  {R(\tau ){p_\theta }(\tau )} 
$$
因为我们要让奖励越大越好，所以可以使用梯度上升（gradient ascent）来最大化期望奖励。我们先要计算期望奖励 ${{\bar R}_\theta }$ 的梯度：

$$
\nabla {{\bar R}_\theta } = \sum\limits_\tau  {R(\tau )\nabla {p_\theta }(\tau )} 
$$
这里的 $R(\tau)$ 不需要可微分。由于 $\nabla {p_\theta }(\tau ) = {p_\theta }(\tau )\nabla \log {p_\theta }(\tau )$

所以可以得到

$$
\eqalign{
  & \nabla {{\bar R}_\theta } = \sum\limits_\tau  {R(\tau )\nabla {p_\theta }(\tau )}  = \sum\limits_\tau  {R(\tau ){p_\theta }(\tau )\nabla \log {p_\theta }(\tau )}   \cr 
  &  = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {R(\tau )\nabla \log {p_\theta }(\tau )} \right] \cr} 
$$
该式无法直接计算，所以使用采样的方式采样 N 个 $\tau$ 并计算每个轨迹的值，把每个值加起来得到梯度。

$$
\eqalign{
  & {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {R(\tau )\nabla \log {p_\theta }(\tau )} \right] \approx {1 \over N}\sum\limits_{n = 1}^N {R({\tau ^n})\nabla \log {p_\theta }({\tau ^n})}   \cr 
  &  = {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {R({\tau ^n})\nabla \log {p_\theta }(a_t^n|s_t^n)} }  \cr} 
$$
其中
$$
\eqalign{
  & \nabla \log {p_\theta }({\tau}) = \nabla \log p({s_1}) + \nabla \sum\limits_{t = 1}^T {\log } {p_\theta }({a_t}|{s_t}) + \nabla \sum\limits_{t = 1}^T {\log } p({s_{t + 1}}|{s_t},{a_t})  \cr 
  &  = \sum\limits_{t = 1}^T {\nabla \log } {p_\theta }({a_t}|{s_t}) \cr} 
$$

为了收集数据，先用参数为 $\theta$ 的智能体与环境交互，也就是拿智能体先与环境交互，得到一些数据和奖励，拿这些数据计算梯度，然后用奖励作为权重，更新模型。


### 实现技巧


技巧1：添加基线。由于在学习时，需要采样动作，但是很有可能一些动作没有采样到，如果全部奖励都是正的，就会导致没有被采样到的动作的概率下降。为了解决这一问题，需要引入基线，即将奖励减b，即

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {R({\tau ^n}) - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$
这样如果 $R(\tau^n)$ 比较小，也会降低采样这个动作的概率。


技巧2：为每一个动作分配合适的分数。一个做法是计算某个状态-动作对的奖励的时候，不把整场游戏得到的奖励全部加起来，只计算从这个动作执行以后得到的奖励。即

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {\sum\limits_{t' = t}^{{T_n}} {r_{t'}^n}  - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$
原来的权重是整场游戏的奖励的总和，现在改成从某个时刻 t 开始，假设这个动作是在 t 开始执行的，从 t 一直到游戏结束所有奖励的总和才能代表这个动作的好坏。接下来更进一步，我们把未来的奖励做一个折扣，即

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {\sum\limits_{t' = t}^{{T_n}} {{\gamma ^{t' - t}}r_{t'}^n}  - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$

### REINFORCE：蒙特卡洛策略梯度

蒙特卡洛方法可以理解为算法完成一个回合之后，再利用这个回合的数据去学习，做一次更新。因为我们已经获得了整个回合的数据，所以也能够获得每一个步骤的奖励，我们可以很方便地计算每个步骤的未来总奖励，即回报 $G_t$ 。$G_t$ 是未来总奖励，代表从这个步骤开始，我们能获得的奖励之和。$G_1$ 代表我们从第一步开始，往后能够获得的总奖励。$G_2$ 代表从第二步开始，往后能够获得的总奖励。相比蒙特卡洛方法一个回合更新一次，时序差分方法是每个步骤更新一次，即每走一步，更新一次，时序差分方法的更新频率更高。时序差分方法使用 Q 函数来近似地表示未来总奖励 $G_t$。

策略梯度中最简单也是最经典的一个算法：REINFORCE，先获取每个步骤的奖励，然后计算每个步骤的未来总奖励 $G_t$，将每个 $G_t$ 代入

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {G_t^n\nabla \log {\pi _\theta }(a_t^n|s_t^n)} } 
$$
优化每一个动作的输出。所以我们在编写代码时会设计一个函数，这个函数的输入是每个步骤获取的奖励，输出是每一个步骤的未来总奖励。因为未来总奖励可写为

$$
{G_t} = \sum\limits_{k = t + 1}^T {{\gamma ^{k - t - 1}}{r_k}}  = {r_{t + 1}} + \gamma {G_{t + 1}}
$$
先产生一个回合的数据，比如 $(s_1, a_1, G_1), (s_2, a_2, G_2), \cdots , (s_T , a_T , G_T)$，然后针对每个动作计算梯度$\nabla \log \pi ({a_t}|{s_t},\theta )$ 。在代码上计算时，我们要获取神经网络的输出。神经网络会输出每个动作对应的概率值（比如 0.2、0.5、0.3），然后我们还可以获取实际的动作 $a_t$，把动作转成独热向量（比如 `[0,1,0]`）与 `log[0.2, 0.5, 0.3]` 相乘就可以得到 $\nabla \log \pi ({a_t}|{s_t},\theta )$ 。


## 近端策略优化（PPO）


### 重要性采样


近端策略优化（proximal policy optimization，PPO）是策略梯度的变形。策略梯度的公式如下：

$$
\nabla {\bar R_\theta } = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {R(\tau )\nabla \log {p_\theta }(\tau )} \right]
$$
这个公式问题在于${E_{\tau  \sim {p_\theta }(\tau )}}$ 是对策略 $\pi_{\theta}$ 采样的轨迹 $\tau$ 求期望，一旦更新了参数，从$\theta$ 变成 $\theta'$ ，概率 $p_{\theta}(\tau)$ 就不对了，需要重新采样数据，这样智能体就需要花费大量时间来采样数据。因此想从同策略改为异策略，用另外一个策略 $\pi_{\theta'}$ 与环境交互（$\theta'$ 固定下来），用 $\theta'$ 采样到的数据去训练 $\theta$，这样就可以多次使用同一批数据，更加高效。

具体的做法，利用了==重要性采样==的概念

假设需要计算$f(x)$ 的期望值，需要从分布 $p$ 采样 $x$，再把 x 带入 f，做平均得到期望值。如果不能从分布 p 采样数据，只能从另外一个分布 q 采样，q 可以是任意分布，那么有如下变换

$$
{E_{x\sim p}}\left[ {f(x)} \right] = \int {f(x)p(x)} dx = \int {f(x){{p(x)} \over {q(x)}}q(x)} dx = {E_{x\sim q}}\left[ {f(x){{p(x)} \over {q(x)}}} \right]
$$

这样可以从 q 中采样 x，再计算 ${f(x){{p(x)} \over {q(x)}}}$，再取期望就能得到想要的值（需要注意的是 $p(x)$ 为零的时候，$q(x)$不能为零）。此外虽然 q 可以是任意分布，但是和 p 差距不能过大，否则方差差距很大。

现在把重要性采样用于异策略算法中，使用策略 $\theta'$ 与环境交互（类似从 q 中采样 x），公式为

$$
\nabla {{\bar R}_\theta } = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {{{{p_\theta }(\tau )} \over {{p_{\theta '}}(\tau )}}R(\tau )\nabla \log {p_\theta }(\tau )} \right]
$$

让 $\theta'$ 与环境交互采样大量的数据，$\theta$ 可以多次更新参数，一直到 $\theta$ 训练到一定的程度。更新多次以后，$\theta'$ 再重新做采样。

实际操作时，不是给整个轨迹 $\tau$ 一样的分数，而是将每个状态-动作单独计算，即

$$
\nabla {{\bar R}_\theta } = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {{{{p_\theta }({s_t},{a_t})} \over {{p_{\theta '}}({s_t},{a_t})}}{A^\theta }({s_t},{a_t})\nabla \log {p_\theta }({a_t}|{s_t})} \right]
$$
这里的 ${{A^\theta }({s_t},{a_t})}$ 假设和 ${{A^{\theta'} }({s_t},{a_t})}$ 差不多，如何估计 ${{A }({s_t},{a_t})}$ ：策略 $\theta'$ 在某一个状态采取某一个动作，接下来会得到累积奖励的值，再减去基线的值（可以是评论员对于动作的评价值）即可。

假设模型是 $\theta$ 和 $\theta'$ 时，看到 $s_t$ 的概率是一样的，即 $p_{\theta}(s_t)=p_{\theta'}(s_t)$。所以有

$$
\eqalign{
  & \nabla {{\bar R}_\theta } = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {{{{p_\theta }({s_t},{a_t})} \over {{p_{\theta '}}({s_t},{a_t})}}{A^{\theta '}}({s_t},{a_t})\nabla \log {p_\theta }({a_t}|{s_t})} \right]  \cr 
  &  = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {{{{p_\theta }({a_t}|{s_t}){p_\theta }({s_t})} \over {{p_{\theta '}}({a_t}|{s_t}){p_{\theta '}}({s_t})}}{A^{\theta '}}({s_t},{a_t})\nabla \log {p_\theta }({a_t}|{s_t})} \right]  \cr 
  &  = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {{{{p_\theta }({a_t}|{s_t})} \over {{p_{\theta '}}({a_t}|{s_t})}}{A^{\theta '}}({s_t},{a_t})\nabla \log {p_\theta }({a_t}|{s_t})} \right] \cr} 
$$
${{p_\theta }({a_t}|{s_t})}$ 很好算，策略网络的参数为 $\theta$，输入状态 $s_t$ 到策略网络中，它会输出每个 $a_t$ 的概率，所以只需要知道 $\theta$ 和 $\theta'$ 就可以计算。

由于 $\nabla f(x) = f(x)\nabla \log f(x)$，再加上对 $\theta$ 求梯度时，$p_{\theta'}(a_t|s_t)$ 和 $A^{\theta'}(s_t,a_t)$ 都是常数，因此要优化的目标函数为

$$
{J^{\theta '}}(\theta ) = {E_{\tau  \sim {p_\theta }(\tau )}}\left[ {{{{p_\theta }({a_t}|{s_t})} \over {{p_{\theta '}}({a_t}|{s_t})}}{A^{\theta '}}({s_t},{a_t})} \right]
$$

### 近端策略优化

为了避免 $p_{\theta}(a_t|s_t)$ 与 $p_{\theta'}(a_t|s_t)$ 相差太多，可以约束 $\theta$ 和 $\theta'$ 输出的动作的 KL 散度，使其更加相似。所以 PPO 算法的目标函数为

$$
J_{PPO}^{\theta '}(\theta ) = {J^{\theta '}}(\theta ) - \beta {\rm{KL}}(\theta ,\theta ')
$$

PPO 目标函数中加入了 KL 散度的约束，使得行为策略 $\theta'$ 和目标策略 $\theta$ 非常接近，PPO 的行为策略和目标策略可认为是同一个策略，因此PPO是同策略算法。


这里对于 $\beta$ 有一个自适应调整方法，先设置一个可以接受的 KL 散度最大值，如果 KL 散度太大，表示惩罚项 ${\rm{KL}}(\theta ,\theta ')$ 没有发挥作用，需要增大 $\beta$，如果 KL 散度比最小值还要小，就需要减小 $\beta$。


如果觉得计算KL散度计算复杂，还有一种裁剪方法

$$
J_{PPO}^{\theta '}(\theta ) = \sum\limits_{({s_t},{a_t})} {\min \left( {{{{p_\theta }({a_t}|{s_t})} \over {{p_{\theta '}}({a_t}|{s_t})}}{A^{\theta '}}({s_t},{a_t}),clip\left( {{{{p_\theta }({a_t}|{s_t})} \over {{p_{\theta '}}({a_t}|{s_t})}},1 - \varepsilon ,1 + \varepsilon } \right){A^{\theta '}}({s_t},{a_t})} \right)} 
$$

$\varepsilon$ 是一个超参数，可以取 0.1 或 0.2。


## 深度 Q 网络

传统的强化学习算法会使用表格的形式存储状态价值函数 $V(s)$ 或动作价值函数 $Q(s,a)$，但是这样的方法存在很大的局限性。例如，现实中的强化学习任务所面临的状态空间往往是连续的，存在无穷多个状态，在这种情况下，就不能再使用表格对价值函数进行存储。价值函数近似利用函数直接拟合状态价值函数或动作价值函数，降低了对存储空间的要求，有效地解决了这个问题。

为了在连续的状态和动作空间中计算值函数 $Q_{\pi}(s,a)$，可以用一个函数 $Q_{\phi}(s,a)$ 来近似计算，称为**价值函数近似**，$Q_{\phi}(s,a)$ 被称为 Q 网络。


### 状态价值函数

深度Q网络是基于价值的算法，在基于价值的算法里面，我们学习的不是策略，而是评论员。评论员的任务是评价现在的动作有多好或有多不好。假设有一个演员，其要学习一个策略来得到尽量高的回报。评论员就是评价演员的策略 $\pi$ 好还是不好，即策略评估。例如有一种评论员称为状态价值函数 $V_{\pi}$，用于表示从状态 s，接下来一直到游戏结束，期望的累积奖励有多大。



### 动作价值函数

还有另外一种评论员称为 Q 函数，又被称为动作价值函数，状态价值函数的输入是一个状态，而动作价值函数的输入是一个状态-动作对，指在某个状态采取某个动作，假设都使用策略 $\pi$，得到的累计奖励的期望值有多大。

Q 函数有一个需要注意的问题是，策略 $\pi$ 在看到状态 $s$ 的时候，它采取的动作不一定是 $a$。Q函数假设在状态 $s$ 强制采取动作 $a$，而不管我们现在考虑的策略 $\pi$ 会不会采取动作 $a$，这并不重要。在状态 $s$ 强制采取动作 $a$。接下来都用策略 $\pi$ 继续玩下去，就只有在状态 $s$，我们才强制一定要采取动作 $a$，接下来就进入自动模式，让策略 $\pi$ 继续玩下去，得到的期望奖励才是$Q_{\pi}(s,a)$。

Q 函数有两种写法：（1）输入是状态与动作，输出就是一个标量。这种Q函数既适用于连续动作（动作是无法穷举的），又适用于离散动作。（2）输入是一个状态，输出就是多个值。这种Q函数只适用于离散动作。

一开始先随机初始化一个策略 $\pi$，$\pi$ 与环境交互，收集数据，接下来计算 $\pi$ 的 Q 值，去衡量一下 $\pi$ 在某一个状态强制采取某一个动作，接下来会得到的期望奖励。学习出一个 Q 函数，就能找到一个新策略 $\pi'$，策略$\pi'$ 会比 $\pi$ 更好，然后根据 $\pi'$ 学习 Q 函数，如此循环下去。新策略 $\pi'$ 的公式为

$$
\pi '(s) = \mathop {\arg \max }\limits_a {Q_\pi }(s,a)
$$
假设我们已经学习出 $\pi$ 的 Q 函数，在某一个状态 $s$，把所有可能的动作 $a$ 一一代入Q 函数，看看哪一个 $a$ 可以让 Q 函数的值最大，这个动作就是 $\pi'$ 会采取的动作。


### 目标网络


在状态 $s_t$ 采取动作 $a_t$ 以后，得到奖励 $r_t$，进入状态 $s_{t+1}$，根据 Q 函数，有

$$
{Q_\pi }({s_t},{a_t}) = {r_t} + {Q_\pi }({s_{t + 1}},\pi ({s_{t + 1}}))
$$

这个公式说明 Q 函数输入 $s_t$、$a_t$ 得到的值，与输入 $s_{t+1}$、$\pi(s_t+1)$ 得到的值之间，我们希望它们相差 $r_t$，但是这样并不容易训练，训练不稳定。通常做法分成两个 Q 网络，两个网络的初始参数相同，先将右边的 Q 网络固定，称之为目标网络，再训练左边 Q 网络的参数，左边 Q 网络更新多次后，再用更新过的 Q 网络替换目标网络，继续训练左边的 Q 网络，损失直接使用MSE损失即可。


### 探索

使用 $\varepsilon$ - 贪心选择动作，

$$
a = \left\{ \matrix{
  \mathop {\arg \max }\limits_a Q(s,a),\quad p = 1 - \varepsilon  \hfill \cr 
  random\quad \quad p = \varepsilon  \hfill \cr}  \right.
$$
另一种方法称为玻尔兹曼探索，对于任意的 $s$，$a$，$Q(s,a)\ge0$，因此 $a$ 被选中的概率与 $e^{Q(s,a)/T}$ 成正比，即

$$
\pi (a|s) = {{{e^{Q(s,a)/T}}} \over {\sum\nolimits_{a' \in A} {{e^{Q(s,a')/T}}} }}
$$

T 称为温度系数，如果 T 很大，那么所有动作几乎以等概率选择，如果 T 很小，Q 值大的动作更容易被选中。


### 经验回放

经验回放会构建一个回放缓冲区，回放缓冲区又被称为回放内存。回放内存中的一条数据指之前在某一个状态 $s_t$，采取某一个动作 $a_t$，得到了奖励 $r_t$，进入状态 $s_{t+1}$。回放缓冲区里面的经验可能来自不同的策略，我们每次用 $\pi$ 与环境交互的时候， 可能只交互10000 次，接下来我们就更新 $\pi$了。但是回放缓冲区里面可以放5万条数据，所以5万条数据可能来自不同的策略。训练时，从回放缓存区中随机挑选一个batch，然后用这个batch去更新 Q 网络。



### 深度Q网络

下面可以总结深度 Q 网络算法了

+ 初始化网络 $Q$ 和目标网络 $Q_t$，令 $Q_t=Q$

+ 对于每一个回合
	+ 对于每个时间步
		+ 对于给定的状态 $s_t$，基于 Q（$\varepsilon$-贪婪）执行动作 $a_t$
		+ 获得反馈 $r_t$，并获得新的状态 $s_{t+1}$
		+ 将 $(s_t,a_t,r_t,s_{t+1})$ 存储到缓冲区
		+ 从缓冲区采样一个batch
		+ 目标值是 $y = {r_i} + \mathop {\max }\limits_a {Q_t}({s_{i + 1}},a)$
		+ 更新 Q 的参数使得 $Q(s_i,a_i)$ 尽可能接近于 y
		+ 每 C 次更新重置 $Q_t=Q$



## 深度 Q 网络进阶技巧



### 双深度 Q 网络


普通的深度 Q 网络往往存在Q值高估的问题，这主要是因为 Q 网络每次都选最大Q值的动作，假如一个动作被高估了，那么就有很大可能被选中。双深度Q网络解决这个问题的方法是，选动作的 Q 函数和计算值的 Q 函数不是同一个，DDQN中有两个 Q 网络 — $Q$ 和 $Q'$。如果 $Q$ 高估了选出来的动作 $a$，只要 $Q'$ 没有高估就行，假设 $Q'$ 高估了某个动作的值，只要 $Q$ 不选这个动作就可以

$$
Q({s_t},{a_t}) \leftrightarrow {r_t} + Q'\left( {{s_{t + 1}},\mathop {\arg \max }\limits_a Q({s_{t + 1}},a)} \right)
$$

在DDQN里面，我们会用会更新参数的 Q 网络去选动作，用目标 Q 网络（固定住的网络）计算值。

DDQN 相较于原来的深度 Q 网络的更改是最少的，它几乎没有增加任何的运算量，也不需要新的网络，因为原来就有两个网络。我们只需要做一件事：本来是用目标网络 $Q'$ 来找使 $Q$ 值最大的 $a$，现在改成用另外一个会更新的 $Q$ 网络来找使 $Q$ 值最大的 $a$。如果只选一个技巧，我们一般都会选DDQN，因为其很容易实现。


### 竞争深度 Q 网络

竞争深度 Q 网络改变了原来 Q 网络的架构，原本是输入状态，输出每个动作的 Q 值。竞争深度 Q 网络则分成两部分，第一部分输出标量 $V(s)$，第二部分输出一个向量 $A(s,a)$，最终的 Q 值是两者之和。这种网络的优点是由于 $V(s)$ 是一个标量，那么即使一些动作没有被采样到，也可以通过 $V(s)$ 修改这些动作的 Q 值，更加高效地估计 Q 值。

为了避免竞争深度 Q 网络训练到最后，$V(s)$ 为0，$A(s,a) = Q(s,a)$，需要对$A(s,a)$ 加一些约束，如让 $\sum\limits_a {A(s,a)}  = 0$，这样网络会被强迫学习 $V(s)$。具体操作是先计算每一列的均值，再将这一列减去均值，这样一列之和为0。


### 优先级经验回放

原本在采样数据时，是均匀地从回放缓冲区采样数据。优先级是根据训练时的时序差分误差（网络输出和目标之间的差距），如果误差大，那么代表这些数据不容易训练，应该更容易被采样到。


### 在蒙特卡洛方法和时序差分方法中取得平衡

为了结合蒙特卡洛方法和时序差分方法的优点，可以使用多步方法。我们记录在 $s_t$ 采取 $a_t$，得到 $r_t$ 时，会进入的 $s_{t+1}$。一直记录到第 N 个步骤以后，在 $s_{t+N}$ 采取 $a_{t+N}$，得到 $r_{t+N}$，进入 $s_{t+N+1}$ 的这些经验，把它们保存下来。实际上在做更新的时候，在做 Q 网络学习的时候，我们要让 $Q(s_t, a_t)$ 与目标值越接近越好。$\hat Q$ 所计算的不是 $s_{t+1}$ 的，而是 $s_{t+N+1}$ 的奖励。我们会把 N 个步骤以后的状态 $s_{t+N+1}$ 输入到 $\hat Q$ 中去计算 N 个步骤以后会得到的奖励。如果要算目标值，要再加上多步（multi-step）的奖励 $\sum_{t'=t}^{t+N} r_{t'}$ ，多步的奖励是从时间 t 一直到 t+N 的 N+1 个奖励的和。我们希望 $Q(s_t,a_t)$ 和目标值越接近越好。


### 噪声网络

噪声网络用来改进探索，噪声网络指在参数空间加上噪声，每一次在一个回合开始的时候，当智能体要与环境交互的时候，智能体使用 Q 函数来采取动作，Q 函数里面就是一个网络，我们在网络的每一个参数上加上一个高斯噪声，就把原来的 $Q$ 函数变成 ${\tilde Q}$。我们把每一个参数都加上一个高斯噪声，就得到一个新的网络 ${\tilde Q}$。使用噪声网络执行的动作为

$$
a = \mathop {\arg \max }\limits_a \tilde Q(s,a)
$$
注意在同一个回合中，参数虽然会被加上噪声，但是同个回合里面参数是固定的，只有换回合时才会重新采样噪声。

相比于 $\varepsilon$ - 探索，噪声网络可以做到面对同一个状态，动作的选择也是一样的，更符合实际情况。

### 分布式Q 函数

分布式 Q 函数是对分布建模，具体做法是：假设分布的值就分布在某一个范围里面，将范围分成一个个小区间，Q函数的输出就是要预测我们在某一个状态采取某一个动作得到的奖励，其落在某一个长条里面的概率。在做测试的时候，我们选平均值最大的动作执行。


### 彩虹

将各种方法结合起来，每个方法训练，取每个方法分数的中位数。



## 针对连续动作的深度 Q 网络


处理连续动作的几种方法

### 对动作进行采样

我们可以采样出 N 个可能的 $a: \{a_1, a_2, \cdots , a_N\}$ ，把它们一个一个地代入 Q 函数，看谁的 Q 值最大。


### 梯度上升

把动作 $a$ 当作参数，用 $a$ 最大化 Q 函数，然后用梯度上升去更新 $a$ 的值，但是这样计算量会比较大。


### 设计网络架构

通常 Q 网络的输入是 s，可以将其改成输入为 s 和 a，输出为向量 $\mu(s)$、矩阵 $\Sigma (s)$ 和标量 $V(s)$，将 Q 函数的定义改为

$$
Q(s,a) =  - {\left( {a - \mu (s)} \right)^T}\Sigma (s)\left( {a - \mu (s)} \right) + V(s)
$$

注意 $a$ 此处是连续动作，所以是一个向量，$\Sigma (s)$ 是一个正定矩阵。当 $a=\mu(s)$ 时，Q 值最大，所以直接将 $\mu(s)$ 作为动作。


### 不使用深度 Q 网络

使用演员—评论员方法


## 演员—评论员算法

演员-评论员算法是一种结合策略梯度和时序差分学习的强化学习方法，其中，演员是指策略函数 $\pi_{\theta}(a|s)$，即学习一个策略以得到尽可能高的回报。评论员是指价值函数 $V_{\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。借助于价值函数，演员-评论员算法可以进行单步参数更新，不需要等到回合结束才进行更新。在演员-评论员算法里面，最知名的算法就是异步优势演员-评论员算法。如果我们去掉异步，则为优势演员-评论员（advantage actor-critic，A2C）算法。A2C 算法又被译作优势演员-评论员算法。如果我们加了异步，变成异步优势演员-评论员算法（A3C）。


### 优势演员-评论员算法（A2C）

在策略梯度算法中，更新策略参数 $\theta$ 时可以通过

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {\sum\limits_{t' = t}^{{T_n}} {{\gamma ^{t' - t}}r_{t'}^n}  - b} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$
来计算梯度，其中我们可以用 G 来表示累积奖励，即

$$
G = \sum\limits_{t' = t}^{{T_n}} {{\gamma ^{t' - t}}r_{t'}^n}  - b
$$

G 是非常不稳定的，因此可以堪称一个随机变量，深度 Q 网络的目的便是估测在状态 s 采取动作 a 时 G 的期望值。随机变量 G 的期望值正好就是 Q 值，即

$$
E\left[ {G_t^n} \right] = {Q_{{\pi _\theta }}}\left( {s_t^n,a_t^n} \right)
$$

所以假设用 $E\left[ {G_t^n} \right]$ 代表 $\sum\limits_{t' = t}^{{T_n}} {{\gamma ^{t' - t}}r_{t'}^n}$ 这一项，对于基线有不同的方法表示，一个常见的方法是利用价值函数 ${V_{{\pi _\theta }}}(s_t^n)$ 表示基线，把策略梯度里面 $\sum\limits_{t' = t}^{{T_n}} {{\gamma ^{t' - t}}r_{t'}^n}  - b$ 换成 ${Q_{{\pi _\theta }}}\left( {s_t^n,a_t^n} \right) - {V_{{\pi _\theta }}}(s_t^n)$，这就是优势演员—评论员算法

$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {{Q_{{\pi _\theta }}}\left( {s_t^n,a_t^n} \right) - {V_{{\pi _\theta }}}(s_t^n)} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$
为了避免同时估计两个网络，将 ${Q_{{\pi _\theta }}}\left( {s_t^n,a_t^n} \right)$ 改写为 $r_t^n + {V_\pi }(s_{t + 1}^n)$ 的期望值，去掉期望值也可以，最终得到
$$
\nabla {{\bar R}_\theta } \approx {1 \over N}\sum\limits_{n = 1}^N {\sum\limits_{t = 1}^{{T_n}} {\left( {r_t^n + {V_\pi }(s_{t + 1}^n) - {V_{{\pi _\theta }}}(s_t^n)} \right)\nabla \log {p_\theta }(a_t^n|s_t^n)} } 
$$

在优势演员-评论员算法中，先用策略 $\pi$ 和环境交互得到数据，利用数据使用时序差分和蒙特卡洛的得到 $V_{\pi}(s)$ ，再根据 $V_{\pi}(s)$ 更新策略。

实现优势演员-评论员算法的时候，有两个一定会用到的技巧。第一个技巧是，我们需要估计两个网络：V 网络和策略的网络（也就是演员）。评论员网络 $V_{\pi}(s)$ 接收一个状态，输出一个标量。演员的策略 $\pi (s)$ 接收一个状态，如果动作是离散的，输出就是一个动作的分布，如果动作是连续的，输出就是一个连续的向量。演员网络和评论员网络的输入都是 s，所以它们前面几个层是可以共享的。

第二个技巧是我们需要探索的机制。在演员-评论员算法中，有一个常见的探索的方法是对 $\pi$ 输出的分布设置一个约束。这个约束用于使分布的熵（entropy）不要太小，也就是希望不同的动作被采用的概率平均一些。这样在测试的时候，智能体才会多尝试各种不同的动作，才会把环境探索得比较好，从而得到比较好的结果。


### 异步优势演员-评论员算法（A3C）

异步优势演员-评论员算法同时使用很多个进程（worker），异步优势演员-评论员算法一开始有一个全局网络。全局网络包含策略网络和价值网络，这两个网络是绑定在一起的，它们的前几个层会被绑在一起。假设全局网络的参数是 $\theta_1$，我们使用多个进程，每个进程用一张 CPU 去跑。比如我们有 8 个进程，则至少 8 张 CPU。每一个进程在工作前都会把全局网络的参数复制过来。接下来演员就与环境交互，每一个演员与环境交互的时候，都要收集到比较多样的数据。例如，如果是走迷宫，可能每一个演员起始的位置都会不一样，这样它们才能够收集到比较多样的数据。每一个演员与环境交互完之后，我们就会计算出梯度。计算出梯度以后，要用梯度去更新参数。我们就计算一下梯度，用梯度去更新全局网络的参数。就是这个进程算出梯度以后，就把梯度传回给中央的控制中心，中央的控制中心就会用这个梯度去更新原来的参数。


### 路径衍生策略梯度

这个方法可以看成是深度 Q 网络求解连续动作的一种特别方法。假设我们学习了一个 Q 函数，Q 函数的输入是 $s$ 与 $a$，输出是 $Q_{\pi}(s, a)$。接下来，我们要学习一个演员，这个演员的工作就是解决 arg max 的问题，即输入一个状态 $s$，希望可以输出一个动作 $a$。$a$ 被代入Q 函数以后，它可以让 $Q_{\pi}(s, a)$ 尽可能大，即

$$
\pi '(s) = \mathop {\arg \max }\limits_a {Q_\pi }(s,a)
$$
实际上在训练的时候，我们就是把 Q 与演员连接起来变成一个比较大的网络。Q 是一个网络，接收输入 s 与 a，输出一个值。演员在训练的时候，它要做的事就是接收输入 s，输出 a。把 a 代入 Q 中，希望输出的值越大越好。我们会固定住 Q 的参数，只调整演员的参数，用梯度上升的方法最大化 Q 的输出，这就是一个生成对抗网络，即有条件的生成对抗网络。

一开始会有一个策略 $\pi$，它与环境交互并估计 Q 值。估计完 Q 值以后，我们就把 Q 值固定，只去学习一个演员。假设这个 Q 值估得很准，它知道在某一个状态采取什么样的动作会得到很大的 Q 值。接下来就学习这个演员，演员在给定 s 的时候，采取了 a，可以让最后 Q 函数算出来的值越大越好。我们用准则去更新策略 $\pi$，用新的 $\pi$ 与环境交互，再估计 Q 值，得到新的 $\pi$ 去最大化 Q 值的输出。深度 Q 网络里面的技巧，在这里也几乎都用得上，比如经验回放、探索等技巧。

路径衍生策略梯度算法：

+ 初始化 Q 函数 $Q$，目标 Q 函数 $\hat Q=Q$，演员 $\pi$，目标演员 $\hat \pi=\pi$ 
+ 在每个回合中
	+ 对于每个时间步
		+ 获取状态 $s_t$，根据 $\pi$ 执行动作 $a_t$ （探索）
		+ 获取奖励 $r_t$，到达新状态 $s_{t+1}$
		+ 存储 $(s_t,a_t,r_t,s_{t+1})$ 到缓冲区
		+ 从缓存区采样 $(s_t,a_t,r_t,s_{t+1})$ （通常是一个批量）
		+ 目标 $y = {r_i} + \hat Q({s_{i + 1}},\hat \pi ({s_{i + 1}}))$
		+ 更新 Q 的参数使得 $Q(s_i,a_i)$ 接近于 y（回归）
		+ 更新 $\pi$ 的参数使得 $Q({s_i},\pi ({s_i}))$ 最大
		+ 每 C 步重置 $\hat Q = Q$，$\hat\pi=\pi$


## 稀疏奖励

许多情况下，智能体在环境中获得的奖励是非常稀疏的，所以需要考虑如何在稀疏奖励的情况下与环境交互。


### 设计奖励

环境有一个固定的奖励，它是真正的奖励，但是为了让智能体学到的结果是我们想要的，我们可以设计一些奖励来引导智能体。如 Meta 玩 ViZDoom 的智能体的例子。ViZDoom 是一个第一人称射击游戏，在这个射击游戏中，杀了敌人得到正奖励，被杀得到负奖励。研究人员设计了一些新的奖励，用新的奖励来引导智能体让它们做得更好，这不是游戏中真正的奖励。比如掉血就扣分，弹药减少就扣分，捡到补给包就加分，待在原地就扣分，移动就加分。活着会扣一个很小的分数，因为如果不这样做，智能体会只想活着，一直躲避敌人，这样会让智能体好战一些。

### 好奇心

在网络加入一个新的奖励函数—内在好奇心模块，用来给智能体加上好奇心，内在好奇心模块需要 3 个输入：状态 $s_1$、动作 $a_1$ 和状态 $s_2$。根据输入，会输出另外一个奖励 $r_1^i$。对于智能体而言，总奖励并不是只有 r，还有 $r^i$。它不是只把所有的 r 都加起来，它还把所有 $r^i$ 加起来当作总奖励。所以在与环境交互的时候，它不是只希望 r 越大越好，它还同时希望 $r^i$ 越大越好，它希望从内在好奇心模块里面得到的奖励越大越好。内在好奇心模块代表一种好奇心。

内在好奇心模块最原始的设计为：输入现在状态 $s_t$，动作 $a_t$ 和下一个状态 $s_{t+1}$，模块里面有一个网络，这个网络的输入是现在状态 $s_t$ 和动作 $a_t$，输出 $\hat s_{t+1}$，接下来比较 $\hat s_{t+1}$ 和 $s_{t+1}$ 的相似度，越不相似得到的奖励越大，即未来的状态越难被预测，得到的奖励越大。这个网络需要额外训练，在智能体与环境交互时，内在好奇心模块需要被固定住。

为了避免内在好奇心模块被一些没有意义的东西吸引，可以增加一个特征提取网络，该网络用来过滤无关紧要的东西。具体训练方法是把向量 $\phi(s_t)$ 和 $\phi(s_{t+1})$ 作为输入，它要预测动作 a 是什么，希望这个动作 a 和真正的动作 a 越接近越好，这样我们提取的特征就会和一些没有意义的东西无关。


### 课程学习

课程学习是指我们为智能体的学习做规划，给他“喂”的训练数据是有顺序的，通常都是由简单到难的。课程设计有一个比较通用的方法：逆向课程生成，从最后最理想的状态开始，一步步寻找接近这些状态的状态。


### 分层强化学习

假设有多个智能体，一些智能体负责比较高级的东西，它们负责定目标，定完目标以后，再将目标分配给其他的智能体，让其他智能体来执行目标。


## 模仿学习

在多数情况下，智能体无法从环境中得到明确的奖励，因此可以通过收集人类的示例来让智能体模仿学习。

### 行为克隆

行为克隆与监督学习较为相似，人在状态 $s_i$ 时采取动作 $a_i$，在训练网络时，我们希望输入状态 $s_i$ 时输出为 $a_i$。虽然行为克隆简单，但是难以收集全部的示例，所以需要收集更多样的数据。


### 逆强化学习

原本的强化学习中有一个环境和奖励函数，根据环境和奖励函数会学习一个最佳策略。但在逆强化学习中没有奖励函数，只有专家的示范和环境。逆强化学习的奖励是从专家那里推出来，专家是因为什么样的奖励函数才会采取这些行为。有了奖励函数以后，接下来，我们就可以使用一般的强化学习的方法去找出最优策略。所以逆强化学习是先找出奖励函数，找出奖励函数以后，再用强化学习找出最优策略。


## 深度确定性策略梯度

深度确定性策略梯度（deep deterministic policy gradient，DDPG）可以拆解成深度、确定性和策略梯度，深度是因为用了神经网络，确定性表示输出的是确定性的动作，可以用于连续动作环境，策略梯度代表用的是策略网络。REINFORCE 算法每隔一个回合就更新一次，但 DDPG 是每个步骤都会更新一次策略网络，它是一个单步更新的策略网络。

DDPG 是深度 Q 网络的一个扩展版本，可以扩展到连续动作空间。在 DDPG 的训练中，它借鉴了深度 Q 网络的技巧：目标网络和经验回放。经验回放与深度 Q 网络是一样的，但目标网络的更新与深度 Q 网络不一样。提出 DDPG 是为了让深度 Q 网络可以扩展到连续的动作空间。DDPG 在深度 Q 网络基础上加了一个策略网络来直接输出动作值，所以 DDPG 需要一边学习 Q 网络，一边学习策略网络。Q 网络的参数用 $w$ 来表示。策略网络的参数用 $\theta$ 来表示。我们称这样的结构为演员-评论员的结构。

DDPG 与深度 Q 网络最大区别在于深度 Q 网络选择使得 Q 值最大的动作，而 DDPG 则有一个策略网络选择动作，策略网络 $\pi$ 也需要一个另外设置目标网络 $\pi_t$ 来使得训练更加稳定。为了然那个DDPG策略更好地探索，训练时给动作加上噪声（不相关、均值为 0 的高斯噪声效果好），训练过程中噪声也可以逐步变小。

DDPG 的算法如下：

+ 初始化演员网络 $\mu(s|\theta^{\mu})$ 和评价员网络 $Q(s,a|\theta^{Q})$， 
+ 初始化对应的目标网络 $\mu'$ 和 $Q'$，并且复制权重
+ 初始化经验回访缓冲区 D
+ 执行M个回合，在每个回合中
	+ 在每个时间步
		+ 根据当前的策略和噪声选择动作 $a_t=\mu(s_t|\theta^{\mu})+N_t$
		+ 环境根据 $a_t$ 反馈奖励 $r_t$ 和下一个状态 $s_{t+1}$
		+ 存储转移即 $(s_t,a_t,r_t,s_{t+1})$ 到经验回放区D
		+ 更新策略如下
			+ 从D中随机采样N个小批量的数据
			+ 计算实际Q值：${y_i} = {r_i} + \gamma Q'\left( {{s_{i + 1}},\mu '({s_{i + 1}}|{\theta ^{\mu '}})|{\theta ^{Q'}}} \right)$
			+ 计算损失函数 $L = {1 \over N}\sum\nolimits_i {{{\left( {{y_i} - Q({s_i},{a_i}|{\theta ^Q})} \right)}^2}}$，更新评论员网络
			+ 计算损失函数 $L =  - {1 \over N}\sum\nolimits_i {Q({s_i},{a_i})}$，更新演员网络
			+ 软更新目标网络
			+ ${\theta ^{Q'}} \leftarrow \tau {\theta ^Q} + (1 - \tau ){\theta ^{Q'}},{\theta ^{\mu '}} \leftarrow \tau {\theta ^\mu } + (1 - \tau ){\theta ^{\mu '}}$


### 双延迟深度确定性策略梯度（TD3）

虽然 DDPG 有时表现很好，但它对于超参数和其他类型的调整方面经常很敏感。双延迟深度确定性策略梯度（twin delayed DDPG，TD3）通过引入 3 个关键技巧来解决这个问题。

+ 截断的双 Q 学习：TD3 学习两个 Q 函数，通过最小化均方误差来同时学习两个 Q 函数：$Q_{\phi_1}$ 和 $Q_{\phi_2}$，两个 Q 函数都使用同个目标，两个 Q 函数中给出的较小的值会被作为 Q-target
$$
y(r,s',d) = r + \gamma (1 - d)\mathop {\min }\limits_{i = 1,2} {Q_{{\phi _i}_t}}(s',{a_{TD3}}(s'))
$$
+ 延迟的策略更新（delayed policy updates）。相关实验结果表明，同步训练动作网络和评价网络，却不使用目标网络，会导致训练过程不稳定；但是仅固定动作网络时，评价网络往往能够收敛到正确的结果。因此 TD3 算法以较低的频率更新动作网络，以较高的频率更新评价网络，通常每更新两次评价网络就更新一次策略。
+ 目标策略平滑（target policy smoothing）。TD3 引入了平滑化（smoothing）思想。TD3 在目标动作中加入噪声，通过平滑 Q 沿动作的变化，使策略更难利用 Q 函数的误差。工作原理如下
$$
{a_{TD3}}(s') = {\rm{clip}}\left( {{\mu _{\theta t}}(s') + {\rm{clip}}(\varepsilon , - c,c),{a_{low}},{a_{high}}} \right)
$$
其中 ϵ 本质上是一个噪声，是从正态分布中取样得到的，即 $\varepsilon \sim N(0, \sigma)$。目标策略平滑化是一种正则化方法。

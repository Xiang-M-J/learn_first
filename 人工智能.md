# D2L

## 编程小技巧

1. 通过**原位操作**减小占用

```python
 before = id(Y)    
 Y = Y + X
 id(Y) == before   # False
```

注意 `Y = Y + X` 会为 `Y` 分配一个新的内存，换成下面中的一个会更好

```python
# 这两种方式均不会为Y分配新内存
Y[:] = X + Y  
Y = Y + X
```


2. `nn.LazyLinear()`：功能与`nn.Linear()`相同，不过只需要给定输出维度，输入维度可以自动推断。


## 神经网络
### 反向传播
反向传播时，需要计算最后得到的结果是一个标量，或者提供向量 `v` （下面的代码中 `gradient` 即为 `v`）使得反向传播时计算 $v^T\partial_x y$（其实还是一个标量），而不是$\partial_x y$。

```python
x = torch.arange(4.0)
x.requires_grad_(True)
# x.grad.zero_()
y = x * x 
y.backward(gradient=torch.ones(len(y))) # Faster: y.sum().backward()
x.grad  # tensor([0., 2., 4., 6.])
```

如果一个值只是中间值，不需要计算梯度，可以使用 `detach` 来截断反向传播

```python
x = torch.arange(4.0)
x.requires_grad_(True)
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad   # 由于detach截断了反向传播，所以x的梯度为u，而不是 3x^2
```

> 一个简单的线性拟合

```python
import torch
import matplotlib.pyplot as plt

x = torch.arange(100.0, requires_grad=True)
w, b = torch.tensor(1., requires_grad=True), torch.tensor(0.,requires_grad=True)
optimizer = torch.optim.SGD(params=[w, b], lr=1e-4)
loss = torch.nn.MSELoss()
for i in range(10):
    y = 2.138*x + 3.213
    y_ = w * x + b
    L = loss(y, y_)
    optimizer.zero_grad()
    L.backward()
    optimizer.step()

plt.plot(x.detach().numpy(), y.detach().numpy())
plt.plot(x.detach().numpy(), y_.detach().numpy())
plt.show()
```

sgd算法的简单实现
```python
def sgd(params, lr, batch_size): 
    #@save """小批量随机梯度下降""" 
    with torch.no_grad(): 
        for param in params: 
            param-= lr * param.grad / batch_size 
            param.grad.zero_()
```

### Covariate Shift 纠正

Covariate Shift 指训练数据（q(x)）和测试数据的分布（p(x)）存在差异，导致模型在测试集上表现变差。幸运的是，独立性假设表明条件分布没有变，即：$p(y|x)=q(y|x)$。对于模型的风险，有下面的等式

$$
\int\!\!\!\int {l(f({\bf{x}}),y)p(y|{\bf{x}})p({\bf{x}})} d{\bf{x}}dy = \int\!\!\!\int {l(f({\bf{x}}),y)q(y|{\bf{x}})q({\bf{x}}){{p({\bf{x}})} \over {q({\bf{x}})}}} d{\bf{x}}dy
$$
换句话说，我们需要根据从正确分布（p(x)）中抽取的概率与从错误分布（q(x)）中抽取的概率之比来重新权衡每个数据。令$\beta_i$为
$$
{\beta _i}\mathop  = \limits^{def} {{p({{\bf{x}}_i})} \over {q({{\bf{x}}_i})}}
$$
这样的话，可以将模型的目标函数改写为
$$
\mathop {\min }\limits_f {1 \over n}\sum\limits_{i = 1}^n {{\beta _i}l(f({{\bf{x}}_i}),{y_i})} 
$$
这里的$\beta_i$并不知道，下面介绍一种借助逻辑回归的算法。假设我们有一个训练集  $\{(x_1, y_1), \cdots ,(x_n,y_n)\}$ 和一个未标记的测试集 $\{u_1, \cdots, u_m\}$。对于 Covariate Shift，我们假设所有 1≤i ≤ n 的 $x_i$ 都来自某个源分布，而所有 1≤ i≤ m 的 $u_i$ 来自目标分布。下面是用于校正 Covariate Shift 的原型算法：
1. 创建一个二分类训练集：$\{(x_1,-1),\cdots, (x_n,-1),(u_1,1),\cdots,(u_m, 1)\}$
2. 使用逻辑回归训练一个二分类器，得到分类函数 h
3. $\beta_i=exp(h(x_i))$，或者 $\beta_i= \min (exp(h(x_i)), c)$（c 为一个固定常数）


### dropout

在标准 dropout 正则化中，将每层中部分节点归零，然后通过按保留（未丢弃）的节点分数归一化来消除每一层的偏差。即将每个中间激活值 $h$ 替换成随机值 $h'$，$h'$取值如下：
$$
h' = \left\{ \matrix{
  0\quad {\rm{with}}\;{\rm{probability}}\;p \hfill \cr 
  {h \over {1 - p}}\quad {\rm{otherwise}} \hfill \cr}  \right.
$$
之所以要除1-p，是为了保证 $E[h']=h$。

具体实现
```python
def dropout(X, p):
    assert 0<= p <= 1
    if p==1: return torch.zeros_like(X)
    mask = (torch.rand(X.shape) > p).float()
    return mask * X / (1.0 - p)
```


### 卷积层

```python
def corr2d(X, K):
    h, w = K.shape
    Y = torch.zeros(X.shape[0] - h + 1, X.shape[1] - w + 1)
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()
    return Y

class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.w = nn.Parameter(torch.rand(kernel_size))
        self.b = nn.Parameter(torch.zeros(1))
    def forward(self, x):
        return corr2d(x, self.w) + self.b
```

池化层的实现与卷积层相似，只不过需要改变 `Y[i,j]` 的计算公式。


### RNN

考虑一个自回归模型，对时序数据，估计 $P(x_t|x_{t-1},\cdots, x_1)$，有下面两种方法
1. 自回归模型
使用观测序列 $x_{t−1},\cdots,x_{t−\tau}$ 来估计 $x_t$。
2. 隐变量自回归模型
保留一些对过去观测的总结 $h_t$，并且同时更新预测 $\hat x_t$ 和总结 $h_t$。其中，$\hat x_t = P(x_t|h_t)$，$h_t=g(h_{t-1},x_{t-1})$。

RNN 中也包含了对于过去观测的总结 $h_t$，与一般的 MLP 不同，RNN 的隐层计算公式如下。
$$
H_t = \phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)
$$
一般的MLP为 $H_t = \phi(X_tW_{xh}+b_h)$，其中 $\phi$ 为激活函数。
对于 RNN 训练语言模型时，可以将输入序列向后移动一位作为输出序列。
对于训练好的 RNN 模型，输入文本，预测后面文本的代码如下：
```python
def predict(self, prefix, num_preds, vocab, device=None):
    state, outputs = None, [vocab[prefix[0]]]  # 一开始state为空
    for i in range(len(prefix) + num_preds - 1):
        X = torch.tensor([[outputs[-1]]], device=device)
        embs = self.one_hot(X)
        rnn_outputs, state = self.rnn(embs, state)
        if i < len(prefix) - 1:  # Warm-up period
            outputs.append(vocab[prefix[i + 1]])
        else:  # Predict num_preds steps
            Y = self.output_layer(rnn_outputs)
            outputs.append(int(Y.argmax(axis=2).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

这里 self.output_layer 是将 RNN 输出的隐层状态与系数相乘后堆叠在一起

```python
def output_layer(self, rnn_outputs):
    outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]
    return torch.stack(outputs, 1)
def forward(self, X, state=None):
    embs = self.one_hot(X)
    rnn_outputs, _ = self.rnn(embs, state)
    return self.output_layer(rnn_outputs)
```

RNN 的梯度更新（BPTT算法）：[9.7. Backpropagation Through Time — Dive into Deep Learning 1.0.3 documentation (d2l.ai)](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)

在 seq2seq 模型的输出时，每个时间步都对应着不同的词元及其概率，这时候需要确定一条路径使得选择具有最高条件概率的词元。如下图所示，每个时间步对应了四种词元，当输出 `<eos>` 时，停止输出。

| 时间步  | 1    | 2    | 3    | 4    |
| ------- | ---- | ---- | ---- | ---- |
| A       | 0.5  | 0.1  | 0.2  | 0    |
| B       | 0.2  | 0.4  | 0.2  | 0.2  |
| C       | 0.2  | 0.3  | 0.4  | 0.2  |
| `<eos>` | 0.1  | 0.2  | 0.2  | 0.6  |

**贪心搜索**直接在每个时间步选择具有最高条件概率的词元，即输出为 `A B C <eos>`。贪心算法虽然简单，但是不能保证得到最优序列，因为每个时间步的输出和前面时间步的输出有关。可能第2步选择了 C 词元会导致条件概率变为

| 时间步  | 1    | 2    | 3    | 4    |
| ------- | ---- | ---- | ---- | ---- |
| A       | 0.5  | 0.1  | 0.1  | 0.1  |
| B       | 0.2  | 0.4  | 0.6  | 0.2  |
| C       | 0.2  | 0.3  | 0.2  | 0.1  |
| `<eos>` | 0.1  | 0.2  | 0.1  | 0.6  |

此时如果输出 `A C B <eos>` 的条件概率比贪心搜索时更高。


**束搜索**（beamsearch）是贪心搜索的一个改进版本。它有一个超参数，名为束宽 k。在时间步1，选择具有最高条件概率的 k 个词元。这 k 个词元将分别是 k 个候选输出序列的第一个词元。在随后的每个时间步，基于上一时间步的 k 个候选输出序列，我们将继续从 ky（y为当前时间步对应的可选词元个数）个可能的选择中挑出具有最高条件概率的 k 个候选输出序列。

假设输出的词表只包含五个元素，束宽为2，输出序列的最大长度为3。每一步选择的词元如下：
第一步：A C
第二步：B E
第三步：D D
最后将会有 6 种可能，分别为：（1）A；（2） C；（3）A,B；（4）C,E；（5）A,B,D；（6）C,E,D。

最后，基于这六个序列（例如，丢弃包括`<eos>`和之后的部分），我们获得最终候选输出序列集合。然后 我们选择其中条件概率乘积最高的序列作为输出序列：
$$
{1 \over {{L^\alpha }}}\log P\left( {{y_1}, \cdots ,{y_L}|{\bf{c}}} \right) = {1 \over {{L^\alpha }}}\sum\limits_{t' = 1}^L {\log P\left( {{y_{t'}}|{y_1}, \cdots ,{y_{t' - 1}},{\bf{c}}} \right)} 
$$
其中 L 是最终候选序列的长度，$\alpha$ 通常设置为0.75。因为一个较长的序列在求和中会有更多的对数项，因此分母中的 $L^{\alpha}$ 用于惩罚长序列。

### 优化算法

**鞍点**指函数梯度为0的点，且该点既不是局部最小点也不是全局最小点。如 $f(x)=x^3$ 的 $x=0$ 的点就是鞍点。

**凸集**指在向量空间中的一个集合 $X$，如果对于任意 $a,b\in X$，连接 $a$ 和 $b$ 的线段也在 $X$ 中，即对于 $\lambda\in[0,1]$，有
$$
\lambda a + (1-\lambda)b\in X
$$

一般情况下，深度学习中的问题被定义为凸集，如一个 d 维的实数集 $R^d$ 是一个凸集。

**凸函数** $f$ 指给定一个凸集 $X$，函数 $f:X\to R$ 如果满足对于所有的 $x_1,x_2\in X$ 和 $\lambda \in [0,1]$，有

$$
\lambda f(x_1) + (1-\lambda)f(x_2) \ge f(\lambda x_1+(1-\lambda)x_2)
$$

**Jensen 不等式**：给定一个凸函数 $f$，$\alpha_i$ 为非负实数，$\sum_i \alpha_i=1$，$X$ 是一个随机变量，有以下结论：

$$
\sum_i \alpha_i f(x_i) \ge f(\sum_i \alpha_i x_i)\; and\; E_X[f(X)] \ge f(E_X[X])
$$

凸函数有以下有用的性质：

1. 局部最小点就是全局最小点
2. 在凸集 $X$ 上定义的凸函数 $f$，下集合 $S_b = \{x|x\in X \;and\; f(x)\le b\}$是凸集。
3. 对于二阶可导的一维函数 $f$，当且仅当 $f''\ge0$，$f$ 是凸的；对于二阶可导的多维函数 $f$，当且仅当海森矩阵半正定 ${\nabla ^2}f \ge 0$。






# 自然语言处理

[自然语言处理3：词向量 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/115532919)

进行word embedding的目的是为了表达词的含义，词汇之间的内在联系，实现对词语更准确地描述。可以使用余弦相似度计算各个词的相似性
$$
Sim(u,v)=\frac{u^Tv}{||u||\cdot||v||}
$$
词向量相减可以表示各个词在哪些维度上存在差距。



## N-gram语言模型

[自然语言处理 03：N-gram 语言模型 - YEY 的博客 | YEY Blog](https://yey.world/2020/03/09/COMP90042-03/)

### 概率：从联合概率到条件概率

我们的目标是得到一个由m个单词组成的任意序列（即一个包含m个单词的句子）的概率：
$$
P(w_1,w_2,\cdots,w_m)
$$
第一步是利用链式法则将联合概率转换成条件概率的连乘形式：
$$
P(w_1,w_2,\cdots,w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\cdots P(w_m|w_1,\cdots,w_{m-1}) 
$$


### 马尔可夫假设

利用马尔可夫假设：某个单词出现的概率不再依赖于全部上下文，而是取决于离它最近的n个单词，因此，可以得到：
$$
P({w_i}|{w_1}, \cdots ,{w_{i - 1}}) \approx P({w_i}|{w_{i - n + 1}}, \cdots ,{w_{i - 1}})
$$
对于某个很小的n：

+ n=1时，一个unigram模型（每个单词出现的概率之间相互独立）：

$$
P({w_1},{w_2}, \cdots ,{w_m}) = \prod\limits_{i = 1}^m {P({w_i})}
$$

+ n=2时，一个bigram模型（每个单词出现的概率都和它前一个单词出现的概率有关）：

$$
P({w_1},{w_2}, \cdots ,{w_m}) = \prod\limits_{i = 1}^m {P({w_i}|{w_{i - 1}})}
$$

+ n=3时，一个trigram模型（每个单词出现的概率都和它前两个单词出现的概率有关）：

$$
P({w_1},{w_2}, \cdots ,{w_m}) = \prod\limits_{i = 1}^m {P({w_i}|{w_{i - 2}},{w_{i - 1}})}
$$

### 最大似然估计

**如何计算上面提到的这些概率**

只需要一个大的用于训练的语料库，我们就可以根据语料库中各个单词的计数，利用最大似然估计来估计该单词出现的概率：

+ 对于unigram模型：

$$
P({w_i}) = {{C({w_i})} \over M}
$$

其中，$C$是一个计数函数，$C(w_i)$表示$w_i$在语料库中出现的次数，$M$表示语料库中所有单词tokens的数量。

+ 对于bigram模型：

$$
P({w_i}|{w_{i - 1}}) = {{C({w_{i - 1}},{w_i})} \over {C({w_{i - 1}})}}
$$

其中，$C(w_{i-1},w_i)$表示单词$w_{i-1}$和单词$w_i$前后相邻一起出现的次数。

+ 对于n-gram模型：

$$
P({w_i}|{w_{i - n + 1}}, \cdots ,{w_{i - 1}}) = {{C({w_{i - n + 1}}, \cdots ,{w_i})} \over {C({w_{i - n + 1}}, \cdots ,{w_{i - 1}})}}
$$

**一个Trigram的例子**

假设语料库为下面的两句话

`<s> <s> yes no no no no yes </s>`

`<s> <s> no no no yes yes yes no </s>`

由于采用的是Trigram，所以每句话开头有两个起始标记。

我们想要知道句子`<s> <s> yes no no yes </yes>`在trigram模型下的概率是多少？
$$
\begin{align}
P(\text{sent} =\textit{yes no no yes}) &= P(\textit{yes}\mid \texttt{<s>},\texttt{<s>})\times P(\textit{no}\mid \texttt{<s>},\textit{yes})\times P(\textit{no}\mid \textit{yes},\textit{no})\\
&\quad \times P(\textit{yes}\mid \textit{no},\textit{no}) \times P(\texttt{</s>} \mid \textit{no},\textit{yes})\\
&= \dfrac{1}{2} \times 1 \times \dfrac{1}{2} \times \dfrac{2}{5} \times \dfrac{1}{2} \\
&= 0.1
\end{align}
$$
对要计算的句子的概率按照trigram模型拆分成条件概率的连乘形式。



### 存在的一些问题

- **语言通常具有长距离效应——需要设置较大的n值**
  有些词对应的上下文可能出现在句子中距离该词很远的地方，这意味着如果我们采用固定长度的上下文（例如：trigram模型），我们可能无法捕捉到足够的上下文相关信息，这是所有有限上下文语言模型的一个通病。
- **计算出的结果的概率通常会非常小**
  一连串条件概率项连乘得到的结果往往会非常小，对于这个问题，可以采用取对数计算log概率来避免数值下溢。
- **对于不存在于语料库中的词，无法计算其出现概率**
  如果我们要计算概率的句子中包含了一些没有在语料库中出现过的单词（例如：人名），我们应该怎么办？一种比较简单的技巧是，我们可以用某种特殊符号（例如：`<UNK>`）来表示这些所谓的 OOV 单词（out-of-vocabulary，即不在词汇表中的单词），并且将语料库中一些非常低频的单词都替换为这种表示未知单词的特殊token。
- **出现在新的上下文（context）中的单词**
  默认情况下，任何我们之前没有在语料库中见过的n-gram的计数都为0，这将导致计算出的整个句子的概率为0。为此，我们需要对语言模型进行**平滑处理（smoothing）**。

### 平滑处理

基本思想：给之前没有见过的事件赋予概率

必须保证概率和为1

#### 拉普拉斯平滑（加一平滑）

假装我们看到每一个n-gram都比它们实际出现的次数多1次，即使是没有出现过的次数都记为1次。

+ 对于unigram模型（V=词汇表）：

$$
{P_{add1}}({w_i}) = {{C({w_i}) + 1} \over {M + \left| V \right|}}
$$

下面的$|V|$（语料库中所有单词的种类总数）是为了保证概率和为1。

+ 对于bigram模型：

$$
{P_{add1}}({w_i}|{w_{i - 1}}) = {{C({w_{i - 1}},{w_i}) + 1} \over {C({w_{i - 1}}) + \left| V \right|}}
$$

#### Lidstone平滑（加k平滑）

很多时候，加1显得太多了，并不想每次都加1，因为这会导致原本罕见事件可能变得有点过于频繁了，并且丢弃了所观测的n-gram的太多有效计数。

+ 对于trigram模型：

$$
{P_{add1}}({w_i}|{w_{i - 1}},{w_{i - 2}}) = {{C({w_{i - 2}},{w_{i - 1}},{w_i}) + k} \over {C({w_{i - 2}},{w_{i - 1}}) + k\left| V \right|}}
$$

如何选择一个合适的k值对于模型影响非常大。k在这里实际上是一个超参数，我们需要尝试对其进行调整以便找到一个使模型表现比较好的k值。

#### 绝对折扣平滑

从每个观测到的n-gram计数中“借”一个固定的概率质量$d$；然后将其重新分配到未知的n-gram上。

还有Backoff平滑、Kneser-Ney平滑等。

#### 插值

插值是一种更好的平滑处理方式。

**一个trigram模型下的Interpolation平滑概率：**
$$
\eqalign{
  {P_{{\rm{Interpolation}}}}({w_m}|{w_{m - 1}},{w_{m - 2}}) &= {\lambda _3} \times P_3^*({w_m}|{w_{m - 1}},{w_{m - 2}}) \\
  &  + {\lambda _2} \times P_2^*({w_m}|{w_{m - 1}})  \\
  &  + {\lambda _1} \times P_1^*({w_m}) \\}
$$
其中，$\lambda_1$，$\lambda_2$和$\lambda_3$是根据留存数据学习到的，并且$\sum\limits_{n = 1}^{{n_{max}}} {{\lambda _n}}  = 1$

首先计算trigram概率$P_3^*$，并将其乘以一个系数$\lambda_3$；然后计算bigram概率$P_2^*$，并将其乘以一个系数$\lambda_2$；然后计算unigram概率$P_1^*$，并将其乘以一个系数$\lambda_1$；最后将三者相加得到结果。

用插值替代back-off可以得到**Interpolated Kneser-Ney平滑**。

#### 实践应用

- 在实践中，我们通常采用Kneser-Ney语言模型并将5-grams作为最高阶数。
- 对于每个n-gram阶数都有不同的discount值。
- 当我们试图学习如何在它们之间进行Interpolation时，我们将从数据中学习。



### 生成语言

#### 生成

- 给定一个初始单词，从语言模型定义的概率分布中抽取一个词作为下一个出现的单词。

- 在我们的n-gram语言模型中包含$n-1$个起始tokens，用于提供生成第一个单词所需的上下文。

  + 永远不会生成起始标记`<s>`，它只作为初始单词的生成条件

  - 生成`</s>`来结束一个序列

#### 如何选择下一个单词

- Argmax：在每一轮中选择与上下文共现概率最高的那个单词。

  - **贪婪搜索（Greedy search ）**
    这其实是一种贪婪搜索策略，因为即使在每一步中我们都选择概率最高的那个单词，也无法保证最终生成的句子具有最优的概率。

- Beam search decoding

  一种更好的方法是 Beam search decoding，它在机器翻译中应用非常广泛。

  - 在每轮中，我们跟踪概率最高的前$N$个单词
  - 我们总是检查这几个候选单词给出的完整句子的概率
  - 这种方法可以生成具有 **近似最优（near-optimal）** 概率的句子

- **从分布中随机抽样**
  另一种方法是从语言模型给出的概率分布中随机抽样，例如：temperature sampling

### 评估语言模型

#### Perplexity

用于衡量语言模型生成的语句是否是合情合理的，逻辑上是连贯的。假设测试集语料库是一个由m个单词$w_1,w_2,\cdots,w_m$组成的序列，用$PP$表示Perplexity，公式如下：
$$
PP({w_1},{w_2}, \cdots ,{w_m}) = \root m \of {{1 \over {P({w_1},{w_2}, \cdots ,{w_m})}}}
$$
Perplexity越低，模型表现越好。

#### Perplexity例子

语料库为Wall Street Journal（华尔街日报）

训练集：3800万个单词tokens，解决2万个单词types（即词汇表）

测试集：150万个单词tokens

实验结果：

|            | Unigram | Bigram | Trigram |
| ---------- | ------- | ------ | ------- |
| Perplexity | 962     | 170    | 109     |



## Word2vec

[[NLP\] 秒懂词向量Word2vec的本质 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/26306795)

[深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/114538417)

word2vec的输出是目标单词与上下文单词的概率，
$$
y_k=Pr(word_k|word_{context})=\frac{exp(activation(k))}{\sum_{n=1}^Vexp(activation(n))}
$$

### 训练Word2Vec

使用gensim对weibo_data.json（已存入网盘/数据集）进行word2vec的训练

```python
import json
import jieba
from tqdm import tqdm

import re
from hanzi import punctuation

punctuation = """！？｡，＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘'‛“”„‟…‧﹏"""
re1 = re.compile("[{}]+".format(punctuation))
re2 = re.compile("[a-zA-Z]+")

#过滤中文标点符号和字母
def filter_punc(desstr,restr=''):
    desstr = re1.sub(restr, desstr)
    return re2.sub(restr,desstr)

data_path = "weibo_data.json"

content = []
with open(data_path, 'r', encoding='utf-8') as f:
    news = json.load(f)
    content = [n['content'] for n in news]

content = content[:50000]

word = []
for c in tqdm(content):
    c = filter_punc(c)
    word.append(list(jieba.cut(c)))

```

其中word的形式如下，每一行表示一个句子
```python
word = [["前天", "在", "郑州", ...],
	   ["近日", "北京", ...],
	   ...
	   ]
```

```python
from gensim.models import Word2Vec
model = Word2Vec(word)
```

```python
vector = model.wv["今天"]
# print(vector)
sims = model.wv.most_similar("今天")
print(sims)
```


### 两个算法

#### skip-grams

输入层为目标单词，上下文单词位于输出层

#### CBOW

CBOW模型的训练输入是某一个特征词的上下文（context）相关的词对应的词向量，而输出就是这特定的一个词（目标单词target）的词向量。比如上下文大小取值为1，上下文对应的词有2个，前后各1个，这2个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这上下文单词都是平等的，也就是不考虑上下文单词和目标单词之间的距离大小，只要在我们上下文之内即可。

### 两个方法

#### Huffman树

将单词按词频进行Huffman编码，[哈夫曼编码及其应用——数据压缩（Huffman compression） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/144562146)

python实现：[huffmanCoding.py](codes\huffmanCoding.py) 

#### 负采样

[（三）通俗易懂理解——Skip-gram的负采样 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/39684349)

基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。



# 机器学习

## EM算法

参考：[【机器学习】EM——期望最大（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/78311644)

EM算法通过迭代来对含有隐变量的概率参数模型进行最大似然估计或极大后验概率估计

代码： [emCoins[EM算法估计硬币正面概率].py](D:\postgraduateLearn\first\codes\emCoins[EM算法估计硬币正面概率].py) 



## 无监督学习

### 自组织映射（self-organizing map）

#### 概述

自组织映射（SOM）是一种无监督机器学习技术，用于生成高维数据集的低维（通常是二维）表示，同时保留数据的拓扑结构。例如，在$n$维观测值中测量的$p$维变量的数据集可以表示为具有相似变量值的观测值聚类，这些聚类可以可视化为二维“地图”，这样近端聚类中的观测值比远端聚类中的观测值具有更相似的值。这可以使高维数据更易于可视化和分析。

SOM是一种人工神经网络，但使用竞争学习进行训练，而不是使用其他人工神经网络使用的纠错学习（如梯度下降的反向传播）。SOM有时也被称为Kohonen地图或Kohonen网络。与大多数人工神经网络一样，自组织映射以两种模式运行：训练和映射。首先，训练使用输入数据集（“输入空间”）来生成输入数据的低维表示（“地图空间”）。其次，映射使用生成的映射对其他输入数据进行分类。

在大多数情况下，训练的目标是将具有$p$维的输入空间表示为具有二维的地图空间。具体来说，具有$p$变量的输入空间被称为具有$p$维。映射空间由称为“节点”或“神经元”的组件组成，这些组件被排列成具有二维的六边形或矩形网格。节点的数量及其排列是根据数据分析和探索的更大目标预先指定的。映射空间中的每个节点都与一个“权重”向量相关联，该向量是节点在输入空间中的位置。虽然地图空间中的节点保持固定，但训练包括将权重向量向输入数据移动（减少距离度量，例如欧几里得距离），而不会破坏地图空间引起的拓扑。训练后，地图可用于通过查找与输入空间向量最接近权重向量（最小距离度量）的节点来对输入空间的其他观测值进行分类。

#### 学习算法

在自组织映射中学习的目标是使网络的不同部分对某些输入模式做出类似的响应。这在一定程度上是由人脑大脑皮层的不同部分如何处理视觉、听觉或其他感官信息所致。神经元的权重被初始化为小的随机值，或者从两个最大的主成分特征向量跨越的子空间中均匀采样。使用后一种选择，学习速度要快得多，因为初始权重已经很好地近似了 SOM 权重。

训练采用竞争性学习，当训练样本被馈送到网络时，将计算其到所有权重向量的欧几里得距离。权重向量与输入最相似的神经元称为最佳匹配单元（BMU）。在SOM网格中，BMU和靠近它的神经元的权重根据输入向量进行调整。变化的幅度随着时间和与BMU的网格距离而减小。权重向量为$W_v(s)$的神经元$v$的更新公式为
$$
W_v(s+1)=W_v(s)+\theta(u,v,s)\cdot \alpha(s)\cdot (D(t)-W_v(s))
$$
其中$s$是步数，$t$是训练样本的索引，$u$是输入向量$D(t)$的BMU指数，$\alpha(s)$是单调递减学习系数，$\theta(u,v,s)$是邻域函数，它给出了步骤$s$中神经元$u$和神经元$v$之间的距离。根据实现的不同，$t$可以系统地扫描训练数据集（$t$是 0， 1， 2...T-1，然后重复，T 是训练样本的大小）， 从数据集中随机抽取（bootstrap采样）， 或者实现一些其他的采样方法（如jackknifing）。

邻域函数$\theta(u,v,s)$（也称为横向相互作用函数）取决于BMU（神经元 $u$）和神经元$v$之间的网格距离。在最简单的形式中，所有足够接近BMU的神经元为1，其他神经元为0，但高斯函数和Mexican-hat函数也是常见的选择。无论函数形式如何，邻域函数都会随时间而缩小。 一开始，当邻域很宽泛时，自组织就会在整个范围内发生。当邻域缩小到只有几个神经元时，权重会收敛到局部估计值。在一些实现中，学习系数$\alpha$和邻域函数$\theta$随着$s$的增加而稳步下降，在其他实现中（特别是那些$t$扫描训练数据集的实现），它们以逐步方式减少，每T步增加一次。

对于每个输入向量，对（通常很大）周期数$\lambda$重复此过程。网络最终会将输出节点与输入数据集中的组或模式相关联。如果可以命名这些模式，则可以将这些名称附加到训练网络中的关联节点。在映射过程中，将有一个获胜神经元：其权重向量最接近输入向量的神经元。这可以通过计算输入向量和权重向量之间的欧几里得距离来简单地确定。

**算法**

1. 随机化地图中的节点权重向量

2. for $s=0,1,2,...,\lambda$

   1. 随机选择一个输入向量$D(t)$；
   2. 在地图中查找最接近输入向量的节点。此节点是最佳匹配单元（BMU），用$v$表示；
   3. 对每个节点$v$，通过拉近输入向量来更新其向量：

   $$
   W_v(s+1)=W_v(s)+\theta(u,v,s)\cdot \alpha(s)\cdot (D(t)-W_v(s))
   $$

$s$是当前迭代，$\lambda$是迭代限制，$t$是输入数据集$\bf{D}$中目标输入数据向量的索引

关键的设计选择是 SOM 的形状、邻域函数和学习率时间表。邻域函数的思想是使 BMU 更新最多，其近邻更新更少，依此类推。学习率的想法是安排它，使地图更新在开始时很大，并逐渐停止更新。

例如，如果想用方形网格来学习SOM，我们可以使用邻域函数可以使BMU完全更新，最近的邻居更新一半，它们的邻居再次更新一半，等等。
$$
\theta \left( {\left( {i,j} \right),\left( {i',j'} \right),s} \right) = {1 \over {{2^{\left| {i - i'} \right| + \left| {j - j'} \right|}}}} = \left\{ \matrix{
  1\qquad {\rm{if}}\;i = i',j = j'   \cr 
  1/2\;\;\;{\rm{if}}\;\left| {i - i'} \right| + \left| {j - j'} \right| = 1 \hfill \cr 
  1/4\;\;\;{\rm{if}}\;\left| {i - i'} \right| + \left| {j - j'} \right| = 2 \hfill \cr 
   \cdots \quad \;\; \cdots  \cr}  \right.
$$
学习率可以是简单的线性：$\alpha(s)=1-s/\lambda$

**替代算法**

1. 随机化地图节点的权重向量

2. 遍历输入数据集中的每个输入向量

   1. 遍历映射中的每个节点
      1. 使用欧式距离公式查找输入向量与地图节点权重向量之间的相似性
      2. 跟踪产生最小距离的节点（此节点便是最佳匹配单元，BMU）
   2. 通过将BMU附近的节点（包括BMU本身）拉近输入向量来更新BMU附近的节点

   $$
   W_v(s+1)=W_v(s)+\theta(u,v,s)\cdot \alpha(s)\cdot (D(t)-W_v(s))
   $$

3. 从步骤2开始增加$s$并重复，同时$s<\lambda$


# 图神经网络


对于pytorch，可以使用 [torch_geometric 库](https://pytorch-geometric.readthedocs.io/en/latest/index.html)


下载命令（注意 pytorch 和 cuda 版本）[Installation — pytorch_geometric](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html)

```sh
pip install torch_geometric
pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.2+cu118.html
```

torch_geometric 库中有许多的现成的模型组件。PyG 中通过一个 `torch_geometric.data.Data` 实例描述单个图，默认有以下属性

`data.X`：节点的特征矩阵 `[num_nodes, num_node_features]`

`data.edge_index`：图的连通性矩阵 `[2, num_edges]` 类型为 long

`data.edge_attr`：边的特征矩阵 `[num_edges, num_edge_features]`

`data.y`：训练时的目标

`data.pos`：节点的位置矩阵 `[num_nodes, num_dimensions]`

注意，data 并不局限于这些属性，也并不需要所有的属性都被赋值。




## 简介

[图神经网络从入门到入门 - 知乎](https://zhuanlan.zhihu.com/p/136521625)

普通的全连接层为特征矩阵 $X$ 乘以权重矩阵 $W$，图神经网络在此基础上还乘上了一个邻接矩阵 $A$。

普通的全连接层：$H=\sigma(XW)$，图神经网络：$H=\sigma(AXW)$


Graph Convolution Networks（GCN）是开山之作，将图像处理中的卷积操作简单的用到图结构数据处理中。具体操作如下

$$
{h_v} = f\left( {{1 \over {\left| {N(v)} \right|}}\sum\nolimits_{u  \in N(v)} {W{x_u}}  + b} \right)
$$

其中，$N(v)$ 为 v 的邻接节点，即将 v 的邻接节点聚合起来再做一个线性映射。





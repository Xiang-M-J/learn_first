


# 理论知识


## D2L

### 编程小技巧

1. 通过**原位操作**减小占用

```python
 before = id(Y)    
 Y = Y + X
 id(Y) == before   # False
```

注意 `Y = Y + X` 会为 `Y` 分配一个新的内存，换成下面中的一个会更好

```python
# 这两种方式均不会为Y分配新内存
Y[:] = X + Y  
Y = Y + X
```


2. `nn.LazyLinear()`：功能与`nn.Linear()`相同，不过只需要给定输出维度，输入维度可以自动推断。


### 神经网络
#### 反向传播
反向传播时，需要计算最后得到的结果是一个标量，或者提供向量 `v` （下面的代码中 `gradient` 即为 `v`）使得反向传播时计算 $v^T\partial_x y$（其实还是一个标量），而不是$\partial_x y$。

```python
x = torch.arange(4.0)
x.requires_grad_(True)
# x.grad.zero_()
y = x * x 
y.backward(gradient=torch.ones(len(y))) # Faster: y.sum().backward()
x.grad  # tensor([0., 2., 4., 6.])
```

如果一个值只是中间值，不需要计算梯度，可以使用 `detach` 来截断反向传播

```python
x = torch.arange(4.0)
x.requires_grad_(True)
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad   # 由于detach截断了反向传播，所以x的梯度为u，而不是 3x^2
```

> 一个简单的线性拟合

```python
import torch
import matplotlib.pyplot as plt

x = torch.arange(100.0, requires_grad=True)
w, b = torch.tensor(1., requires_grad=True), torch.tensor(0.,requires_grad=True)
optimizer = torch.optim.SGD(params=[w, b], lr=1e-4)
loss = torch.nn.MSELoss()
for i in range(10):
    y = 2.138*x + 3.213
    y_ = w * x + b
    L = loss(y, y_)
    optimizer.zero_grad()
    L.backward()
    optimizer.step()

plt.plot(x.detach().numpy(), y.detach().numpy())
plt.plot(x.detach().numpy(), y_.detach().numpy())
plt.show()
```

sgd算法的简单实现
```python
def sgd(params, lr, batch_size): 
    #@save """小批量随机梯度下降""" 
    with torch.no_grad(): 
        for param in params: 
            param-= lr * param.grad / batch_size 
            param.grad.zero_()
```

#### Covariate Shift 纠正

Covariate Shift 指训练数据（q(x)）和测试数据的分布（p(x)）存在差异，导致模型在测试集上表现变差。幸运的是，独立性假设表明条件分布没有变，即：$p(y|x)=q(y|x)$。对于模型的风险，有下面的等式

$$
\int\!\!\!\int {l(f({\bf{x}}),y)p(y|{\bf{x}})p({\bf{x}})} d{\bf{x}}dy = \int\!\!\!\int {l(f({\bf{x}}),y)q(y|{\bf{x}})q({\bf{x}}){{p({\bf{x}})} \over {q({\bf{x}})}}} d{\bf{x}}dy
$$
换句话说，我们需要根据从正确分布（p(x)）中抽取的概率与从错误分布（q(x)）中抽取的概率之比来重新权衡每个数据。令$\beta_i$为
$$
{\beta _i}\mathop  = \limits^{def} {{p({{\bf{x}}_i})} \over {q({{\bf{x}}_i})}}
$$
这样的话，可以将模型的目标函数改写为
$$
\mathop {\min }\limits_f {1 \over n}\sum\limits_{i = 1}^n {{\beta _i}l(f({{\bf{x}}_i}),{y_i})} 
$$
这里的$\beta_i$并不知道，下面介绍一种借助逻辑回归的算法。假设我们有一个训练集  $\{(x_1, y_1), \cdots ,(x_n,y_n)\}$ 和一个未标记的测试集 $\{u_1, \cdots, u_m\}$。对于 Covariate Shift，我们假设所有 1≤i ≤ n 的 $x_i$ 都来自某个源分布，而所有 1≤ i≤ m 的 $u_i$ 来自目标分布。下面是用于校正 Covariate Shift 的原型算法：
1. 创建一个二分类训练集：$\{(x_1,-1),\cdots, (x_n,-1),(u_1,1),\cdots,(u_m, 1)\}$
2. 使用逻辑回归训练一个二分类器，得到分类函数 h
3. $\beta_i=exp(h(x_i))$，或者 $\beta_i= \min (exp(h(x_i)), c)$（c 为一个固定常数）


#### dropout

在标准 dropout 正则化中，将每层中部分节点归零，然后通过按保留（未丢弃）的节点分数归一化来消除每一层的偏差。即将每个中间激活值 $h$ 替换成随机值 $h'$，$h'$取值如下：
$$
h' = \left\{ \matrix{
  0\quad {\rm{with}}\;{\rm{probability}}\;p \hfill \cr 
  {h \over {1 - p}}\quad {\rm{otherwise}} \hfill \cr}  \right.
$$
之所以要除1-p，是为了保证 $E[h']=h$。

具体实现
```python
def dropout(X, p):
    assert 0<= p <= 1
    if p==1: return torch.zeros_like(X)
    mask = (torch.rand(X.shape) > p).float()
    return mask * X / (1.0 - p)
```


#### 卷积层

```python
def corr2d(X, K):
    h, w = K.shape
    Y = torch.zeros(X.shape[0] - h + 1, X.shape[1] - w + 1)
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()
    return Y

class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.w = nn.Parameter(torch.rand(kernel_size))
        self.b = nn.Parameter(torch.zeros(1))
    def forward(self, x):
        return corr2d(x, self.w) + self.b
```

池化层的实现与卷积层相似，只不过需要改变 `Y[i,j]` 的计算公式。


#### RNN

考虑一个自回归模型，对时序数据，估计 $P(x_t|x_{t-1},\cdots, x_1)$，有下面两种方法
1. 自回归模型
使用观测序列 $x_{t−1},\cdots,x_{t−\tau}$ 来估计 $x_t$。
2. 隐变量自回归模型
保留一些对过去观测的总结 $h_t$，并且同时更新预测 $\hat x_t$ 和总结 $h_t$。其中，$\hat x_t = P(x_t|h_t)$，$h_t=g(h_{t-1},x_{t-1})$。

RNN 中也包含了对于过去观测的总结 $h_t$，与一般的 MLP 不同，RNN 的隐层计算公式如下。
$$
H_t = \phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)
$$
一般的MLP为 $H_t = \phi(X_tW_{xh}+b_h)$，其中 $\phi$ 为激活函数。
对于 RNN 训练语言模型时，可以将输入序列向后移动一位作为输出序列。
对于训练好的 RNN 模型，输入文本，预测后面文本的代码如下：
```python
def predict(self, prefix, num_preds, vocab, device=None):
    state, outputs = None, [vocab[prefix[0]]]  # 一开始state为空
    for i in range(len(prefix) + num_preds - 1):
        X = torch.tensor([[outputs[-1]]], device=device)
        embs = self.one_hot(X)
        rnn_outputs, state = self.rnn(embs, state)
        if i < len(prefix) - 1:  # Warm-up period
            outputs.append(vocab[prefix[i + 1]])
        else:  # Predict num_preds steps
            Y = self.output_layer(rnn_outputs)
            outputs.append(int(Y.argmax(axis=2).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

这里 self.output_layer 是将 RNN 输出的隐层状态与系数相乘后堆叠在一起

```python
def output_layer(self, rnn_outputs):
    outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]
    return torch.stack(outputs, 1)
def forward(self, X, state=None):
    embs = self.one_hot(X)
    rnn_outputs, _ = self.rnn(embs, state)
    return self.output_layer(rnn_outputs)
```

RNN 的梯度更新（BPTT算法）：[9.7. Backpropagation Through Time — Dive into Deep Learning 1.0.3 documentation (d2l.ai)](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)

在 seq2seq 模型的输出时，每个时间步都对应着不同的词元及其概率，这时候需要确定一条路径使得选择具有最高条件概率的词元。如下图所示，每个时间步对应了四种词元，当输出 `<eos>` 时，停止输出。

| 时间步  | 1    | 2    | 3    | 4    |
| ------- | ---- | ---- | ---- | ---- |
| A       | 0.5  | 0.1  | 0.2  | 0    |
| B       | 0.2  | 0.4  | 0.2  | 0.2  |
| C       | 0.2  | 0.3  | 0.4  | 0.2  |
| `<eos>` | 0.1  | 0.2  | 0.2  | 0.6  |

**贪心搜索**直接在每个时间步选择具有最高条件概率的词元，即输出为 `A B C <eos>`。贪心算法虽然简单，但是不能保证得到最优序列，因为每个时间步的输出和前面时间步的输出有关。可能第2步选择了 C 词元会导致条件概率变为

| 时间步  | 1    | 2    | 3    | 4    |
| ------- | ---- | ---- | ---- | ---- |
| A       | 0.5  | 0.1  | 0.1  | 0.1  |
| B       | 0.2  | 0.4  | 0.6  | 0.2  |
| C       | 0.2  | 0.3  | 0.2  | 0.1  |
| `<eos>` | 0.1  | 0.2  | 0.1  | 0.6  |

此时如果输出 `A C B <eos>` 的条件概率比贪心搜索时更高。


**束搜索**（beamsearch）是贪心搜索的一个改进版本。它有一个超参数，名为束宽 k。在时间步1，选择具有最高条件概率的 k 个词元。这 k 个词元将分别是 k 个候选输出序列的第一个词元。在随后的每个时间步，基于上一时间步的 k 个候选输出序列，我们将继续从 ky（y为当前时间步对应的可选词元个数）个可能的选择中挑出具有最高条件概率的 k 个候选输出序列。

假设输出的词表只包含五个元素，束宽为2，输出序列的最大长度为3。每一步选择的词元如下：
第一步：A C
第二步：B E
第三步：D D
最后将会有 6 种可能，分别为：（1）A；（2） C；（3）A,B；（4）C,E；（5）A,B,D；（6）C,E,D。

最后，基于这六个序列（例如，丢弃包括`<eos>`和之后的部分），我们获得最终候选输出序列集合。然后 我们选择其中条件概率乘积最高的序列作为输出序列：
$$
{1 \over {{L^\alpha }}}\log P\left( {{y_1}, \cdots ,{y_L}|{\bf{c}}} \right) = {1 \over {{L^\alpha }}}\sum\limits_{t' = 1}^L {\log P\left( {{y_{t'}}|{y_1}, \cdots ,{y_{t' - 1}},{\bf{c}}} \right)} 
$$
其中 L 是最终候选序列的长度，$\alpha$ 通常设置为0.75。因为一个较长的序列在求和中会有更多的对数项，因此分母中的 $L^{\alpha}$ 用于惩罚长序列。

#### 优化算法

**鞍点**指函数梯度为0的点，且该点既不是局部最小点也不是全局最小点。如 $f(x)=x^3$ 的 $x=0$ 的点就是鞍点。

**凸集**指在向量空间中的一个集合 $X$，如果对于任意 $a,b\in X$，连接 $a$ 和 $b$ 的线段也在 $X$ 中，即对于 $\lambda\in[0,1]$，有
$$
\lambda a + (1-\lambda)b\in X
$$

一般情况下，深度学习中的问题被定义为凸集，如一个 d 维的实数集 $R^d$ 是一个凸集。

**凸函数** $f$ 指给定一个凸集 $X$，函数 $f:X\to R$ 如果满足对于所有的 $x_1,x_2\in X$ 和 $\lambda \in [0,1]$，有

$$
\lambda f(x_1) + (1-\lambda)f(x_2) \ge f(\lambda x_1+(1-\lambda)x_2)
$$

**Jensen 不等式**：给定一个凸函数 $f$，$\alpha_i$ 为非负实数，$\sum_i \alpha_i=1$，$X$ 是一个随机变量，有以下结论：

$$
\sum_i \alpha_i f(x_i) \ge f(\sum_i \alpha_i x_i)\; and\; E_X[f(X)] \ge f(E_X[X])
$$

凸函数有以下有用的性质：

1. 局部最小点就是全局最小点
2. 在凸集 $X$ 上定义的凸函数 $f$，下集合 $S_b = \{x|x\in X \;and\; f(x)\le b\}$是凸集。
3. 对于二阶可导的一维函数 $f$，当且仅当 $f''\ge0$，$f$ 是凸的；对于二阶可导的多维函数 $f$，当且仅当海森矩阵半正定 ${\nabla ^2}f \ge 0$。






## 自然语言处理

[自然语言处理3：词向量 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/115532919)

进行word embedding的目的是为了表达词的含义，词汇之间的内在联系，实现对词语更准确地描述。可以使用余弦相似度计算各个词的相似性
$$
Sim(u,v)=\frac{u^Tv}{||u||\cdot||v||}
$$
词向量相减可以表示各个词在哪些维度上存在差距。



### N-gram语言模型

[自然语言处理 03：N-gram 语言模型 - YEY 的博客 | YEY Blog](https://yey.world/2020/03/09/COMP90042-03/)

#### 概率：从联合概率到条件概率

我们的目标是得到一个由m个单词组成的任意序列（即一个包含m个单词的句子）的概率：
$$
P(w_1,w_2,\cdots,w_m)
$$
第一步是利用链式法则将联合概率转换成条件概率的连乘形式：
$$
P(w_1,w_2,\cdots,w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\cdots P(w_m|w_1,\cdots,w_{m-1}) 
$$


#### 马尔可夫假设

利用马尔可夫假设：某个单词出现的概率不再依赖于全部上下文，而是取决于离它最近的n个单词，因此，可以得到：
$$
P({w_i}|{w_1}, \cdots ,{w_{i - 1}}) \approx P({w_i}|{w_{i - n + 1}}, \cdots ,{w_{i - 1}})
$$
对于某个很小的n：

+ n=1时，一个unigram模型（每个单词出现的概率之间相互独立）：

$$
P({w_1},{w_2}, \cdots ,{w_m}) = \prod\limits_{i = 1}^m {P({w_i})}
$$

+ n=2时，一个bigram模型（每个单词出现的概率都和它前一个单词出现的概率有关）：

$$
P({w_1},{w_2}, \cdots ,{w_m}) = \prod\limits_{i = 1}^m {P({w_i}|{w_{i - 1}})}
$$

+ n=3时，一个trigram模型（每个单词出现的概率都和它前两个单词出现的概率有关）：

$$
P({w_1},{w_2}, \cdots ,{w_m}) = \prod\limits_{i = 1}^m {P({w_i}|{w_{i - 2}},{w_{i - 1}})}
$$

#### 最大似然估计

**如何计算上面提到的这些概率**

只需要一个大的用于训练的语料库，我们就可以根据语料库中各个单词的计数，利用最大似然估计来估计该单词出现的概率：

+ 对于unigram模型：

$$
P({w_i}) = {{C({w_i})} \over M}
$$

其中，$C$是一个计数函数，$C(w_i)$表示$w_i$在语料库中出现的次数，$M$表示语料库中所有单词tokens的数量。

+ 对于bigram模型：

$$
P({w_i}|{w_{i - 1}}) = {{C({w_{i - 1}},{w_i})} \over {C({w_{i - 1}})}}
$$

其中，$C(w_{i-1},w_i)$表示单词$w_{i-1}$和单词$w_i$前后相邻一起出现的次数。

+ 对于n-gram模型：

$$
P({w_i}|{w_{i - n + 1}}, \cdots ,{w_{i - 1}}) = {{C({w_{i - n + 1}}, \cdots ,{w_i})} \over {C({w_{i - n + 1}}, \cdots ,{w_{i - 1}})}}
$$

**一个Trigram的例子**

假设语料库为下面的两句话

`<s> <s> yes no no no no yes </s>`

`<s> <s> no no no yes yes yes no </s>`

由于采用的是Trigram，所以每句话开头有两个起始标记。

我们想要知道句子`<s> <s> yes no no yes </yes>`在trigram模型下的概率是多少？
$$
\begin{align}
P(\text{sent} =\textit{yes no no yes}) &= P(\textit{yes}\mid \texttt{<s>},\texttt{<s>})\times P(\textit{no}\mid \texttt{<s>},\textit{yes})\times P(\textit{no}\mid \textit{yes},\textit{no})\\
&\quad \times P(\textit{yes}\mid \textit{no},\textit{no}) \times P(\texttt{</s>} \mid \textit{no},\textit{yes})\\
&= \dfrac{1}{2} \times 1 \times \dfrac{1}{2} \times \dfrac{2}{5} \times \dfrac{1}{2} \\
&= 0.1
\end{align}
$$
对要计算的句子的概率按照trigram模型拆分成条件概率的连乘形式。



#### 存在的一些问题

- **语言通常具有长距离效应——需要设置较大的n值**
  有些词对应的上下文可能出现在句子中距离该词很远的地方，这意味着如果我们采用固定长度的上下文（例如：trigram模型），我们可能无法捕捉到足够的上下文相关信息，这是所有有限上下文语言模型的一个通病。
- **计算出的结果的概率通常会非常小**
  一连串条件概率项连乘得到的结果往往会非常小，对于这个问题，可以采用取对数计算log概率来避免数值下溢。
- **对于不存在于语料库中的词，无法计算其出现概率**
  如果我们要计算概率的句子中包含了一些没有在语料库中出现过的单词（例如：人名），我们应该怎么办？一种比较简单的技巧是，我们可以用某种特殊符号（例如：`<UNK>`）来表示这些所谓的 OOV 单词（out-of-vocabulary，即不在词汇表中的单词），并且将语料库中一些非常低频的单词都替换为这种表示未知单词的特殊token。
- **出现在新的上下文（context）中的单词**
  默认情况下，任何我们之前没有在语料库中见过的n-gram的计数都为0，这将导致计算出的整个句子的概率为0。为此，我们需要对语言模型进行**平滑处理（smoothing）**。

#### 平滑处理

基本思想：给之前没有见过的事件赋予概率

必须保证概率和为1

##### 拉普拉斯平滑（加一平滑）

假装我们看到每一个n-gram都比它们实际出现的次数多1次，即使是没有出现过的次数都记为1次。

+ 对于unigram模型（V=词汇表）：

$$
{P_{add1}}({w_i}) = {{C({w_i}) + 1} \over {M + \left| V \right|}}
$$

下面的$|V|$（语料库中所有单词的种类总数）是为了保证概率和为1。

+ 对于bigram模型：

$$
{P_{add1}}({w_i}|{w_{i - 1}}) = {{C({w_{i - 1}},{w_i}) + 1} \over {C({w_{i - 1}}) + \left| V \right|}}
$$

##### Lidstone平滑（加k平滑）

很多时候，加1显得太多了，并不想每次都加1，因为这会导致原本罕见事件可能变得有点过于频繁了，并且丢弃了所观测的n-gram的太多有效计数。

+ 对于trigram模型：

$$
{P_{add1}}({w_i}|{w_{i - 1}},{w_{i - 2}}) = {{C({w_{i - 2}},{w_{i - 1}},{w_i}) + k} \over {C({w_{i - 2}},{w_{i - 1}}) + k\left| V \right|}}
$$

如何选择一个合适的k值对于模型影响非常大。k在这里实际上是一个超参数，我们需要尝试对其进行调整以便找到一个使模型表现比较好的k值。

##### 绝对折扣平滑

从每个观测到的n-gram计数中“借”一个固定的概率质量$d$；然后将其重新分配到未知的n-gram上。

还有Backoff平滑、Kneser-Ney平滑等。

##### 插值

插值是一种更好的平滑处理方式。

**一个trigram模型下的Interpolation平滑概率：**
$$
\eqalign{
  {P_{{\rm{Interpolation}}}}({w_m}|{w_{m - 1}},{w_{m - 2}}) &= {\lambda _3} \times P_3^*({w_m}|{w_{m - 1}},{w_{m - 2}}) \\
  &  + {\lambda _2} \times P_2^*({w_m}|{w_{m - 1}})  \\
  &  + {\lambda _1} \times P_1^*({w_m}) \\}
$$
其中，$\lambda_1$，$\lambda_2$和$\lambda_3$是根据留存数据学习到的，并且$\sum\limits_{n = 1}^{{n_{max}}} {{\lambda _n}}  = 1$

首先计算trigram概率$P_3^*$，并将其乘以一个系数$\lambda_3$；然后计算bigram概率$P_2^*$，并将其乘以一个系数$\lambda_2$；然后计算unigram概率$P_1^*$，并将其乘以一个系数$\lambda_1$；最后将三者相加得到结果。

用插值替代back-off可以得到**Interpolated Kneser-Ney平滑**。

##### 实践应用

- 在实践中，我们通常采用Kneser-Ney语言模型并将5-grams作为最高阶数。
- 对于每个n-gram阶数都有不同的discount值。
- 当我们试图学习如何在它们之间进行Interpolation时，我们将从数据中学习。



#### 生成语言

##### 生成

- 给定一个初始单词，从语言模型定义的概率分布中抽取一个词作为下一个出现的单词。

- 在我们的n-gram语言模型中包含$n-1$个起始tokens，用于提供生成第一个单词所需的上下文。

  + 永远不会生成起始标记`<s>`，它只作为初始单词的生成条件

  - 生成`</s>`来结束一个序列

##### 如何选择下一个单词

- Argmax：在每一轮中选择与上下文共现概率最高的那个单词。

  - **贪婪搜索（Greedy search ）**
    这其实是一种贪婪搜索策略，因为即使在每一步中我们都选择概率最高的那个单词，也无法保证最终生成的句子具有最优的概率。

- Beam search decoding

  一种更好的方法是 Beam search decoding，它在机器翻译中应用非常广泛。

  - 在每轮中，我们跟踪概率最高的前$N$个单词
  - 我们总是检查这几个候选单词给出的完整句子的概率
  - 这种方法可以生成具有 **近似最优（near-optimal）** 概率的句子

- **从分布中随机抽样**
  另一种方法是从语言模型给出的概率分布中随机抽样，例如：temperature sampling

#### 评估语言模型

##### Perplexity

用于衡量语言模型生成的语句是否是合情合理的，逻辑上是连贯的。假设测试集语料库是一个由m个单词$w_1,w_2,\cdots,w_m$组成的序列，用$PP$表示Perplexity，公式如下：
$$
PP({w_1},{w_2}, \cdots ,{w_m}) = \root m \of {{1 \over {P({w_1},{w_2}, \cdots ,{w_m})}}}
$$
Perplexity越低，模型表现越好。

##### Perplexity例子

语料库为Wall Street Journal（华尔街日报）

训练集：3800万个单词tokens，解决2万个单词types（即词汇表）

测试集：150万个单词tokens

实验结果：

|            | Unigram | Bigram | Trigram |
| ---------- | ------- | ------ | ------- |
| Perplexity | 962     | 170    | 109     |



### Word2vec

[[NLP\] 秒懂词向量Word2vec的本质 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/26306795)

[深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/114538417)

word2vec的输出是目标单词与上下文单词的概率，
$$
y_k=Pr(word_k|word_{context})=\frac{exp(activation(k))}{\sum_{n=1}^Vexp(activation(n))}
$$

#### 训练Word2Vec

使用gensim对weibo_data.json（已存入网盘/数据集）进行word2vec的训练

```python
import json
import jieba
from tqdm import tqdm

import re
from hanzi import punctuation

punctuation = """！？｡，＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘'‛“”„‟…‧﹏"""
re1 = re.compile("[{}]+".format(punctuation))
re2 = re.compile("[a-zA-Z]+")

#过滤中文标点符号和字母
def filter_punc(desstr,restr=''):
    desstr = re1.sub(restr, desstr)
    return re2.sub(restr,desstr)

data_path = "weibo_data.json"

content = []
with open(data_path, 'r', encoding='utf-8') as f:
    news = json.load(f)
    content = [n['content'] for n in news]

content = content[:50000]

word = []
for c in tqdm(content):
    c = filter_punc(c)
    word.append(list(jieba.cut(c)))

```

其中word的形式如下，每一行表示一个句子
```python
word = [["前天", "在", "郑州", ...],
	   ["近日", "北京", ...],
	   ...
	   ]
```

```python
from gensim.models import Word2Vec
model = Word2Vec(word)
```

```python
vector = model.wv["今天"]
# print(vector)
sims = model.wv.most_similar("今天")
print(sims)
```


#### 两个算法

##### skip-grams

输入层为目标单词，上下文单词位于输出层

##### CBOW

CBOW模型的训练输入是某一个特征词的上下文（context）相关的词对应的词向量，而输出就是这特定的一个词（目标单词target）的词向量。比如上下文大小取值为1，上下文对应的词有2个，前后各1个，这2个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这上下文单词都是平等的，也就是不考虑上下文单词和目标单词之间的距离大小，只要在我们上下文之内即可。

#### 两个方法

##### Huffman树

将单词按词频进行Huffman编码，[哈夫曼编码及其应用——数据压缩（Huffman compression） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/144562146)

python实现：[huffmanCoding.py](codes\huffmanCoding.py) 

##### 负采样

[（三）通俗易懂理解——Skip-gram的负采样 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/39684349)

基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。



# 机器学习

## EM算法

参考：[【机器学习】EM——期望最大（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/78311644)

EM算法通过迭代来对含有隐变量的概率参数模型进行最大似然估计或极大后验概率估计

代码： [emCoins[EM算法估计硬币正面概率].py](D:\postgraduateLearn\first\codes\emCoins[EM算法估计硬币正面概率].py) 


## 条件随机场 CRF

CRF 常用于中文分词，命名实体识别，词性标注等场景。还可以与深度学习结合，如 BiLSTM-CRF 等模型。


### 马尔可夫随机场（概率无向图模型）

如果随机过程的参数 $t$ 是一个 n 维向量，那么可以将随机过程称为随机场。

马尔可夫性质：

$$
P({Y_v}|X,{Y_w},w \ne v) = P({Y_v}|X,{Y_w},w \sim v)
$$

其中 $w \sim v$ 表示图中所有和顶点 v 相连接的顶点，$w \neq v$ 表示顶点 $v$ 以外的顶点。

上面这个性质指在图中，顶点 v 所对应的随机变量的概率只和相邻顶点的随机变量的概率有关。


### 最大团

给定一个概率无向图 G，团的定义为任意两个顶点在图中相邻（有边连接），最大团 C 则是保证顶点两两相邻的最大集合。概率无向图的联合概率分布 $P(Y)$ 可以写成所有最大团上的函数 $\varphi_C(Y_C)$ 的乘积

$$
P(Y) = {1 \over Z}\prod\limits_C {{\varphi _C}\left( {{Y_C}} \right)} \quad Z = \sum {\sum\limits_C {{\varphi _C}\left( {{Y_C}} \right)} } 
$$

其中 $\varphi_C(Y_C)$ 是 C 上定义的严格正函数，通常为指数函数。

```mermaid
graph LR
E --- A
F --- B
G --- C
H --- D
E --- F
F --- G
G --- H
```

如上图，由 A E F B 构成的无向图的联合概率分布为

$$
P(A,E,F,B) = {1 \over Z}{e^{f(A,E)}}{e^{g(E,F)}}{e^{h(F,B)}}
$$
### 线性链条件随机场

一般应用比较广泛的是线性链条件随机场，定义为：

设 $X=(X_1,X_2,\cdots, X_n)$ 和 $Y=(Y_1,Y_2,\cdots,Y_n)$ 均为线性表示的随机变量序列，马尔可夫性定义为

$$
P({Y_i}|X,{Y_{j \ne i}}) = P({Y_i}|X,{Y_{i - 1}},{Y_{i + 1}})
$$

>习惯将线性链条件随机场称为条件随机场

假设输入观测序列 $X=(x_1,x_2,\cdots,x_n)$，输出状态序列 $Y=(y_1,y_2,\cdots,y_n)$，将观测到状态之间的函数称为状态特征函数 $s_l(y_i,x,i)$，状态之间的函数称为 $t_k(y_{i-1},y_i,x,i)$

则在随机变量 X 取值为 x 时，随机变量 Y 取值为 y 的条件概率为

$$
P(y|x) = {1 \over {Z(x)}}\exp \left( {\sum\limits_{i,k} {{\lambda _k}{t_k}\left( {{y_{i - 1}},{y_i},x,i} \right)}  + \sum\limits_{i,l} {{u_l}{s_l}\left( {{y_i},x,i} \right)} } \right)
$$
其中 $\lambda_k$ 和 $\mu_l$ 为权值。$t_k$ 和 $s_l$ 特征函数是二值函数，函数值为 0 或者 1。



## 无监督学习

### 自组织映射（self-organizing map）

#### 概述

自组织映射（SOM）是一种无监督机器学习技术，用于生成高维数据集的低维（通常是二维）表示，同时保留数据的拓扑结构。例如，在$n$维观测值中测量的$p$维变量的数据集可以表示为具有相似变量值的观测值聚类，这些聚类可以可视化为二维“地图”，这样近端聚类中的观测值比远端聚类中的观测值具有更相似的值。这可以使高维数据更易于可视化和分析。

SOM是一种人工神经网络，但使用竞争学习进行训练，而不是使用其他人工神经网络使用的纠错学习（如梯度下降的反向传播）。SOM 有时也被称为 Kohonen 地图或 Kohonen 网络。与大多数人工神经网络一样，自组织映射以两种模式运行：训练和映射。首先，训练使用输入数据集（“输入空间”）来生成输入数据的低维表示（“地图空间”）。其次，映射使用生成的映射对其他输入数据进行分类。

在大多数情况下，训练的目标是将具有 $p$ 维的输入空间表示为具有二维的地图空间。具体来说，具有$p$变量的输入空间被称为具有 $p$ 维。映射空间由称为“节点”或“神经元”的组件组成，这些组件被排列成具有二维的六边形或矩形网格。节点的数量及其排列是根据数据分析和探索的更大目标预先指定的。映射空间中的每个节点都与一个“权重”向量相关联，该向量是节点在输入空间中的位置。虽然地图空间中的节点保持固定，但训练包括将权重向量向输入数据移动（减少距离度量，例如欧几里得距离），而不会破坏地图空间引起的拓扑。训练后，地图可用于通过查找与输入空间向量最接近权重向量（最小距离度量）的节点来对输入空间的其他观测值进行分类。

#### 学习算法

在自组织映射中学习的目标是使网络的不同部分对某些输入模式做出类似的响应。这在一定程度上是由人脑大脑皮层的不同部分如何处理视觉、听觉或其他感官信息所致。神经元的权重被初始化为小的随机值，或者从两个最大的主成分特征向量跨越的子空间中均匀采样。使用后一种选择，学习速度要快得多，因为初始权重已经很好地近似了 SOM 权重。

训练采用竞争性学习，当训练样本被馈送到网络时，将计算其到所有权重向量的欧几里得距离。权重向量与输入最相似的神经元称为最佳匹配单元（BMU）。在SOM网格中，BMU和靠近它的神经元的权重根据输入向量进行调整。变化的幅度随着时间和与BMU的网格距离而减小。权重向量为$W_v(s)$的神经元 $v$ 的更新公式为
$$
W_v(s+1)=W_v(s)+\theta(u,v,s)\cdot \alpha(s)\cdot (D(t)-W_v(s))
$$
其中$s$是步数，$t$是训练样本的索引，$u$是输入向量$D(t)$的BMU指数，$\alpha(s)$是单调递减学习系数，$\theta(u,v,s)$是邻域函数，它给出了步骤$s$中神经元$u$和神经元$v$之间的距离。根据实现的不同，$t$可以系统地扫描训练数据集（$t$是 0， 1， 2...T-1，然后重复，T 是训练样本的大小）， 从数据集中随机抽取（bootstrap采样）， 或者实现一些其他的采样方法（如jackknifing）。

邻域函数$\theta(u,v,s)$（也称为横向相互作用函数）取决于BMU（神经元 $u$）和神经元$v$之间的网格距离。在最简单的形式中，所有足够接近BMU的神经元为1，其他神经元为0，但高斯函数和Mexican-hat函数也是常见的选择。无论函数形式如何，邻域函数都会随时间而缩小。 一开始，当邻域很宽泛时，自组织就会在整个范围内发生。当邻域缩小到只有几个神经元时，权重会收敛到局部估计值。在一些实现中，学习系数$\alpha$和邻域函数$\theta$随着$s$的增加而稳步下降，在其他实现中（特别是那些$t$扫描训练数据集的实现），它们以逐步方式减少，每T步增加一次。

对于每个输入向量，对（通常很大）周期数$\lambda$重复此过程。网络最终会将输出节点与输入数据集中的组或模式相关联。如果可以命名这些模式，则可以将这些名称附加到训练网络中的关联节点。在映射过程中，将有一个获胜神经元：其权重向量最接近输入向量的神经元。这可以通过计算输入向量和权重向量之间的欧几里得距离来简单地确定。

**算法**

1. 随机化地图中的节点权重向量

2. for $s=0,1,2,...,\lambda$

   1. 随机选择一个输入向量$D(t)$；
   2. 在地图中查找最接近输入向量的节点。此节点是最佳匹配单元（BMU），用$v$表示；
   3. 对每个节点$v$，通过拉近输入向量来更新其向量：

   $$
   W_v(s+1)=W_v(s)+\theta(u,v,s)\cdot \alpha(s)\cdot (D(t)-W_v(s))
   $$

$s$ 是当前迭代，$\lambda$ 是迭代限制，$t$ 是输入数据集 $\bf{D}$ 中目标输入数据向量的索引

关键的设计选择是 SOM 的形状、邻域函数和学习率时间表。邻域函数的思想是使 BMU 更新最多，其近邻更新更少，依此类推。学习率的想法是安排它，使地图更新在开始时很大，并逐渐停止更新。

例如，如果想用方形网格来学习SOM，我们可以使用邻域函数可以使BMU完全更新，最近的邻居更新一半，它们的邻居再次更新一半，等等。
$$
\theta \left( {\left( {i,j} \right),\left( {i',j'} \right),s} \right) = {1 \over {{2^{\left| {i - i'} \right| + \left| {j - j'} \right|}}}} = \left\{ \matrix{
  1\qquad {\rm{if}}\;i = i',j = j'   \cr 
  1/2\;\;\;{\rm{if}}\;\left| {i - i'} \right| + \left| {j - j'} \right| = 1 \hfill \cr 
  1/4\;\;\;{\rm{if}}\;\left| {i - i'} \right| + \left| {j - j'} \right| = 2 \hfill \cr 
   \cdots \quad \;\; \cdots  \cr}  \right.
$$
学习率可以是简单的线性：$\alpha(s)=1-s/\lambda$

**替代算法**

1. 随机化地图节点的权重向量

2. 遍历输入数据集中的每个输入向量

   1. 遍历映射中的每个节点
      1. 使用欧式距离公式查找输入向量与地图节点权重向量之间的相似性
      2. 跟踪产生最小距离的节点（此节点便是最佳匹配单元，BMU）
   2. 通过将BMU附近的节点（包括BMU本身）拉近输入向量来更新BMU附近的节点

   $$
   W_v(s+1)=W_v(s)+\theta(u,v,s)\cdot \alpha(s)\cdot (D(t)-W_v(s))
   $$

3. 从步骤2开始增加$s$并重复，同时$s<\lambda$


# 图神经网络


对于pytorch，可以使用 [torch_geometric 库](https://pytorch-geometric.readthedocs.io/en/latest/index.html)


下载命令（注意 pytorch 和 cuda 版本）[Installation — pytorch_geometric](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html)

```sh
pip install torch_geometric
pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.2+cu118.html
```

torch_geometric 库中有许多的现成的模型组件。PyG 中通过一个 `torch_geometric.data.Data` 实例描述单个图，默认有以下属性

`data.X`：节点的特征矩阵 `[num_nodes, num_node_features]`

`data.edge_index`：图的连通性矩阵 `[2, num_edges]` 类型为 long

`data.edge_attr`：边的特征矩阵 `[num_edges, num_edge_features]`

`data.y`：训练时的目标

`data.pos`：节点的位置矩阵 `[num_nodes, num_dimensions]`

注意，data 并不局限于这些属性，也并不需要所有的属性都被赋值。




## 简介

[图神经网络从入门到入门 - 知乎](https://zhuanlan.zhihu.com/p/136521625)

普通的全连接层为特征矩阵 $X$ 乘以权重矩阵 $W$，图神经网络在此基础上还乘上了一个邻接矩阵 $A$。

普通的全连接层：$H=\sigma(XW)$，图神经网络：$H=\sigma(AXW)$


Graph Convolution Networks（GCN）是开山之作，将图像处理中的卷积操作简单的用到图结构数据处理中。具体操作如下

$$
{h_v} = f\left( {{1 \over {\left| {N(v)} \right|}}\sum\nolimits_{u  \in N(v)} {W{x_u}}  + b} \right)
$$

其中，$N(v)$ 为 v 的邻接节点，即将 v 的邻接节点聚合起来再做一个线性映射。


# 损失函数


## 交叉熵损失

两个分布 $p(x)$ 和 $q(x)$之间的交叉熵定义为

$$
H(p,q) = p(x)\log {1 \over {q(x)}} =  - p(x)\log q(x)
$$
下面是一个例子，有三个分布 $p(x)$、$q_1(x)$ 和 $q_2(x)$


|            | $x_1$ | $x_2$ | $x_3$ |
| ---------- | ----- | ----- | ----- |
| $p(x_i)$   | 1     | 0     | 0     |
| $q_1(x_i)$ | 0.5   | 0.4   | 0.1   |
| $q_2(x_i)$ | 0.8   | 0.1   | 0.1   |
那么 $p(x)$ 和 $q_1(x)$ 的交叉熵为

$$
H(p,{q_1}) =  - \sum\limits_{i = 1} {p({x_i})\log {q_1}({x_i})}  = \log 2
$$
由于一般正标签为1，交叉熵损失可以定义为

$$
L =  - \sum\limits_{i = 1} {\log q({x_i})} 
$$
这里的 $q(x_i)$ 一般经过softmax。


# 经典模型


## 图像识别

一些可供使用的库：

1. [huggingface/pytorch-image-models: The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more](https://github.com/huggingface/pytorch-image-models)



### Resnet

Resnet 的基本模块为 Res Block，基本结构如下（右图为加入输入输出尺寸不同时的结构）

![](https://d2l.ai/_images/resnet-block.svg)


输入尺寸为 224 × 224。

### Res2net

将特征图（$B\times C \times H\times W$）先经过一个 1 x 1 卷积+ BN + Relu，之后在通道维度分割为 s 个特征子图（$B\times (C/S) \times H\times W$），对于 s 个通道子图进行如下处理
$$
{y_i} = \left\{ \matrix{
  {x_i}\quad i = 1 \hfill \cr 
  {K_i}({x_i})\quad i = 2 \hfill \cr 
  {K_i}({x_i} + {y_{i - 1}})\quad 2 < i \le s \hfill \cr}  \right.
$$
这里的 $K_i$​ 可以是简单的 3 × 3 卷积 + BN + Relu，也可以是分组卷积（减少计算量），最后经过一个 1 × 1 的卷积 + BN，再经过一个可选的 SE block 与输入相加连接，最后再经过一个 Relu。


### ResNeXt

将 `[256, 3×3, 256]` 卷积改为 `[256, 1×1, 128]+[128, 3×3, 128, groups=32]+[128, 1×1, 256]`

下面三种网络架构等效

![](https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/ResNeXt.jpg)



### SENet

用于图像分类，在 Resnet 的基础上加入了 SE Block，SE Block 的架构如下：

<img src="https://i-blog.csdnimg.cn/blog_migrate/16d6a483fe3f04bbc9a01adc6151804d.png" style="zoom:67%;" />

注意此处的 FC 可以是二维卷积，一维卷积等。SENet 将 SE Block 加在每个小卷积块之后（整个模块可能有几十个SE Block，r=16），一些更加简单的模型将 SE Block 加在模块之后（可能整个模型中只有3、4个 SE Block），控制好r的大小，对于模型的计算量影响很小。

### VIT

将二维图片 $x\in R^{H\times W\times C}$ 重整为 $x_p\in R^{N\times(P^2\cdot C)}$，P 为一个图片块的长宽，$N=HW/P^2$​。

输入 $x\in R^{B\times C \times H \times W}$ 经过一个二维卷积 `[3, 768]`（`kernel_size` 和 `stride_size` 相等，均为 16），对于一个 3 × 224 × 224 的图片，可以得到 768 × 14 × 14 的特征，然后摊平最后两个维度得到 196 × 768 的特征，196 相当于文本中的token长度，768 为token的特征维度。

然后进行位置编码，可以设置一个参数进行学习

```python
self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)
```

然后将分类或者回归token（可选）连接到第一个维度，即token长度上

```python
self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
self.reg_token = nn.Parameter(torch.zeros(1, reg_tokens, embed_dim))
```

之后便是若干个 Block，Block 中包含了常规的多头自注意力和前馈层。

$$
\eqalign{
  & {z_0} = \left[ {{x_{class}};x_p^1E;x_p^2E; \cdots ;x_p^NE} \right] + {E_{pos}}\quad E \in {R^{({P^2} \cdot C) \times D}},{E_p} \in {R^{\left( {N + 1} \right) \times D}}  \cr 
  & {z_l}' = MSA(LN({z_{l - 1}})) + {z_{l - 1}}\quad l = 1...L  \cr 
  & {z_l} = MLP(LN({z_l}')) + {z_l}'\quad l = 1...L  \cr 
  & y = LN(z_L^0) \cr} 
$$

计算多头自注意力（MSA）时，代码如下

```python
class Attention(nn.Module):  
    def __init__(  
            self,  
            dim: int,  
            num_heads: int = 8,  
            qkv_bias: bool = False,  
            qk_norm: bool = False,  
            proj_bias: bool = True,  
            attn_drop: float = 0.,  
            proj_drop: float = 0.,  
            norm_layer: nn.Module = nn.LayerNorm,  
    ) -> None:  
        super().__init__()  
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'  
        self.num_heads = num_heads  
        self.head_dim = dim // num_heads  
        self.scale = self.head_dim ** -0.5  
  
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()  
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()  
        self.attn_drop = nn.Dropout(attn_drop)  
        self.proj = nn.Linear(dim, dim, bias=proj_bias)  
        self.proj_drop = nn.Dropout(proj_drop)  
  
    def forward(self, x: torch.Tensor) -> torch.Tensor:  
        B, N, C = x.shape  
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  
        q, k, v = qkv.unbind(0)  
        q, k = self.q_norm(q), self.k_norm(k)  
  
		x = F.scaled_dot_product_attention(  
			q, k, v, dropout_p=self.attn_drop.p if self.training else 0.,  
		)  
		
        x = x.transpose(1, 2).reshape(B, N, C)  
        x = self.proj(x)  
        x = self.proj_drop(x)  
        return x
```

`MLP` 为两个全连接层，中间加一个激活函数，全连接层的中间维度为 4 倍输入维度。

在经过若干个 Block 提取特征后，经过 norm 和其它层进行分类等操作。


### Swin Transformer

视觉模态的信息与文本信息的差距体现在视觉信息会存在快速变化，此外图像的分辨率更高。VIT 中计算自注意力时对于整张图进行计算，Swin Transformer 在局部窗口中计算自注意力，假设每个窗口包含了 $M\times M$个图像块，窗口之间不重叠。为了加入窗口间的连接，引入了 Shifted Window 多头自注意力。Shifted window 在计算时

![](https://img2022.cnblogs.com/blog/1571518/202202/1571518-20220211104213844-333037308.png)

Shifted Window MSA 计算时先对图片进行不均匀的划分，再进行类似围绕中心旋转的操作，再进行均匀的分割，对每个小块计算MSA，最后再倒回来。

输入 $x\in R^{B\times C \times H \times W}$ 经过一个二维卷积 `[3, 96]`（`kernel_size` 和 `stride_size` 相等，均为 4），对于一个 3 × 224 × 224 的图片，可以得到 96 × 56 × 56 的特征，与 VIT 不同，Swin Transformer不会摊平最后两个维度。然后经过若干个 Block，单个 Block 中包含了若干个 Swin Transformer Block。Swin Transformer Block 与 VIT 中的 Block 类似，但是注意力计算方式不同。对于单个样本 $x_i\in R^{H'\times W'\times C’}$ 分割成多个patch，单个patch 的大小为 `7 × 7`，对于 `56 × 56` 大小的图像，可以分割成 `8 × 8` 个 patch（放在批次这个维度上），$x_i$ 转为 $R^{64B \times 49 \times 96}$ ，这时就可以参考 VIT 中的注意力进行计算，计算完注意力后，将特征图重新翻转过来。

上面是普通的 MSA 计算方式，每一个普通的 MSA 之后的 Block 中的注意力为 Shifted Window MSA，即在计算注意力前先进行 shift

```python
shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))
```

计算完注意力之后再 Shift 回来

```python
x = torch.roll(shifted_x, shifts=self.shift_size, dims=(1, 2))
```

在输入 MLP 之前，将中间两个维度展平，变成三维，经过 MLP 后再 reshape 回来。

每隔几个 Block 需要融合图像块，先将 $x\in R^{N\times H\times W \times C}$ 转为 $x\in R^{N\times (H/2) \times 2\times(W/2)\times 2\times C}$，再转为$x\in R^{N\times (H/2) \times(W/2)\times 4C}$，最后经过一个全连接层，输入为 $4\times C$，输出为 $C$ 得到融合后的特征图。

>相比于 VIT，Swin Transformer 的适用范围更广。


### ConvNeXt

ConvNeXt 改进了Block的设计，先是 7×7 的二维卷积，然后经过reshape，经过两个 1×1 卷积，具体实现时使用两个全连接层。

![](https://user-images.githubusercontent.com/8370623/180626875-fe958128-6102-4f01-9ca4-e3a30c3148f9.png)

ConvNext还有V2版本。

### CRNN

[meijieru/crnn.pytorch: Convolutional recurrent network in pytorch](https://github.com/meijieru/crnn.pytorch)

用于OCR

CRNN 由两部分网络组成，首先是CNN，，CNN 总共有 7 层（记为 0 - 6），每一层由 Conv2d +Relu + BatchNorm（BatchNorm只在2、4，6三层有），0-1，1-2，3-4，5-6之间有 MaxPool2d。通过 Cnn 将高转为1。第二部分为两个双向LSTM层，每个双向LSTM层包括一个双向LSTM和Linear。


### EdgeNeXt

架构见下图，Conv Encoder 为 DWConv + FF 的设计，SDTA Encoder 则采用了类似 Res2net 的设计。

![](https://github.com/mmaaz60/EdgeNeXt/raw/main/images/EdgeNext.png)


## 扩散模型


### DDPM 

扩散模型包括两个过程，**前向过程**：在扩散模型中，前向过程指的是将原始数据（如图像）逐步加入噪声，使其逐渐变得完全无序。这个过程通常是通过一个马尔可夫链来实现，即每一步都将数据逐渐转变成高斯噪声。具体来说，前向过程将原始数据逐渐添加噪声，直到数据变为纯噪声。这个过程是可逆的，且需要设定一定的噪声步数和噪声分布。**反向过程**：反向过程是扩散模型的核心，也是生成数据的关键。反向过程的目标是从纯噪声出发，通过逐步去噪，恢复原始数据。在反向过程中，模型学会如何去除噪声，以便从随机噪声生成逼真的数据。通常，反向过程是通过训练神经网络来预测每一步的去噪过程，网络输出的是去噪后的样本。反向过程的训练通常需要最小化一个损失函数，损失函数衡量的是生成样本与真实样本之间的差异，通常是基于均方误差（MSE）来度量。

**训练阶段**： 在训练阶段，扩散模型使用一个输入数据集，通过添加不同强度的噪声来训练模型。模型的目标是学习如何从噪声中恢复数据。通过反向传播，网络会逐步优化其去噪能力，最终能够将噪声数据恢复为逼真的样本。

**推理阶段**： 在推理阶段，扩散模型从一个随机的噪声样本开始，逐步通过反向去噪过程生成新的数据。这一过程通常比传统的生成模型（如GANs）更稳定且容易训练。


扩散模型是隐变量模型，形式为 ${p_\theta }({x_0}) = \int {{p_\theta }({x_{0:T}})} d{x_{1:T}}$，这里的 $x_1, ..., x_T$ 和数据 $x_0\sim q(x_0)$ 相同维度。联合分布 $p_{\theta}(x_{0:T})$ 称为反向过程，反向过程定义为起始于 $p(x_T)=N(x_T;0,I)$ 的马尔可夫链。

$$
{p_\theta }({x_{0:T}}) = p({x_T})\prod\limits_{t = 1}^T {{p_\theta }({x_{t - 1}}|{x_t})}
$$
这里的 $p_{\theta}(x_{t-1}|x_t)$ 是需要学习的高斯函数

$$
{p_\theta }({x_{t - 1}}|{x_t}) = N({x_{t - 1}};{\mu _\theta }({x_t},t),{\Sigma _\theta }({x_t},t))
$$
前向过程 $q(x_{1:T}|x_0)$ 是一个马尔科夫链，每次根据方差 $\beta_1,..., \beta_T$ 在输入中加入高斯噪声

$$
q({x_{0:T}}) = q({x_0})\prod\limits_{t = 1}^T {q({x_t}|{x_{t - 1}})}  \Rightarrow q({x_{1:T}}|{x_0}) = \prod\limits_{t = 1}^T {q({x_t}|{x_{t - 1}})} 
$$
$q(x_t|x_{t-1})$ 也是一个高斯函数

$$
q({x_t}|{x_{t - 1}}) = N\left( {{x_t};\sqrt {1 - {\beta _t}} {x_{t - 1}},{\beta _t}I} \right)
$$
训练时优化负对数似然的变分下界

$$
\eqalign{
  & E\left[ { - \log {p_\theta }({x_0})} \right] = E\left[ { - \log \left( {\int {{p_\theta }({x_{0:T}})} d{x_{1:T}}} \right)} \right]  \cr 
  &  = E\left[ { - \log \left( {\int {{p_\theta }({x_{0:T}})} d{x_{1:T}}{{q({x_{1:T}}|{x_0})} \over {q({x_{1:T}}|{x_0})}}} \right)} \right] = E\left[ { - \log \left( {\int {q({x_{1:T}}|{x_0})} d{x_{1:T}}{{{p_\theta }({x_{0:T}})} \over {q({x_{1:T}}|{x_0})}}} \right)} \right]  \cr 
  &  \le E\left[ { - \log \left( {{{{p_\theta }({x_{0:T}})} \over {q({x_{1:T}}|{x_0})}}} \right)} \right] = E\left[ { - \log p({x_T})\left( {{{\prod\limits_{t = 1}^T {{p_\theta }({x_{t - 1}}|{x_t})} } \over {\prod\limits_{t = 1}^T {q({x_t}|{x_{t - 1}})} }}} \right)} \right]  \cr 
  &  = E\left[ { - \log p({x_T}) - \sum\limits_{t \ge 1} {\log {{{p_\theta }({x_{t - 1}}|{x_t})} \over {q({x_t}|{x_{t - 1}})}}} } \right] = L \cr} 
$$

$\beta_t$ 可以通过重参数化学习或者作为超参数，反向过程的表达能力由 $p_{\theta}(x_{t-1}|x_t)$ 的选择保证。前向过程允许在任意时间步长 t 上以封闭形式对 $x_t$ 进行采样，使用记号：$\alpha_t=1-\beta_t$，${\overline \alpha  _t} = \prod\nolimits_{s = 1}^t {{\alpha _s}}$，有

$$
q({x_t}|{x_0}) = N\left( {{x_t};\sqrt {{{\bar \alpha }_t}} {x_0},(1 - \bar \alpha_t )I} \right)
$$
上式的推导过程如下：

使用重参数化技巧对 $x_t$ 采样得到

$$
{x_t} = \sqrt {{\alpha _t}} {x_{t - 1}} + \sqrt {1 - {\alpha _t}} {\varepsilon _{t - 1}}
$$

同理，对 $x_{t-1}$ 采样得到：

$$
{x_{t - 1}} = \sqrt {{\alpha _{t - 1}}} {x_{t - 2}} + \sqrt {1 - {\alpha _{t - 1}}} {\varepsilon _{t - 2}}
$$

带入 $x_t$ 的采样结果得到

$$
\eqalign{
  & {x_t} = \sqrt {{\alpha _t}} \left( {\sqrt {{\alpha _{t - 1}}} {x_{t - 2}} + \sqrt {1 - {\alpha _{t - 1}}} {\varepsilon _{t - 2}}} \right) + \sqrt {1 - {\alpha _t}} {\varepsilon _{t - 1}}  \cr 
  &  = \sqrt {{\alpha _t}{\alpha _{t - 1}}} {x_{t - 2}} + \sqrt {{{\left( {\sqrt {{\alpha _t} - {\alpha _t}{\alpha _{t - 1}}} } \right)}^2} + {{\left( {\sqrt {1 - {\alpha _t}} } \right)}^2}} {{\bar \varepsilon }_{t - 2}}  \cr 
  &  = \sqrt {{\alpha _t}{\alpha _{t - 1}}} {x_{t - 2}} + \sqrt {1 - {\alpha _t}{\alpha _{t - 1}}} {{\bar \varepsilon }_{t - 2}} = ... \cr} 
$$
其中 $\bar \varepsilon_t$ 是两个高斯噪声的混合，混合后的方差等于两个噪声的方差之和。最终的带入结果为

$$
{x_t} = \sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon 
$$

由上可以定义 $x_t$ 的 snr 为 
$$
{{{{\bar \alpha }_t}} \over {1 - {{\bar \alpha }_t}}}
$$

逆前向过程的真实分布 $q(x_{t-1}|x_t)$ 无法直接求解，但是加上条件 $x_0$ 的后验概率 $q(x_{t-1}|x_t,x_0)$ 可以求解

$$
q({x_{t - 1}}|{x_t},{x_0}) = {{q({x_t},{x_{t - 1}},{x_0})} \over {q({x_t},{x_0})}} = {{q({x_0})q({x_{t - 1}}|{x_0})q({x_t}|{x_{t - 1}},{x_0})} \over {q({x_0})q({x_t}|{x_0})}} = q({x_t}|{x_{t - 1}},{x_0}){{q({x_{t - 1}}|{x_0})} \over {q({x_t}|{x_0})}}
$$
由于前向过程为马尔可夫链，所以有

$$
q({x_t}|{x_{t - 1}},{x_0}) = q({x_t}|{x_{t - 1}}) = N({x_t};\sqrt {1 - {\beta _t}} {x_{t - 1}},{\beta _t}I)
$$
又有

$$
q({x_t}|{x_0}) = N\left( {{x_t};\sqrt {{{\bar \alpha }_t}} {x_0},(1 - {{\bar \alpha }_t})I} \right)\quad q({x_{t - 1}}|{x_0}) = N\left( {{x_{t - 1}};\sqrt {{{\bar \alpha }_{t - 1}}} {x_0},(1 - {{\bar \alpha }_{t - 1}})I} \right)
$$

将概率密度函数带入 $q(x_{t-1}|x_t,x_0)$ 可得

$$
\eqalign{
  & q({x_{t - 1}}|{x_t},{x_0}) \propto \exp \left( { - {1 \over 2}\left( {{{{{\left( {{x_t} - \sqrt {1 - {\beta _t}} {x_{t - 1}}} \right)}^2}} \over {{\beta _t}}} + {{{{\left( {{x_{t - 1}} - \sqrt {{{\bar \alpha }_{t - 1}}} {x_0}} \right)}^2}} \over {1 - {{\bar \alpha }_{t - 1}}}} - {{{{\left( {{x_t} - \sqrt {{{\bar \alpha }_t}} {x_0}} \right)}^2}} \over {1 - {{\bar \alpha }_t}}}} \right)} \right)  \cr 
  &  \propto \exp \left( { - {1 \over 2}\left( {\left( {{1 \over {1 - {{\bar \alpha }_{t - 1}}}} + {{1 - {\beta _t}} \over {{\beta _t}}}} \right)x_{t - 1}^2 - \left( {{{2\sqrt {1 - {\beta _t}} {x_t}} \over {{\beta _t}}} + {{2\sqrt {{{\bar \alpha }_{t - 1}}} {x_0}} \over {1 - {{\bar \alpha }_{t - 1}}}}} \right){x_{t - 1}} + C({x_t},{x_0})} \right)} \right) \cr} 
$$

与正态分布的指数项展开式 $\exp \left( { - {1 \over 2}\left( {{{{{\left( {x - \mu } \right)}^2}} \over {{\sigma ^2}}}} \right)} \right) = \exp \left( { - {1 \over 2}\left( {{1 \over {{\sigma ^2}}}{x^2} - {{2\mu } \over {{\sigma ^2}}}x + {{{\mu ^2}} \over {{\sigma ^2}}}} \right)} \right)$ 相对比，可以得到方差和均值为

$$
\eqalign{
  & {\sigma ^2} = {1 \over {\left( {{1 \over {1 - {{\bar \alpha }_{t - 1}}}} + {{1 - {\beta _t}} \over {{\beta _t}}}} \right)}} = {{1 - {{\bar \alpha }_{t - 1}}} \over {1 - {{\bar \alpha }_t}}}{\beta _t}  \cr 
  & \mu  = \left( {{{2\sqrt {1 - {\beta _t}} {x_t}} \over {{\beta _t}}} + {{2\sqrt {{{\bar \alpha }_{t - 1}}} {x_0}} \over {1 - {{\bar \alpha }_{t - 1}}}}} \right){{{\sigma ^2}} \over 2} = {{\sqrt {{\alpha _t}} \left( {1 - {{\bar \alpha }_{t - 1}}} \right){x_t}} \over {1 - {{\bar \alpha }_t}}} + {{\sqrt {{{\bar \alpha }_{t - 1}}} {\beta _t}{x_0}} \over {1 - {{\bar \alpha }_t}}} \cr} 
$$

考虑 KL 散度

$$
{D_{KL}}\left[ {P(x)||Q(x)} \right] = \sum\nolimits_{x \in X} {\left[ {P(x)\log {{P(x)} \over {Q(x)}}} \right]}  = {E_{x \sim p(x)}}\left[ {\log {{P(x)} \over {Q(x)}}} \right]
$$
将损失函数重写为

$$
\eqalign{
  & L = E\left[ { - \log p({x_T}) - \sum\limits_{t \ge 1} {\log {{{p_\theta }({x_{t - 1}}|{x_t})} \over {q({x_t}|{x_{t - 1}})}}} } \right]  \cr 
  &  = E\left[ { - \log p({x_T}) + \sum\limits_{t = 2} {\log {{q({x_t}|{x_{t - 1}})} \over {{p_\theta }({x_{t - 1}}|{x_t})}}}  + \log {{q({x_1}|{x_0})} \over {{p_\theta }({x_0}|{x_1})}}} \right] \cr} 
$$
因为马尔可夫性

$$
q({x_t}|{x_{t - 1}}) = q({x_t}|{x_{t - 1}},{x_0}) = {{q({x_t},{x_{t - 1}},{x_0})} \over {q({x_{t - 1}},{x_0})}} = {{q({x_t}|{x_0})q({x_{t - 1}}|{x_t},{x_0})} \over {q({x_{t - 1}}|{x_0})}}
$$

带入上面

$$
L = E\left[ { - \log p({x_T}) + \sum\limits_{t = 2} {\log {{q({x_{t - 1}}|{x_t},{x_0})} \over {{p_\theta }({x_{t - 1}}|{x_t})}}}  + \sum\limits_{t = 2} {\log {{q({x_t}|{x_0})} \over {q({x_{t - 1}}|{x_0})}}}  + \log {{q({x_1}|{x_0})} \over {{p_\theta }({x_0}|{x_1})}}} \right]
$$

其中 

$$
\sum\limits_{t = 2} {\log {{q({x_t}|{x_0})} \over {q({x_{t - 1}}|{x_0})}}}  = \log \left( {{{q({x_2}|{x_0})} \over {q({x_1}|{x_0})}} \cdot {{q({x_3}|{x_0})} \over {q({x_2}|{x_0})}} \cdots {{q({x_T}|{x_0})} \over {q({x_{T - 1}}|{x_0})}}} \right) = \log {{q({x_T}|{x_0})} \over {q({x_1}|{x_0})}}
$$

所以

$$
\eqalign{
  & L = E\left[ { - \log p({x_T}) + \sum\limits_{t = 2} {\log {{q({x_{t - 1}}|{x_t},{x_0})} \over {{p_\theta }({x_{t - 1}}|{x_t})}}}  + \sum\limits_{t = 2} {\log {{q({x_t}|{x_0})} \over {q({x_{t - 1}}|{x_0})}}}  + \log {{q({x_1}|{x_0})} \over {{p_\theta }({x_0}|{x_1})}}} \right]  \cr 
  &  = E\left[ { - \log p({x_T}) + \sum\limits_{t = 2} {\log {{q({x_{t - 1}}|{x_t},{x_0})} \over {{p_\theta }({x_{t - 1}}|{x_t})}}}  + \log {{q({x_T}|{x_0})} \over {q({x_1}|{x_0})}} + \log {{q({x_1}|{x_0})} \over {{p_\theta }({x_0}|{x_1})}}} \right]  \cr 
  &  = E\left[ { - \log p({x_T}) + \sum\limits_{t = 2} {\log {{q({x_{t - 1}}|{x_t},{x_0})} \over {{p_\theta }({x_{t - 1}}|{x_t})}}}  + \log {{q({x_T}|{x_0})} \over {{p_\theta }({x_0}|{x_1})}}} \right]  \cr 
  &  = E\left[ {\log {{q({x_T}|{x_0})} \over {p({x_T})}} + \sum\limits_{t = 2} {\log {{q({x_{t - 1}}|{x_t},{x_0})} \over {{p_\theta }({x_{t - 1}}|{x_t})}}}  - \log {p_\theta }({x_0}|{x_1})} \right] \cr} 
$$

用 KL 散度表示为

$$
L_{\rm{vlb}} = {D_{KL}}\left[ {q({x_T}|{x_0})||p({x_T})} \right] + \sum\limits_{t = 2} {{D_{KL}}\left[ {q({x_{t - 1}}|{x_t},{x_0})||{p_\theta }({x_{t - 1}}|{x_t})} \right]}  - E\left[ {\log {p_\theta }({x_0}|{x_1})} \right]
$$
第一项为最后得到的噪声分布 $p(x_T)$ 和先验分布 $q(x_T|x_0)$ 的KL散度，在前向过程中 q 没有需要学习的参数，所以最后的 $q(x_T|x_0)$ 近似于标准高斯噪声，而 $p(x_T)$ 为标准高斯噪声，所以这一项差异很小。

最后一项可以看成是原始数据重建，这一项可以用估计的 $N\left( {{x_0};{\mu _\theta }({x_1},1),\sum\nolimits_\theta  {({x_1},1)} } \right)$ 来计算。

$$
{p_\theta }\left( {{x_0}|{x_1}} \right) = \prod\nolimits_{i = 1}^D {\int_{{\delta _ - }x_0^i}^{{\delta _ + }x_0^i} {N\left( {{x_0};{\mu _\theta }({x_1},1),\sum\nolimits_\theta  {({x_1},1)} } \right)} } dx
$$
计算高斯分布落在ground truth为中心且范围大小为 2/255 时的概率积分。

第二项表示的是估计分布 $p_{\theta}(x_{t-1}|x_t)$ 和真实后验分布 $q(x_{t-1}|x_t,x_0)$ KL 散度，减小 KL 散度使得估计的逆扩散过程和依赖真实数据的逆扩散过程近似一致。

根据上面的计算，真实后验分布 
$$
q({x_{t - 1}}|{x_t},{x_0}) = N\left( {{x_{t - 1}};{{\tilde \mu }_t}({x_t},{x_0}),{{\tilde \beta }_t}I} \right)
$$

其中
$$
{{\tilde \mu }_t}({x_t},{x_0}) = {{\sqrt {{\alpha _t}} \left( {1 - {{\bar \alpha }_{t - 1}}} \right){x_t}} \over {1 - {{\bar \alpha }_t}}} + {{\sqrt {{{\bar \alpha }_{t - 1}}} {\beta _t}{x_0}} \over {1 - {{\bar \alpha }_t}}}\quad {{\tilde \beta }_t} = {{1 - {{\bar \alpha }_{t - 1}}} \over {1 - {{\bar \alpha }_t}}}{\beta _t}
$$
网络希望拟合的目标分布为

$$
{p_\theta }\left( {{x_{t - 1}}|{x_t}} \right) = N\left( {{x_{t - 1}};{\mu _\theta }({x_t},t),\sum\nolimits_\theta  {({x_t},t)} } \right)
$$
DDPM 中通过网络训练得到 ${{\mu _\theta }({x_t},t)}$，而采用固定的方差 $\sigma_t^2 I$，这里的 $\sigma_t^2$ 可以设为 $\beta_t$ 或者 ${{\tilde \beta }_t}$。

两个分布均为高斯分布的 KL 散度公式为

$$
KL\left( {{p_1}||{p_2}} \right) = {1 \over 2}\left( {tr\left( {\Sigma _2^{ - 1}{\Sigma _1}} \right)} \right) + {\left( {{\mu _2} - {\mu _1}} \right)^T}\Sigma _2^{ - 1}\left( {{\mu _2} - {\mu _1}} \right) - n + \log {{\det \left( {{\Sigma _2}} \right)} \over {\det \left( {{\Sigma _1}} \right)}}
$$
因此第二项可以计算为
$$
\eqalign{
  & {D_{KL}}\left( {q({x_{t - 1}}|{x_t},{x_0})||{p_\theta }\left( {{x_{t - 1}}|{x_t}} \right)} \right) = {D_{KL}}\left( {N\left( {{x_{t - 1}};{{\tilde \mu }_t}({x_t},{x_0}),\sigma _t^2I} \right)||N\left( {{x_{t - 1}};{\mu _\theta }({x_t},t),\sigma _t^2I} \right)} \right)  \cr 
  &  = {1 \over {2\sigma _t^2}}{\left\| {{{\tilde \mu }_t}({x_t},{x_0}) - {\mu _\theta }({x_t},t)} \right\|^2} \cr} 
$$
所以优化目标是两个分布均值的二范数。


这样的优化目标并不是最好的选择，根据扩散过程的推导

$$
{x_t}({x_0},\varepsilon ) = \sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon \quad \varepsilon  \sim N(0,I)
$$

将加噪公式带入均值计算公式，替换 $x_0$，可以将均值写成 $x_t$ 和 $\varepsilon$ 的函数：

$$
\eqalign{
  & {{\tilde \mu }_t}({x_t},{x_0}) = {{\sqrt {{\alpha _t}} \left( {1 - {{\bar \alpha }_{t - 1}}} \right){x_t}} \over {1 - {{\bar \alpha }_t}}} + {{\sqrt {{{\bar \alpha }_{t - 1}}} {\beta _t}{x_0}} \over {1 - {{\bar \alpha }_t}}}  \cr 
  &  = {{\sqrt {{\alpha _t}} \left( {1 - {{\bar \alpha }_{t - 1}}} \right){x_t}} \over {1 - {{\bar \alpha }_t}}} + {{\sqrt {{{\bar \alpha }_{t - 1}}} {\beta _t}} \over {1 - {{\bar \alpha }_t}}}\left( {{{{x_t} - \sqrt {1 - {{\bar \alpha }_t}} \varepsilon } \over {\sqrt {{{\bar \alpha }_t}} }}} \right)  \cr 
  &  = {{{\alpha _t}\left( {1 - {{\bar \alpha }_{t - 1}}} \right){x_t}} \over {\sqrt {{\alpha _t}} \left( {1 - {{\bar \alpha }_t}} \right)}} + {{\left( {1 - {\alpha _t}} \right)\left( {{x_t} - \sqrt {1 - {{\bar \alpha }_t}} \varepsilon } \right)} \over {\sqrt {{\alpha _t}} \left( {1 - {{\bar \alpha }_t}} \right)}}  \cr 
  &  = {1 \over {\sqrt {{\alpha _t}} }}\left( {{x_t} - {{1 - {\alpha _t}} \over {\sqrt {1 - {{\bar \alpha }_t}} }}\varepsilon } \right) \cr} 
$$

从 $p_{\theta}(x_{t-1}|x_t)$ 采样得到的 $x_{t-1}$ 可以重参数化为
$$
{x_{t - 1}} = {1 \over {\sqrt {{\alpha _t}} }}\left( {{x_t} - {{1 - {\alpha _t}} \over {\sqrt {1 - {{\bar \alpha }_t}} }}{\varepsilon _\theta }({x_t},t)} \right) + \sqrt {{{1 - {{\bar \alpha }_{t - 1}}} \over {1 - {{\bar \alpha }_t}}}{\beta _t}} z
$$

其中 $z\sim N(0, 1)$ 为标准正态分布。这个公式便是推理时的公式，推理时，先随机生成一个正态噪声 $x_{T}$，然后根据这个公式计算 $x_{T-1}$，$\varepsilon_{\theta}$ 为预测的噪声，$z$ 则是随机的噪声。

带入损失的计算公式可得

$$
\eqalign{
  & {L_t} = E\left[ {{1 \over {2\sigma _t^2}}{{\left\| {{{\tilde \mu }_t}({x_t},{x_0}) - {\mu _\theta }({x_t},t)} \right\|}^2}} \right]  \cr 
  &  = {E_{{x_0},\varepsilon  \sim N(0,I)}}\left[ {{1 \over {2\sigma _t^2}}{{\left\| {{1 \over {\sqrt {{{ \alpha }_t}} }}\left( {{x_t}({x_0},\varepsilon ) - {{1 - {\alpha _t}} \over {\sqrt {1 - {{\bar \alpha }_t}} }}\varepsilon } \right) - {\mu _\theta }({x_t}({x_0},\varepsilon ),t)} \right\|}^2}} \right] \cr} 
$$
由于 $x_t$ 是 $\mu_{\theta}$ 的输入，其它量均为常数，所以未知量只有 $\varepsilon$，所以可以将 ${{\mu _\theta }({x_t}({x_0},\varepsilon ),t)}$ 定义为

$$
{\mu _\theta }({x_t}({x_0},\varepsilon ),t) = {1 \over {\sqrt {{{ \alpha }_t}} }}\left( {{x_t}({x_0},\varepsilon ) - {{1 - {\alpha _t}} \over {\sqrt {1 - {{\bar \alpha }_t}} }}{\varepsilon _\theta }\left( {{x_t}({x_0},\varepsilon ),t} \right)} \right)
$$
从用神经网络拟合均值转为拟合噪声 $\varepsilon_{\theta}$，新的损失函数写成

$$
\eqalign{
  & {L_t} = {E_{{x_0},\varepsilon  \sim N(0,I)}}\left[ {{1 \over {2\sigma _t^2}}{{\left\| {{1 \over {\sqrt {{\alpha _t}} }}\left( {{x_t} - {{1 - {\alpha _t}} \over {\sqrt {1 - {{\bar \alpha }_t}} }}\varepsilon } \right) - {1 \over {\sqrt {{\alpha _t}} }}\left( {{x_t} - {{1 - {\alpha _t}} \over {\sqrt {1 - {{\bar \alpha }_t}} }}{\varepsilon _\theta }\left( {{x_t}({x_0},\varepsilon ),t} \right)} \right)} \right\|}^2}} \right]  \cr 
  &  = {E_{{x_0},\varepsilon  \sim N(0,I)}}\left[ {{{\beta _t^2} \over {2\sigma _t^2{\alpha _t}\left( {1 - {{\bar \alpha }_t}} \right)}}{{\left\| {\varepsilon  - {\varepsilon _\theta }\left( {\sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon ,t} \right)} \right\|}^2}} \right] \cr} 
$$
DDPM 进一步简化，去掉了权重系数，变成了

$$
{L_{\rm{simple}}} = {\left\| {\varepsilon  - {\varepsilon _\theta }\left( {\sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon ,t} \right)} \right\|^2}
$$

在 DDPM 的原始论文中 beta 使用的是 linear beta schedule，timesteps 是前向过程添加噪声的步数（设置为1000）

```python
def linear_beta_schedule(timesteps):  
    """  
    linear schedule, proposed in original ddpm paper    """    
    scale = 1000 / timesteps  
    beta_start = scale * 0.0001  
    beta_end = scale * 0.02  
    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)
```

在训练时，先对图像进行归一化，从 `[0, 255]` 归一化到 `[-1, 1]`。在前向过程中，使用公式

$$
{x_t} = \sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon 
$$
给图像加噪声（$\varepsilon$ 可以是高斯噪声）。然后将加完噪声的图像送入模型中（原论文是Unet模型）得到预测输出，如果需要预测噪声，那么标签就是刚刚加入的噪声。除此之外，还有预测速度

$$
v = \sqrt {{{\bar \alpha }_t}} \varepsilon  - \sqrt {1 - {{\bar \alpha }_t}} {x_0}
$$
推导过程如下，考虑加噪公式 ${x_t} = \sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon$，两个系数的平方和为1，所以可以令 $\sqrt {{{\bar \alpha }_t}}  = \cos \phi$，$\sqrt {1 - {{\bar \alpha }_t}}  = \sin \phi$，可以得到
$$
{x_t}(\phi ) = \cos \phi {x_0} + \sin \phi \varepsilon
$$

速度的定义为

$$
v(\phi ) = {{d{x_t}(\phi )} \over {d\phi }} = \cos \phi \varepsilon  - \sin \phi {x_0} = \sqrt {{{\bar \alpha }_t}} \varepsilon  - \sqrt {1 - {{\bar \alpha }_t}} {x_0}
$$
联立 $v$ 和加噪公式可以得到预测的 $x_0$ 的公式为

$$
\left\{ \matrix{
  {x_t} = \sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon  \hfill \cr 
  v = \sqrt {{{\bar \alpha }_t}} \varepsilon  - \sqrt {1 - {{\bar \alpha }_t}} {x_0} \hfill \cr}  \right.
$$
$$
\left\{ \matrix{
  \sqrt {{{\bar \alpha }_t}} {x_t} = {{\bar \alpha }_t}{x_0} + \sqrt {{{\bar \alpha }_t}} \sqrt {1 - {{\bar \alpha }_t}} \varepsilon  \hfill \cr 
  \sqrt {1 - {{\bar \alpha }_t}} v = \sqrt {1 - {{\bar \alpha }_t}} \sqrt {{{\bar \alpha }_t}} \varepsilon  - \left( {1 - {{\bar \alpha }_t}} \right){x_0} \hfill \cr}  \right.
$$
$$
{{\hat x}_0} = \sqrt {{{\bar \alpha }_t}} {x_t} - \sqrt {1 - {{\bar \alpha }_t}} v
$$
>虽然是让模型预测 v，但是在前向过程时还是加入噪声。

>模型在训练时，每一批样本会随机采样一批时间步用于添加噪声，每个样本对应的时间步可能不同


### DDIM


DDPM 在生成数据时需要一步一步采样，生成效率低。DDIM 通过跳步采样，减少了推理时间。由于DDPM遵循马尔可夫性质，导致无法跳步。DDIM假设不具有马尔可夫性质，因此有

$$
q({x_{t - 1}}|{x_t},{x_0}) = q({x_t}|{x_{t - 1}},{x_0}){{q({x_{t - 1}}|{x_0})} \over {q({x_t}|{x_0})}}
$$
这里需要注意 $q({x_t}|{x_{t - 1}},{x_0})$ 不具有马尔可夫性质。这里我们不计算 $q({x_t}|{x_{t - 1}},{x_0})$，而是假设一个分布，但是需要使得该分布满足加噪公式。

论文中给出的 $q({x_{t-1}}|{x_{t}},{x_0})$为
$$
q({x_{t-1}}|{x_{t}},{x_0}) = N\left( {\sqrt {{\alpha _{t - 1}}} {x_0} + \sqrt {1 - {\alpha _{t - 1}} - \sigma _t^2} {{{x_t} - {{\sqrt \alpha  }_t}{x_0}} \over {\sqrt {1 - {\alpha _t}} }},\sigma _t^2I} \right)
$$
上式可以通过待定系数法求解，假设 $q({x_{t-1}}|{x_{t}},{x_0}) \sim N\left( {k{x_0} + m{x_t},{\sigma ^2}I} \right)$，重采样得到

$$
{x_{t - 1}} = k{x_0} + m{x_t} + \sigma \varepsilon 
$$

带入 $x_t$ 的加噪公式

$$
\eqalign{
  & {x_{t - 1}} = k{x_0} + m\left( {\sqrt {{{\bar \alpha }_t}} {x_0} + \sqrt {1 - {{\bar \alpha }_t}} \varepsilon '} \right) + \sigma \varepsilon   \cr 
  &  = \left( {k + m\sqrt {{{\bar \alpha }_t}} } \right){x_0} + \sqrt {{\sigma ^2} + {m^2}\left( {1 - {{\bar \alpha }_t}} \right)} \varepsilon  \cr} 
$$
由于加噪公式中

$$
{x_{t-1}} = \sqrt {{\alpha _{t-1}}} {x_{0}} + \sqrt {1 - {\alpha _{t-1}}} \varepsilon
$$
联立等式算出 k 和 m

$$
k = \sqrt {{{\bar \alpha }_{t - 1}}}  - \sqrt {1 - {{\bar \alpha }_{t - 1}} - {\sigma ^2}} {{\sqrt {{{\bar \alpha }_t}} } \over {\sqrt {1 - {{\bar \alpha }_t}} }}\quad m = {{\sqrt {1 - {{\bar \alpha }_{t - 1}} - {\sigma ^2}} } \over {\sqrt {1 - {{\bar \alpha }_t}} }}
$$

### Imporved DDPM

DDPM 在训练时使用固定的方差，即 $\sigma_t^2=\beta_t$，并且令 $\sigma^2_t=\tilde \beta_t$ 的效果相同。论文中给出的原因是，$\beta_t$ 和 $\tilde\beta_t$ 除了 t=0 附近时，两者十分接近。但是对于对数似然损失来说，只有前面的部分采样步对于损失的影响大，可以通过学习一个方差 ${\Sigma _\theta }({x_t},t)$ 来提升对数似然指标。

直接学习一个方差函数是困难的，论文中考虑使用插值的方法，即

$$
{\Sigma _\theta }({x_t},t) = \exp \left( {v\log {\beta _t} + (1 - v)\log {{\tilde \beta }_t}} \right)
$$
其中 $v$ 是一个向量，不对 $v$ 进行约束，这里的 $v$ 可以是模型输出的一部分。

由于DDPM中的简单损失没有用到 ${\Sigma _\theta }({x_t},t)$，该论文定义了新的混合目标函数

$$
{L_{{\rm{hybrid}}}} = {L_{{\rm{simple}}}} + \lambda {L_{{\rm{vlb}}}}
$$

其中 $\lambda$ 设为0.001。同时计算 $L_{\rm{vlb}}$ 时停止 $\mu_{\theta}(x_t,t)$ 的梯度回传，只让 $L_{\rm{vlb}}$ 优化 ${\Sigma _\theta }({x_t},t)$，由 $L_{\rm{simple}}$ 来优化  $\mu_{\theta}(x_t,t)$。

为了避免前向加噪过程的后续阶段噪声太大使得其对采样质量贡献不大，该论文还将噪声设置为余弦形式
$$
{{\bar \alpha }_t} = {{f(t)} \over {f(0)}}\quad f(t) = \cos {\left( {{{t/T + s} \over {1 + s}}{\pi  \over 2}} \right)^2}
$$
将 $\beta_t$ 限制不大于0.999。为了避免 $\beta_t$ 过小， 给接近0的 $\beta_t$ 添加一个小的offset $s$，$s$ 设置为 0.008。原论文的 $\beta_t$ 的设置方式如下：

```python
def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):  
    betas = []  
    for i in range(num_diffusion_timesteps):  
        t1 = i / num_diffusion_timesteps  
        t2 = (i + 1) / num_diffusion_timesteps  
        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))  
    return np.array(betas)

betas = betas_for_alpha_bar(num_diffusion_timesteps, lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2)
```


### Guided DDPM

在带噪图像上训练一个分类器 $p_{\phi}(y|x_t,t)$，使用梯度 ${\nabla _{{x_t}}}\log {p_\phi }(y|{x_t},t)$ 来引导扩散采样的过程。简记 ${p_\phi }(y|{x_t},t) = {p_\phi }(y|{x_t})$，${\varepsilon _\theta }({x_t},t) = {\varepsilon _\theta }({x_t})$。

对于一个无条件的逆加噪过程 $p_{\theta}(x_t|x_{t+1})$，为了让其以标签 y 作为条件，可以将采样写为

$$
{p_{\theta ,\phi }}\left( {{x_t}|{x_{t + 1}},y} \right) = Z{p_\theta }\left( {{x_t}|{x_{t + 1}}} \right){p_\phi }\left( {y|{x_t}} \right)
$$
Z 为归一化常数（证明详见原论文附录 H）。

根据 DDPM 的证明结果可以得到，$p_{\theta}(x_t|x_{t+1})$ 服从高斯分布

$$
\eqalign{
  & {p_\theta }\left( {{x_t}|{x_{t + 1}}} \right) = N(\mu ,\Sigma )  \cr 
  & \log {p_\theta }\left( {{x_t}|{x_{t + 1}}} \right) =  - {1 \over 2}{\left( {{x_t} - \mu } \right)^T}{\Sigma ^{ - 1}}\left( {{x_t} - \mu } \right) + C \cr} 
$$

使用泰勒展开在 $x_t=\mu$ 点近似 $\log{p_\phi }\left( {y|{x_t}} \right)$
$$
\eqalign{
  & \log {p_\phi }\left( {y|{x_t}} \right) \approx \log {p_\phi }\left( {y|{x_t}} \right){|_{{x_t} = \mu }} + \left( {{x_t} - \mu } \right){\nabla _{{x_t}}}\log {p_\phi }\left( {y|{x_t}} \right){|_{{x_t} = \mu }}  \cr 
  &  = \left( {{x_t} - \mu } \right){\nabla _{{x_t}}}\log {p_\phi }\left( {y|{x_t}} \right){|_{{x_t} = \mu }} + {C_1} \cr}
$$
这里的 $C_1$ 是常数，不用管，此外为了方便推导，令 $g = {\nabla _{{x_t}}}\log {p_\phi }\left( {y|{x_t}} \right){|_{{x_t} = \mu }}$

所以可以得到
$$
\eqalign{
  & \log \left( {{p_\theta }\left( {{x_t}|{x_{t + 1}}} \right){p_\phi }\left( {y|{x_t}} \right)} \right) \approx  - {1 \over 2}{\left( {{x_t} - \mu } \right)^T}{\Sigma ^{ - 1}}\left( {{x_t} - \mu } \right) + \left( {{x_t} - \mu } \right)g + {C_2}  \cr 
  &  =  - {1 \over 2}{\left( {{x_t} - \mu  - \Sigma g} \right)^T}{\Sigma ^{ - 1}}\left( {{x_t} - \mu  - \Sigma g} \right) + {1 \over 2}{g^T}\Sigma g + {C_2}  \cr 
  &  =  - {1 \over 2}{\left( {{x_t} - \mu  - \Sigma g} \right)^T}{\Sigma ^{ - 1}}\left( {{x_t} - \mu  - \Sigma g} \right) + {C_3}  \cr 
  &  = \log p(z) + {C_4}\quad z \sim N\left( {\mu  + \Sigma g,\Sigma } \right) \cr} 
$$

综上可以得到有条件的转换操作可以近似为一个均值偏移 $\Sigma g$ 的无条件转换操作。

算法流程为：

输入类标签 $y$，梯度比例 $s$（$s>1$，可以取10）

从 $N(0,I)$ 中采样 $x_T$ 

从 T 到 1：

$$
\mu ,\Sigma  \leftarrow {\mu _\theta }({x_t}),{\Sigma _\theta }({x_t})
$$
从 $N\left( {\mu  + s\Sigma {\nabla _{{x_t}}}\log {p_\phi }\left( {y|{x_t}} \right),\Sigma } \right)$ 采样 $x_{t-1}$

> 注意训练diffusion时还是正常训练，但是在采样生成图片时，需要加上分类器梯度乘上方差

使用梯度比例，可以放大分类器的效果，产生更加符合类别的样本。

分类器先在 ImageNet 上训练，结构为 Unet 的下采样部分，训练好之后在diffusion过程中使用。

训练分类器需要对图片进行diffusion时根据时间进行加噪，再送入分类器。



### DiT

DiT 将扩散模型中的 U-net 换成了 VIT，DiT 中的条件输入无需分类器，而是用 nn.Embedding 将标签编码为一个向量，然后经过 Adaptive layer norm 层实现。

![](https://picx.zhimg.com/v2-6cdf04ff2ca336b6fa77c9f9e4c5dc5f_1440w.jpg)



## 生成对抗模型

### gan

GAN 包括一个生成器 G 和一个判别器 D，生成网络的目标就是尽量生成真实的图片去欺骗判别网络 D。而网络D的目标就是尽量把网络G生成的图片和真实的图片分辨开来。

GAN 模型的目标函数为




# 数值分析

## 基础知识

### 实数的浮点表示

#### 机器数的格式

浮点数的表示：详见[二进制转换成其它格式数据](计算机科学.md#其它#二进制转换成其它格式数据)，此处的浮点数精度为64位，即指数部分为11位，尾数部分为52位。标准化的IEEE浮点数表示为：
$$
\pm 1.bb \cdots b \times {2^p}
$$
对于64位浮点数，精度为$\varepsilon_{mach}=2^{-52}$

**IEEE舍入最近法则**

> 如果在二进制数右边的第53位是0，则向下舍去（在第52位后面截断）。
>
> 如果第53位是1，则向上进位（在第52位上加1)；但是如果在53位的右边的所有已知位都是0，此时当且仅当第52位是1时才在第52位上加1，否则在52位后面截断。

**定义**：将IEEE双精度浮点数字记做x，利用舍入最近法则记做fl(x)。

**举例**：对于数字9.4，二进制表示为 $\left( {1001.\overline {0110} } \right)_2$，计算舍入误差。

将二进制转成浮点数为：$1.001\overline {0110}  \times {2^3} = 1.0010\overline {1100}  \times {2^3}$，因此第53位是1，又53位的右边也有1，所以需要在52位加1，即加上$1 \times {2^{ - 52}} \times {2^3}$，并且舍弃$0.\overline {1100}  \times {2^{ - 52}} \times {2^3} = 0.\overline {1100}  \times {2^{ - 49}}$，

对于$0.\overline {1100}$，可以这样计算：
$$
\eqalign{
  & x = 0.\overline {1100}   \cr 
  & {2^4}x = 1100.\overline {1100}   \cr 
  & \left( {{2^4} - 1} \right)x = 1100 \Rightarrow x = 0.8 \cr}
$$
所以舍入误差为
$$
fl(x) - x =  - 0.8 \times {2^{ - 49}} + 1 \times {2^{ - 52}} \times {2^3} = 0.2 \times {2^{ - 49}}
$$


**相对舍入误差**

> 在IEEE机器算术模型中，fl(x)的相对舍入误差不会比机器精度的一半大：
> $$
> {{\left| {fl(x) - x} \right|} \over {\left| x \right|}} \le {1 \over 2}{\varepsilon _{mach}}
> $$

#### 机器数的表示

**举例**：`fl(9.4)` 对应的存储在机器中的表达？

对于指数：3 + 1023 = 1026 = ${\left( {10000000010} \right)_2}$，再加上符号位0，则为
$$
{\left( {010000000010} \right)_2} = {\left( {402} \right)_{16}}
$$
对于尾数：直接转成16进制可得${\left( {{{2CCCCCCCCCCCD}}} \right)_{16}}$。

整体16进制表示为4022CCCCCCCCCCCD。



**特殊数的十六进制表示**

| 机器数 | 例子 | 十六进制数       |
| ------ | ---- | ---------------- |
| +Inf   | 1/0  | 7FF0000000000000 |
| -Inf   | -1/0 | FFF0000000000000 |
| NaN    | 0/0  | FFFxxxxxxxxxxxxx |
| +0     |      | 0000000000000000 |
| -0     |      | 8000000000000000 |

其中x表示不全为0的位，+0和-0在计算中相同，只有符号位不同。

当指数全为0时，如0000000000000001表示的数为${2^{ - 52}} \times {2^{ - 1022}} = {2^{ - 1074}}$，这是最小可表达的数字，在此之下的可表达双精度数字完全不能被表示。



#### 浮点数加法

对于1和$2^{-53}$的加法，对于$2^{-53}$，虽然53位是1，但是右边全是0且第52位是0，浮点数表示为0，即
$$
1+2^{-53}=1
$$
**举例**：计算双精度浮点的求和($1+3\times2^{-53}$)-1

![image-20231101144341444](D:\TyporaImages\image-20231101144341444.png)

由于求和后的53位是1，又52位也是1，所以需要在第52位上加上1，即最后结果为：

![image-20231101144503039](D:\TyporaImages\image-20231101144503039.png)

在减去1后，结果为$2^{-51}$，因为舍入误差导致最后的结果不为0。



### 有效数字缺失

**举例**：找到二次等式$x^2+9^{12}x=3$的两个根。

使用求根公式：$x = {{ - b \pm \sqrt {{b^2} - 4ac} } \over {2a}}$，负号的值可以找到，但是对于正号的值，存在舍入误差会为0，这就是**有效数字缺失（两个近似的数字相减）**。可以将正号时的求根公式转成
$$
\eqalign{
  & x_2 = {{ - b + \sqrt {{b^2} - 4ac} } \over {2a}} = {{\left( { - b + \sqrt {{b^2} - 4ac} } \right)\left( { - b - \sqrt {{b^2} - 4ac} } \right)} \over {2a\left( { - b - \sqrt {{b^2} - 4ac} } \right)}}  \cr 
  &  = {{2c} \over { - b - \sqrt {{b^2} - 4ac} }} \cr}
$$
这样便可解得正号的根$x_2=1.062\times10^{-11}$。



## 求解方程

### 二分法

**二分法**

>给定初始区间 `[a,b]` 使得f(a)f(b)<0
>while (b-a)/2 > TOL
>	c = (a+b) / 2
>	if f(c) = 0, stop,end
>	if f(a)f(c) < 0
>		b=c
>	else
>		a=c
>	end
>end
>最终的区间 `[a,b]` 中包含一个根
>近似根为(a+b)/2

**二分法的求解误差**
$$
|{x_c} - r| < {{b - a} \over {{2^{n + 1}}}}
$$
其中$x_c=(a_n+b_n)/2$是解的最优估计值。

**定义**：如果误差小于$0.5\times 10^{-p}$，解精确到小数点后p位。

**举例**：使用二分法求解函数在区间 `[0,1]` 上的解，要求解精确到小数点后6位，则至少要迭代多少次。
$$
{{b - a} \over {{2^{n + 1}}}} = {1 \over {{2^{n + 1}}}} < 0.5 \times {10^{ - 6}} \Rightarrow n \ge 20
$$
至少要20次迭代。



### 不动点迭代

#### 函数的不动点

**定义**：当 g(r)=r，实数 r 是函数 g 的不动点。

**不动点迭代**

> $x_0$=初始估计
>
> $x_{i+1}=g(x_i)$，其中i=0,1,2,...

**举例**：使用不动点迭代求解$x^3+x-1=0$的根。

将方程重写为$x=1-x^3$，可以定义$g(x)=1-x^3$，还可定义$g(x) = \root 3 \of {1 - x}  = x$。

还可以将两边加上$2x^3$，可以得到
$$
\eqalign{
  & 3{x^3} + x = 1 + 2{x^3}  \cr 
  & x\left( {1 + 3{x^2}} \right) = 1 + 2{x^3}  \cr 
  & x = {{1 + 2{x^3}} \over {1 + 3{x^2}}}=g(x) \cr} 
$$
使用$x_0=0.5$作为初始值，但是第一种没有成功收敛（在0和1之间来回跳跃），第二种和第三种成功收敛并且第三种收敛得更快。



#### 不动点迭代几何

使用**cobweb图**描述不动点迭代的几何图示

![image-20231101185232379](D:\TyporaImages\image-20231101185232379.png)

可以看到对于 $g(x)=1-x^3$ 而言，在迭代时会出现发散的情况，因此最后无法收敛，而是否能够收敛与函数在不动点附近的斜率有关。



#### 不动点迭代的线性收敛

**定义**：令$e_i$表示迭代过程中第 i 步时的误差，如果
$$
\mathop {\lim }\limits_{i \to \infty } {{{e_{i + 1}}} \over {{e_i}}} = S < 1
$$
该方法被称为满足线性收敛，收敛速度为 S。

**定理**：假设函数g是连续可微函数，g(r)=r，S=$|g'(r)|$<1，则不动点迭代对于一个足够接近r的初始估计，以速度S线性收敛到不动点r。

证明：令$x_i$表示第i步迭代，根据均值定理，在$x_i$和r之间存在$c_i$，满足
$$
g({x_i}) - g(r) = g'({c_i})({x_i} - r)
$$
代入$x_{i+1}=g(x_i)$以及$r=g(r)$可得
$$
x_{i+1}-r=g'(c_i)(x_i-r)
$$
定义$e_i=|x_i-r|$，则有
$$
e_{i+1}=|g'(c_i)|e_i
$$
如果S=|g'(r)|小于1，则通过替换g'，在r附近有一个小的区间满足|g'(x)|<(S+1)/2，这个值仍然小于1，如果$x_i$恰好出现在该区间，则$c_i$也在该区间，因而
$$
{e_{i + 1}} \le {{S + 1} \over 2}{e_i}
$$
所以，误差以(S+1)/2的速度下降，这意味着$\mathop {\lim }\limits_{i \to \infty } {x_i} = r$。



**定义**：如果迭代方法对于一个足够接近 r 的初值能收敛到 r，该迭代方法被称为**局部收敛**到 r。

**举例**：解释为什么不动点迭代 g(x)=cos(x) 收敛

g'(r)=-sin(r)=-sin(0.74)≈-0.67，绝对值比1小，对应解r≈0.74会把附近的初始猜测值吸引过来。

**举例**：使用不动点迭代找出方程 cos x = sin x 的根。

在方程两边加上x可得
$$
x+\cos x-\sin x=x
$$
定义g(x)=x + cos x - sin x，可用不动点迭代法求得x=0.7853981633957986。



### 精度的极限

#### 前向和后向误差

**举例**：使用二分法计算函数$f(x)=x^3-2x^2+{4\over3}x-{8\over 27}$的根，精确到小数点后6位。

在使用二分法时，在16步计算后便出现错误，f(0.6666641)=0，无法精确到小数点后6位。即使使用MATLAB提供的 fzero.m 函数计算仍然无法找到5位以上的正确数位。


**定义**：假设f是一个函数，r是一个根，意味着满足f(r)=0。假设$x_a$是r的近似值，对于根求解问题，近似$x_a$的**后向误差**是$|f(x_a)|$，**前向误差**是$|r-x_a|$。

```mermaid
graph LR
A(方程) ---> B(方程求解器) ---> C(解) 
```

后向误差在左边或者输入一侧（问题数据），这是我们需要对于问题（函数f)改变的量使得方程平衡并输出近似解$x_a$，这个量是$|f(x_a)|$。前向误差是右边或者输出一侧（问题求解），这是我们对于近似解要做的修正，该误差是$|r-x_a|$。



在上面的例子中，后向误差接近${\varepsilon _{mach}} \approx 2.2 \times {10^{ - 16}}$，前向误差则大约为$10^{-5}$，双精度数不能在机器精度的相对误差以下可靠计算，因为后向误差不能被可靠降低，同时前向误差也不能减小。

方程求解方法的终止条件可以基于前向或者后向误差。还有其他相关的终止条件，诸如计算时间的上限问题的上下文对于我们的选择具有指导作用。



#### 威尔金森多项式

由于函数在多重根位置上 $f'$ 为0，所以在重根附近形状很平。正因为如此，分离重根可能会遇到困难，但是多重根问题仅仅是冰山一角，没有多重根问题时也可能也会出现问题，比如威尔金森多项式。
$$
W(x)=(x-1)(x-2)\cdots(x-20)
$$

#### 根搜索的敏感性

如果在输入中是一个小误差，在这种情况下对问题进行求解，造成输出中的大误差，这种情况被称为敏感性问题。

假设对输入做了一个小变化 εg(x)，其中 ε 很小，令 Δr 对应根中的变化，则
$$
f(r+\Delta r)+\varepsilon g(r+\Delta r)=0
$$
进行泰勒展开得到：
$$
f(r) + (\Delta r)f'(r) + \varepsilon g(r) + \varepsilon (\Delta r)g'(r) + O\left( {{{(\Delta r)}^2}} \right) = 0
$$
忽略高阶项，得到
$$
(\Delta r)\left( {f'(r) + \varepsilon g'(r)} \right) \approx  - f(r) - \varepsilon g(r) = \varepsilon g(r)
$$
或者，由于ε很小，所以εg'(r)很小。
$$
\Delta r \approx {{\varepsilon g(r)} \over {f'(r) + \varepsilon g'(r)}} \approx {{\varepsilon g(r)} \over {f'(r)}}
$$

**根的敏感公式**

> 假设r是函数f(x)的根，并且r+Δr是f(x)+εg(r)的根，则当ε<<f'(r)时，
> $$
> \Delta r \approx {{\varepsilon g(r)} \over {f'(r)}}
> $$

**举例**：估计$P(x) = (x - 1)(x - 2)(x - 3)(x - 4)(x - 5)(x - 6) - {10^{ - 6}}{x^7}$的最大根。

令$f(x) = (x - 1)(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)$，$\varepsilon  =  - {10^{ - 6}}$，$g(x)=x^7$。如果没有εg(x)，则有最大根r=6，但当加上这一项后，根变成了
$$
r + \Delta r = 6 - {{\varepsilon {6^7}} \over {5!}} = 6 - 2332.8\varepsilon  = 6.0023328
$$
误差放大因子 = 相对前向误差 / 相对后向误差



​	条件数也是误差放大度量的一种方式，数值分析是对算法的研究，算法把定义问题的数据作为输入，对应的结果作为输出。**条件数指的是理论问题本身所带来的误差放大部分，和用于求解问题的特定算法无关。**注意到条件数仅仅度量由于问题本身带来的误差放大，这点很重要。和条件一起，还有一个平行概念，即稳定。稳定指的是由于算法小的输入误差造成的放大，而不是问题本身。如果一个算法在小的后向误差存在的时候，总能给出一个近似解，则称该算法是稳定的。如果问题的条件好，算法稳定，我们可以期望同时具有小的后向误差和前向误差。

​	前面的误差放大例子表明根求解过程对于特定的输入变化的敏感性。问题可能或多或少地敏感，依赖于如何设计输入的变化。问题的条件数定义为所有输入变化，或者至少规定类型的变化所造成的最大误差放大。条件数高的问题称为**病态**问题，条件数在1附近的问题称为**良态**问题。



### 牛顿方法

**牛顿方法**

> $x_0$=初始估计
>
> $x_{i+1}=x_i-{f(x_i)\over f'(x_i)},\quad i=0,1,2,...$



**定义**：令$e_i$表示一个迭代方法第i步后得到的误差，如果满足下式，则该迭代为二次迭代
$$
M = \mathop {\lim }\limits_{i \to \infty } {{{e_{i + 1}}} \over {e_i^2}} < \infty
$$
**定理**：令f是二阶连续可微函数，f(r)=0。如果f'(r)≠0，则牛顿方法局部二次收敛到r，第i步的误差$e_i$满足
$$
 \mathop {\lim }\limits_{i \to \infty } {{{e_{i + 1}}} \over {e_i^2}} = M
$$
证明参见原书，注意此处
$$
M = \left| {{{f''(r)} \over {2f'(r)}}} \right|
$$
注意，该定理并不能保证牛顿方法每次都能二次收敛。如求f(x)=$x^2$的实根，此时牛顿迭代公式为
$$
x_{i+1}={x_i\over 2}
$$
误差公式是$e_{i+1}=e_i/2$，是线性收敛。

**定理**：假设在区间 `[a,b]` 上，(m+1)阶连续可微函数f在r点有一个m阶多重根，则牛顿方法局部收敛到r，第i步误差$e_i$满足
$$
\mathop {\lim }\limits_{i \to \infty } {{{e_{i + 1}}} \over {{e_i}}} = S = {{m - 1} \over m}
$$
**定理**：如果在 `[a,b]` 区间上f是(m+1)阶连续函数，包含m>1的多重根，则改进的牛顿方法为：
$$
x_{i+1}=x_i-{mf(x_i)\over f'(x_i)}
$$
收敛到 r，并具有二次收敛速度。

牛顿方法如同FPI（不动点迭代），可能不会收敛到根．下面的例子是一种可能不收敛的情况。

**举例**：对函数f(x)=$4x^4-6x^2-11/4$使用牛顿方法，初始估计$x_0=1/2$



### 不需要导数的根求解

除了重根，牛顿方法比二分法和FPI方法的收敛速度更快。它达到了这种更快的速度是因为使用了更多的信息，尤其是通过函数导数得到的函数切线方向的信息。在某些情况下，可能难以计算导数。在这种情况下，割线方法是牛顿方法的一个非常好的替代。它使用近似值割线替代了切线，并且收敛速度差不多快。

#### 割线方法及其变体

在$x_i$处的导数的近似可写为差商：
$$
{{f({x_i}) - f({x_{i - 1}})} \over {{x_i} - {x_{i - 1}}}}
$$
**割线方法**

> $x_0$，$x_1$=初始估计
>
> ${x_{i + 1}} = {x_i} - {{f({x_i})\left( {{x_i} - {x_{i - 1}}} \right)} \over {f({x_i}) - f({x_{i - 1}})}},\quad i = 1,2,3, \cdots$

割线方法以**超线性**的速度收敛到一个单根，意味着它在线性和二次收敛方法之间。



**试位方法**和二分法相似，将中点从a和b的平均值改为割线方法定义的点，注意f(a)f(b)<0
$$
c = a - {{f(a)\left( {a - b} \right)} \over {f(a) - f(b)}} = {{bf(a) - af(b)} \over {f(a) - f(b)}}
$$
c对应的便是a和b所定义的直线与x轴的交点。

试位方法开始表现得比二分法和割线方法都要好，具有二者最好的性质。但是，二分法在每一步中可以确保消除1/2的不确定性，试位方法却没有能力做出这样的保证，而且在一些例子中可能收敛很慢。


**Muller方法**是割线方法在不同方向的推广。该方法不是计算经过先前两个点的直线和x轴的交点，而是使用三个前面生成的点$x_0$、$x_1$、$x_2$，画出通过它们的抛物线y=p(x)，并计算抛物线和x轴的交点。一般来讲，抛物线会生成0个或者2个交点。如果有两个交点，接近上一步中的$x_2$的点会被选作$x_3$，通过简单的二次公式计算，就可以确定两种可能。如果抛物线和x轴不相交，就会出现复数解，能够处理复数代数的软件就可以计算对应的解。

**逆二次插值(IQI)** 是割线方法到抛物线的一种相近的泛化方法，但是使用形如 x=p(y) 的抛物线，而不是 Muller 方法所使用的 y=p(x)，这个抛物线和 x 轴只有一个交点。经过三点(a,A)，(b,B)，(c,C)的二阶多项式 x=P(y) 为：
$$
P(y) = a{{(y - B)(y - C)} \over {(A - B)(A - C)}} + b{{(y - A)(y - C)} \over {(B - A)(B - C)}} + c{{(y - A)(y - B)} \over {(C - A)(C - B)}}
$$
其中P(A)=a，P(B)=b，P(C)=c，用 y=0 带入可得抛物线和 x 轴的交点，可以得到
$$
P(0) = c - {{r(r - q)(c - b) + (1 - r)s(c - a)} \over {(q - 1)(r - 1)(s - 1)}}
$$
其中 q=f(a)/f(b)，r=f(c)/f(b)，s=f(c)/f(a)。

**逆二次插值**

> $a=x_i$，$b=x_{i+1}$，$c=x_{i+2}$，$A=f(x_i)$，$B=f(x_{i+1})$，$C=f(x_{i+2})$
>
> 下一步的$x_{i+3}=P(0)$为
>
> ${x_{i + 3}} = {x_{i + 2}} - {{r(r - q)({x_{i + 2}} - {x_{i + 1}}) + (1 - r)s({x_{i + 2}} - {x_i})} \over {(q - 1)(r - 1)(s - 1)}}$
>
> 可以用新的$x_{i+3}$替代最旧的估计$x_i$，另一种方式使用最新估计替换最近的三个估计中的后向误差最大的那个。

#### Brent方法

用于连续函数 f，区间的边界是 a 和 b，同时 f(a)f(b)<0。Brent 方法记录当前点$x_i$，该点具有最优的后向误差，同时有包含根的区间$[a_i,b_i]$。简单来讲，尝试使用逆二次方法，并在下述情况下，使用结果来替代$x_i$、$a_i$、$b_i$中的一个：

（1）后向误差得到改进；

（2）包含根的区间至少减小一半； 

否则，尝试使用割线方法以实现相同的目的。如果割线方法也失败了，则使用二分法，保证至少减少一半的不确定性。



## 方程组

高斯消去法不过多介绍

### LU分解

可以将高斯消去法的消去步骤表示为LU分解，其中L是单位矩阵进行高斯消去操作后的下三角矩阵，U是原矩阵经过高斯消去得到的矩阵。

一旦知道L和U，问题Ax=b可以写作LUx=b，定义新的“辅助”向量c=Ux，则回代是个两步的过程

（1）对于方程Lc=b，求解c

（2）对于方程Ux=c，求解x

但并不是所有的矩阵都能进行LU分解，如
$$
\left[ {\matrix{
   0 & 1  \cr 
   1 & 1  \cr 

 } } \right]
$$
无法被LU分解。



### 误差来源

在高斯消去法中有两个潜在的误差来源，第一个是病态问题的概念和解对于输入数据的敏感性相关，第二个是淹没，在大部分问题中可以通过一个简单的修正，称为部分主元，进行避免。

#### 误差放大和条件数

**定义**：向量$x=(x_1,\cdots,x_n)$的无穷范数或者最大范数为${\left\| x \right\|_\infty } = \max \left| {{x_i}} \right|$，i=1,...,n，即x所有元素的最大值。

**定义**：令$x_a$是线性方程组Ax=b的近似解，余项是向量r=b-A$x_a$，**后向误差**是余项的范数${\left\| {b - A{x_a}} \right\|_\infty }$，**前向误差**是${\left\| {x - {x_a}} \right\|_\infty }$。

**举例**：找出近似解 `[-1,3.0001]` 的后向误差和前向误差，方程组如下：
$$
\eqalign{
  & {x_1} + {x_2} = 2  \cr 
  & 1.0001{x_1} + {x_2} = 2.0001 \cr}
$$
精确解为 `[1,1]`，后向误差计算：
$$
b - A{x_a} = \left[ {\matrix{
   2  \cr 
   {2.0001}  \cr 

 } } \right] - \left[ {\matrix{
   1 & 1  \cr 
   {1.0001} & 1  \cr 

 } } \right]\left[ {\matrix{
   { - 1}  \cr 
   {3.0001}  \cr 

 } } \right] = \left[ {\matrix{
   { - 0.0001}  \cr 
   {0.0001}  \cr 

 } } \right]
$$
后向误差为0.0001，前向误差计算：
$$
x - {x_a} = \left[ {\matrix{
   1  \cr 
   1  \cr 

 } } \right] - \left[ {\matrix{
   { - 1}  \cr 
   {3.0001}  \cr 

 } } \right] = \left[ {\matrix{
   2  \cr 
   { - 2.0001}  \cr 

 } } \right]
$$
前向误差为2.0001。可以看到前向误差和后向误差差别很大。下面这张图可以帮助理解这一情况

<img src="D:\TyporaImages\image-20231102104200536.png" alt="image-20231102104200536" style="zoom:67%;" />

可以看到(1,1)是它们的交点，不过(-1,3.0001)也差一点在两条直线上（图中的差异是人为放大了1000倍，实际上离得非常近）。

把余项表示为 $r=b-Ax_a$，系统 Ax=b 的**相对后向误差**定义为
$$
{{{{\left\| r \right\|}_\infty }} \over {{{\left\| b \right\|}_\infty }}}
$$
**相对前向误差**定义为
$$
{{{{\left\| {x - {x_a}} \right\|}_\infty }} \over {{{\left\| x \right\|}_\infty }}}
$$
**误差放大因子**则是二者的比值，即 相对前向误差/相对后向误差。

**定义**：方阵A的条件数cond(A)为求解Ax=b时，对于所有右侧向量b，可能出现的最大误差放大因子。

方阵A的矩阵范数定义为每行元素绝对值求和，其中的最大值作为矩阵A的范数。

**定理**：n×n矩阵A的条件数是：
$$
cond(A) = \left\| A \right\| \cdot \left\| {{A^{ - 1}}} \right\|
$$
**举例**：计算矩阵A的条件数，其中
$$
A=\left[ {\matrix{
   1 & 1  \cr 
   {1.0001} & 1  \cr 

 } } \right]
$$
计算$\left\| A \right\|=2.0001$，A的逆为
$$
{A^{ - 1}} = \left[ {\matrix{
   { - 10000} & {10000}  \cr 
   {10001} & { - 10000}  \cr 

 } } \right]
$$
$\left\| A^{-1} \right\|=20001$，A的条件数是cond(A)=40004.0001，在这个系统中，对于任何其他的b，误差放大因子将小于或者等于40004.0001。

在浮点算术中，相对后向误差不可能小于${\varepsilon _{mach}}$，这是由于 b 的元素的存储已经引入了和${\varepsilon _{mach}}$差不多大的误差，在求解 Ax=b 可能出现的相关前向误差是${\varepsilon _{mach}}$·cond(A)。换句话讲，如果$cond(A) \approx {10^k}$，我们在计算 x 时，将丢掉 k 位数字精度。

以上例为例，$cond(A) \approx 4 \times {10^4}$，对于双精度16位精确度而言，只能求解到大约12位的精确数字（实际上会更低，matlab只能求解到11位精确值）。

对于希尔伯特矩阵H（${H_{ij}} = 1/(i + j - 1)$），其对应的条件数非常大。

对于向量和矩阵范数，有$\left\| {A + B} \right\| \le \left\| A \right\| + \left\| B \right\|$。

对于向量x的1-范数为${\left\| x \right\|_1} = \left| {{x_1}} \right| +  \cdots  + \left| {{x_n}} \right|$，n×n矩阵A的矩阵1-范数是${\left\| A \right\|_1}$的最大绝对列和，即列向量的1－范数的最大值。



#### 淹没

**举例**：考虑方程组
$$
\eqalign{
  & {10^{ - 20}}{x_1} + {x_2} = 1  \cr 
  & {x_1} + 2{x_2} = 4 \cr} 
$$
（1）精确解约为`[2,1]`

（2）IEEE双精度，
$$
\left[ {\matrix{
   {{{10}^{ - 20}}} & 1 & | & 1  \cr 
   1 & 2 & | & 4  \cr 

 } } \right]\buildrel {{r_2} - {{10}^{20}}{r_1}} \over
 \longrightarrow \left[ {\matrix{
   {{{10}^{ - 20}}} & 1 & | & 1  \cr 
   0 & {2 - {{10}^{20}}} & | & {4 - {{10}^{20}}}  \cr 

 } } \right]
$$
由于存在舍入，${2 - {{10}^{20}}}$和${4 - {{10}^{20}}}$存为${ - {{10}^{20}}}$，因此解为 `[0,1]`

（3）IEEE双精度，使用行交换
$$
\left[ {\matrix{
   1 & 2 & | & 4  \cr 
   {{{10}^{ - 20}}} & 1 & | & 1  \cr 

 } } \right]\buildrel {{r_2} - {{10}^{ - 20}}{r_1}} \over
 \longrightarrow \left[ {\matrix{
   1 & 2 & | & 4  \cr 
   0 & {1 - 2 \times {{10}^{ - 20}}} & | & {1 - 4 \times {{10}^{ - 20}}}  \cr 

 } } \right]
$$
同样存在舍入导致${1 - 2\times{{10}^{-20}}}$和${1 - 4\times{{10}^{-20}}}$存为1，解得 `[2,1]`，注意这里的结果是确切的结果，不像精确解存在微小的差距。

第2种方法在消去过程中的乘子$10^{20}$，从底部的方程减去顶部方程的$10^{20}$倍，底部方程会被抑制，或者称为"swamp,"，尽管初始的时候有两个独立的方程或者源信息，但是经过第2种计算的消去，这里仅仅留下了顶部方程的两个副本。由于底部方程消失，出于所有实际的目的，我们不能指望计算结果可以满足底部的方程，实际上得到的解也不能满足对应的方程。而另一方面，第3种方法在消去过程中没有产生覆盖，因为乘子是$10^{-20}$， 在消去之后，原始的两个方程仍然存在，并且变为三角形式，结果是更加精确的近似解。高斯消去的过程中保证乘子尽可能小，同时避免淹没。幸运的是，通过对朴素的高斯消去法的简单修正，可以使得高斯消去法中的乘子的绝对值不大于1。这种新的原则，包括在表格中明智的行交换，称为部分主元方法，在下一节中将进行详细的描述。



### PA=LU分解

#### 部分主元

部分主元包含在每一步消去步骤之前的比较，找到第一列中最大的一个元素，其对应行和主元行进行交换，在当前情况下，主元行是第一行。在消去过程中，对每一列都实施相同的策略。在消去第k列时，找到第 p 行，k≤p≤n，定位最大的$|a_{pk}|$，必要时交换第p行和第k行，然后继续进行消去。注意到使用部分主元方法保证所有乘子，或者 L 的元素的绝对值不大于1。通过这种对于高斯消去方法所进行的小的改变，淹没问题可以完全避免。

**举例**：使用高斯消去的部分主元方法求解方程组
$$
\eqalign{
  & {x_1} - {x_2} + 3{x_3} =  - 3  \cr 
  &  - {x_1} - 2{x_3} = 1  \cr 
  & 2{x_1} + 2{x_2} + 4{x_3} = 0 \cr}
$$
由于第一列中$a_{31}$最大，所以交换第一行和第三行
$$
\eqalign{
  & \left[ {\matrix{
   1 & { - 1} & 3 & | & { - 3}  \cr 
   { - 1} & 0 & { - 2} & | & 1  \cr 
   2 & 2 & 4 & | & 0  \cr 

 } } \right]\buildrel {{r_1} \leftrightarrow {r_3}} \over
 \longrightarrow \left[ {\matrix{
   2 & 2 & 4 & | & 0  \cr 
   { - 1} & 0 & { - 2} & | & 1  \cr 
   1 & { - 1} & 3 & | & { - 3}  \cr 

 } } \right]  \cr 
  & \buildrel {{r_2} + {1 \over 2}{r_1}} \over
 \longrightarrow \left[ {\matrix{
   2 & 2 & 4 & | & 0  \cr 
   0 & 1 & 0 & | & 1  \cr 
   1 & { - 1} & 3 & | & { - 3}  \cr 

 } } \right]\buildrel {{r_3} - {1 \over 2}{r_1}} \over
 \longrightarrow \left[ {\matrix{
   2 & 2 & 4 & | & 0  \cr 
   0 & 1 & 0 & | & 1  \cr 
   0 & { - 2} & 1 & | & { - 3}  \cr 

 } } \right] \cr} 
$$
在消去第二列时，由于$a_{32}$比$a_{22}$大，所以交换第二行和第三行
$$
\eqalign{
  & \left[ {\matrix{
   2 & 2 & 4 & | & 0  \cr 
   0 & 1 & 0 & | & 1  \cr 
   0 & { - 2} & 1 & | & { - 3}  \cr 

 } } \right]\buildrel {{r_2} \leftrightarrow {r_3}} \over
 \longrightarrow \left[ {\matrix{
   2 & 2 & 4 & | & 0  \cr 
   0 & { - 2} & 1 & | & { - 3}  \cr 
   0 & 1 & 0 & | & 1  \cr 

 } } \right]  \cr 
  & \buildrel {{r_3} + {1 \over 2}{r_2}} \over
 \longrightarrow \left[ {\matrix{
   2 & 2 & 4 & | & 0  \cr 
   0 & { - 2} & 1 & | & { - 3}  \cr 
   0 & 0 & {0.5} & | & { - 0.5}  \cr 

 } } \right] \cr} 
$$
解得 `x=[1,1,-1]`



#### 置换矩阵

**定义**：置换矩阵是一个n×n的矩阵，其在每一行、每一列仅有一个1，其他全部为0。

**定理**：令P是通过对单位矩阵实施一组特定的行交换后得到的一个 n×n 的置换矩阵。则对于任意的n×n 矩阵 A，PA对应于对矩阵 A 实施同样的行交换得到的结果。


#### PA=LU分解

我们把关于高斯消去所知道的一切组合起来，进行PA=LU分解。这是部分主元的高斯消去的矩阵形式，PA=LU分解是求解线性方程组的主要方法。正如名字中所暗示的，PA=LU是对于矩阵 A 包含行交换的 LU 分解。在部分主元中，开始的时候我们并不知道需要进行置换的列，所以我们必须谨慎地把行置换的信息加入分解中，我们需要记录高斯消去法中所有的主元。

使用PA=LU求解方程组 Ax=b时
$$
\eqalign{
  & PAx = Pb  \cr 
  & LUx = Pb  \cr 
  & Lc = Pb \Rightarrow c  \cr 
  & Ux = c \Rightarrow x \cr} 
$$
其中P即为单位阵进行部分主元法交换行后产生的置换矩阵。



### 迭代方法

#### 雅可比方法

雅可比(Jacobi)方法是方程组系统中的一种形式的不动点迭代。在FPI中，第一步是重写方程，进而求解未知量。雅可比方法以如下标准方式进行该重写步骤：求解第i个方程得到第i个未知变量，然后使用不动点迭代，从初始估计开始，进行迭代。

**举例**：使用雅可比方法求解下面的线性方程组
$$
\eqalign{  & 3{x_1} -{x_2} + 2{x_3} =  - 5  \cr   &  - {x_1} +4x_2- 2{x_3} = 4  \cr   & 2{x_1} + {x_2} +4 {x_3} = 0 \cr}
$$
求解未知变量得到
$$
\eqalign{
  & {x_1} = {{{x_2} - 2{x_3} - 5} \over 3}  \cr 
  & {x_2} = {{{x_1} + 2{x_3} + 4} \over 4}  \cr 
  & {x_3} = {{ - 2{x_1} - {x_2}} \over 4} \cr} 
$$
迭代过程为：
$$
\eqalign{
  & \left[ {\matrix{
   {{x_1}}  \cr 
   {{x_2}}  \cr 
   {{x_3}}  \cr 

 } } \right] = \left[ {\matrix{
   0  \cr 
   0  \cr 
   0  \cr 

 } } \right]  \cr 
  & \left[ {\matrix{
   {{x_1}}  \cr 
   {{x_2}}  \cr 
   {{x_3}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{{x_2} - 2{x_3} - 5} \over 3}}  \cr 
   {{{{x_1} + 2{x_3} + 4} \over 4}}  \cr 
   {{{ - 2{x_1} - {x_2}} \over 4}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{ - 5} \over 3}}  \cr 
   {{4 \over 4}}  \cr 
   {{0 \over 4}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{ - 5} \over 3}}  \cr 
   1  \cr 
   0  \cr 

 } } \right]  \cr 
  & \left[ {\matrix{
   {{x_1}}  \cr 
   {{x_2}}  \cr 
   {{x_3}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{{x_2} - 2{x_3} - 5} \over 3}}  \cr 
   {{{{x_1} + 2{x_3} + 4} \over 4}}  \cr 
   {{{ - 2{x_1} - {x_2}} \over 4}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{1 - 2 \times 0 - 5} \over 3}}  \cr 
   {{{{{ - 5} \over 3} + 2 \times 0 + 4} \over 4}}  \cr 
   {{{ - 2 \times {{ - 5} \over 3} - 1} \over 4}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{ - 4} \over 3}}  \cr 
   {{7 \over {12}}}  \cr 
   {{7 \over {12}}}  \cr 

 } } \right]  \cr 
  & ...  \cr 
  & \left[ {\matrix{
   {{x_1}}  \cr 
   {{x_2}}  \cr 
   {{x_3}}  \cr 

 } } \right] = \left[ {\matrix{
   { - 1.833}  \cr 
   {0.889}  \cr 
   {0.694}  \cr 

 } } \right] \cr} 
$$
**定义**：当对角线上的元素的绝对值比该行上其它元素的绝对值之和还要大的时候，认为n×n的矩阵$A=(a_{ij})$是**严格对角占优**矩阵。

**定理**：如果n×n矩阵A是严格对角占优矩阵，则（1） A是非奇异矩阵，（2）对于所有向量b和初始估计，对Ax=b应用雅可比方法都会收敛到（唯一）解。

但是这个只是充分条件，不代表不满足严格对角占优矩阵，就不会收敛到唯一解，上例中就不是严格对角占优矩阵，可以看到第一行的对角线元素为3，只是等于第一行其它元素的绝对值之和。

雅可比方法是不动点迭代的一种形式，令D表示A的主对角线矩阵，L表示矩阵A的下三角矩阵（主对角线以下的元素），U表示上三角矩阵（主对角线以上的元素），则A=L+D+U，求解的方程变为Lx+Dx+Ux=b。 注意，这里对于L和U的使用和LU分解中不同，当前L和U的主对角线元素都是零。

Ax=b

(D+L+U)x=b

Dx=b-(L+U)x

x=$D^{-1}$(b-(L+U)x)

令g(x)=$D^{-1}$(b-(L+U)x)

**雅可比方法**

> $x_0$=初始向量
>
> $x_{k+1}$=$D^{-1}$(b-(L+U)$x_k$), k = 0,1,2,...

#### 高斯-塞德尔方法和SOR

高斯－塞德尔方法和雅可比方法的唯一差异是在前者中，最近更新的未知变量的值在每一步中都使用，即使更新发生在当前步骤。在使用相同步骤的情况下，该近似更加精确。如果收敛，高斯-塞德尔方法常常比雅可比方法收敛更快。

**高斯-塞德尔方法**

> $x_0$=初始向量
>
> $x_{k+1}$=$D^{-1}$(b-U$x_k$-L$x_{k+1}$) k = 0,1,2,...

**举例**：对如下系统应用高斯-塞德尔方法
$$
\eqalign{  & 3u -v + 2w =  - 5  \cr   &  - u +4v- 2w = 4  \cr   & 2u + v +4w = 0 \cr}
$$
迭代公式为
$$
\left[ {\matrix{
   {{u_{k + 1}}}  \cr 
   {{v_{k + 1}}}  \cr 
   {{w_{k + 1}}}  \cr 

 } } \right] = \left[ {\matrix{
   {{{{v_k} - 2{w_k} - 5} \over 3}}  \cr 
   {{{{u_{k + 1}} + 2{w_k} + 4} \over 4}}  \cr 
   {{{ - 2{u_{k + 1}} - {v_{k + 1}}} \over 4}}  \cr 

 } } \right]
$$
**连续过松弛**(SOR)方法使用高斯－塞德尔的求解方向，并使用过松弛以加快收敛速度。令w是一个实数，并将新的估计中的每个元素$x_{k+1}$定义为w乘上高斯－塞德尔公式和1-w乘上当前估计$x_k$的平均。数字w被称为松弛参数，而当w>1时被称为过松弛。

使用SOR后的迭代公式
$$
\left[ {\matrix{
   {{u_{k + 1}}}  \cr 
   {{v_{k + 1}}}  \cr 
   {{w_{k + 1}}}  \cr 

 } } \right] = \left[ {\matrix{
   {\left( {1 - w} \right){u_k} + w{{{v_k} - 2{w_k} - 5} \over 3}}  \cr 
   {\left( {1 - w} \right){v_k} + w{{{u_{k + 1}} + 2{w_k} + 4} \over 4}}  \cr 
   {\left( {1 - w} \right){w_k} + w{{ - 2{u_{k + 1}} - {v_{k + 1}}} \over 4}}  \cr 

 } } \right]
$$
可以将系统看成不动点迭代，如下：
$$
\eqalign{
  & Ax = b  \cr 
  & \left( {wL + wD + wU} \right)x = wb  \cr 
  & \left( {wL + D} \right)x = wb - wUx + \left( {1 - w} \right)Dx  \cr 
  & x = {\left( {wL + D} \right)^{ - 1}}\left[ {\left( {1 - w} \right)Dx - wUx} \right] + w{\left( {wL + D} \right)^{ - 1}}b \cr} 
$$
**连续过松弛（SOR）**

> $x_0$=初始向量
>
> ${x_{k + 1}} = {\left( {wL + D} \right)^{ - 1}}\left[ {\left( {1 - w} \right)D{x_k} - wU{x_k}} \right] + w{\left( {wL + D} \right)^{ - 1}}b$



**收敛性的证明略**



#### 稀疏矩阵计算

基于高斯消去的直接方法，通过有限步数的计算就可以得到解。为什么我们要研究并使用迭代方法，特别是迭代方法只能得到近似解，并需要多步计算以得到收敛。有两个主要的原因让我们使用诸如高斯－塞德尔这样的迭代方法。两个原因都是基于如下的事实：迭代方法中的一步计算，仅仅需要LU分解的浮点计算所需要时间的一小部分。对于n×n的矩阵做一次高斯消去需要$n^3$次的操作，雅可比方法的一步，仅需要大约$n^2$次的乘法（对于每个矩阵元素做一次乘法操作）以及大约相同数量的加法。

如果已知解的较好的近似，在这种特定的情况下支持使用迭代技术。例如，假设知道Ax＝b的解，随后A与b同时或者单个仅仅发生小的变化。我们可以想象一个动态系统，当A和b改变时，对A和b进行待续的度量，因而需要待续更新精确的解向量x。如果把前面问题的解作为新的相似问题的初始估计，使用雅可 比或者高斯－塞德尔可以得到更快的收敛。这种技术通常称为修饰，这是由于该问题从一个近似解开始，该近似解可能对应前面一个相关问题的解，然后仅仅修饰近似解使其更加精确。

第二个使用迭代方法的主要原因是用于求解稀疏的方程组。当矩阵中的很多元素都是0系数矩阵被称为稀 疏矩阵。通常，对于稀疏矩阵中的$n^2$个矩阵元素，只有O(n)个非零元素。一个完全的矩阵恰恰相反，其中没有几个元素可以假设为0。对稀疏矩阵使用高斯消去经常会导致填充，这使得其中系数矩阵由于必要的行交换从稀疏变为不稀疏。由于这个原因，高斯消去以及PA=LU实现的效率对于稀疏矩阵变得可疑，在这种情况下迭代方法是一种合理的选择。 

```matlab
% 程序 2.1 稀疏矩阵生成
% 输入：n = 系统大小
% 输出：稀疏矩阵 a, r.h.s. b
function [a,b] = sparsesetup (n)
e = ones(n,l); n2=n/2;
a= spdiags( [-e 3*e -e], -1 : 1,n,n);   % a的元素
c= spdiags( e/2, 0,n,n) ; c= fliplr(c); a= a+c;
a(n2+1,n2) = -1; a (n2,n2+1) = -1; 		% 设置2个元素
b=zeros(n,1); 					        % r.h.s. b的元素
b(1)=2.5; b(n)=2.5; b(2:n-1)=1.5; b(n2:n2+1)=1;

% 程序 2.2 雅可比方法
% 输入：完全矩阵或者稀疏矩阵 a, r.h.s. b,
% 雅可比迭代次数k
% 输出：解x
function x = jacobi(a, b, k)
n=length(b); 		    % 确定n
d=diag(a); 		        % 提取a的对角线元素
r=a-diag(d ); 		    % r为余项
x=zeros(n,1); 		    % 初始化向量x
for j =1:k 			    % 雅可比迭代的循环
    x = (b-r*x) ./ d;
end
```



### 用来对称正定矩阵的方法

#### 对称正定矩阵

**定义**：如果$A^T=A$，则n×n矩阵A是**对称矩阵**，如果对于所有向量x≠0，$x^TAx>0$，则矩阵A是**正定矩阵**。

**性质**：如果n×n矩阵A是对称矩阵，则A是正定矩阵，当且仅当所有特征值是正数。

**性质**：如果A是n×n对称正定矩阵，X是一个满秩n×m矩阵，n≥m，则$X^TAX$是m×m对称正定矩阵。

**定义**：方阵A的主子矩阵是一个方的子矩阵，其对角线元素是矩阵A的对角线元素。

**性质**：任何对称正定矩阵的主子矩阵也是对称正定矩阵。



#### Cholesky分解

考虑一个对称正定矩阵
$$
\left[ {\matrix{
   a & b  \cr 
   b & c  \cr 

 } } \right]
$$
将A写成$R^TR$的形式
$$
\left[ {\matrix{
   a & b  \cr 
   b & c  \cr 

 } } \right] = \left[ {\matrix{
   {\sqrt a } & 0  \cr 
   u & v  \cr 

 } } \right]\left[ {\matrix{
   {\sqrt a } & u  \cr 
   0 & v  \cr 

 } } \right] = \left[ {\matrix{
   a & {u\sqrt a }  \cr 
   {u\sqrt a } & {{u^2} + {v^2}}  \cr 

 } } \right]
$$
与上式作比较，$u = b/\sqrt a$以及$v^2=c-u^2$，Cholesky分解如下：
$$
A = \left[ {\matrix{
   a & b  \cr 
   b & c  \cr 

 } } \right] = \left[ {\matrix{
   {\sqrt a } & 0  \cr 
   {{b \over {\sqrt a }}} & {\sqrt {c - {b^2}/a} }  \cr 

 } } \right]\left[ {\matrix{
   {\sqrt a } & {{b \over {\sqrt a }}}  \cr 
   0 & {\sqrt {c - {b^2}/a} }  \cr 

 } } \right] = {R^T}R
$$
**定理**：如果A是n×n对称正定矩阵，则存在上三角n×n矩阵R满足$A=R^TR$。

**Cholesky分解**

> for k = 1,2,...,n 
>
> ​	if $A_{kk}$<0，stop，end
>
> ​	$R_{kk}=\sqrt{A_{kk}}$
>
> ​	$u^T={1\over R_{kk}}A_{k,k+1:n}$
>
> ​	$R_{k,k+1:n}$ = $u^T$
>
> ​	$A_{k+1:n,k+1:n}=A_{k+1:n,k+1:n}-uu^T$
>
> end

**举例**：计算如下矩阵的Cholesky分解
$$
\left[ {\matrix{
   4 & { - 2} & 2  \cr 
   { - 2} & 2 & { - 4}  \cr 
   2 & { - 4} & {11}  \cr 

 } } \right]
$$
k=1：$R_{11}=\sqrt{A_{11}}=2$，$u^T=A_{1,2:3}/R_{11}=[-2,2]/2=[-1,1]$

$R_{1,2:3}=[-1,1]$，对$A_{2:3,2:3}$进行运算：
$$
\left[ {\matrix{
   2 & { - 4}  \cr 
   { - 4} & {11}  \cr 

 } } \right] - \left[ {\matrix{
   1 & { - 1}  \cr 
   { - 1} & 1  \cr 

 } } \right] = \left[ {\matrix{
   1 & { - 3}  \cr 
   { - 3} & {10}  \cr 

 } } \right]
$$
k=2：现在在这个2×2的子矩阵中重复这个过程，找到$R_{22}=\sqrt{A_{22}}=1$以及$u^T=A_{23}=-3$，$R_{23}=u^T=-3$，下面的1×1主子矩阵是10-(-3)(-3)=1，因而$R_{33}=\sqrt{1}$，所以R等于
$$
R = \left[ {\matrix{
   2 & { - 1} & 1  \cr 
   0 & 1 & { - 3}  \cr 
   0 & 0 & 1  \cr 

 } } \right]
$$

#### 共轭梯度方法

共扼梯度的思路依赖于内积思想的推广，因为(v,w)=(w,v)，以及对于标量α和β，有(αv+βw,u)=α(v,u)+β(w,u)，所以欧几里得内积$(v,w)=v^Tw$对称并对于输入v和w线性，欧几里得内积也是正定的，当v≠0时，(v,v)>0。

**定义**：令A是对称正定的n×n矩阵，对于两个n维向量v和w，定义**A内积**
$$
(v,w)_{A}=v^TAw
$$
当$(v,w)_A=0$时，向量v和w为**A共轭**。

**共轭梯度方法**

> $x_0$=初始估计
>
> $d_0=r_0=b-Ax_0$
>
> for k = 0,1,2,...,n-1
>
> ​	if $r_k$=0，stop，end
>
> ​	${\alpha _k} = {{{r_k}^T{r_k}} \over {d_k^TA{d_k}}}$
>
> ​	$x_{k+1}=x_k+\alpha_kd_k$
>
> ​	$r_{k+1}=r_k-\alpha_kAd_k$
>
> ​	$\beta_k={r_{k+1}^Tr_{k+1}\over r_k^Tr_k}$
>
> ​	$d_{k+1}=r_{k+1}+\beta_kd_k$
>
> end

$x_k$表示当前解，$r_k$表示余项，$d_k$表示更新$x_k$得到改进的$x_{k+1}$时所使用的新的搜索方向。该方法能够成功的原因在于所有的余项都和前面的余项正交。如果能做到所有的余项正交，该方法搜索所有的正交方向，在经过至多n步就可以得到余项为零的正确解。实现所有余项的正交的关键在于选择搜索方向$d_k$，并使之两两共扼。共轭的概念推广了正交的概念，并据此在算法的名字中也包含“共轭”。

对于$\alpha_k$和$\beta_k$的选择，是为了保证下一余项向量和前面所有的余项向量都正交。选择$\alpha_k$使得新的余项$r_{k+1}$和方向$d_k$正交：
$$
\eqalign{
  & d_k^T{r_{k + 1}} = d_k^T{r_k} - {\alpha _k}d_k^TA{d_k} = 0  \cr 
  & {\alpha _k} = {{{r_k}^T{r_k}} \over {d_k^TA{d_k}}} \cr} 
$$
对于$\beta_k$使$d_{k-1}$和$r_k$正交，有
$$
\eqalign{
  & {d_k} - {r_k} = {\beta _{k - 1}}{d_{k - 1}}  \cr 
  & r_k^T{d_k} - r_k^T{r_k} = 0 \cr}
$$
有$r_k^T{d_k} = r_k^T{r_k}$，为了保证$d_k$两两A共轭，
$$
\eqalign{
  & {d_{k + 1}} = {r_{k + 1}} + {\beta _k}{d_k}  \cr 
  & d_k^TA{d_{k + 1}} = d_k^TA{r_{k + 1}} + {\beta _k}d_k^TA{d_k}  \cr 
  & 0 = d_k^TA{r_{k + 1}} + {\beta _k}d_k^TA{d_k}  \cr 
  & {\beta _k} =  - {{d_k^TA{r_{k + 1}}} \over {d_k^TA{d_k}}} \cr} 
$$
$\beta_k$还可以继续化简成伪代码中的形式，具体的证明参考原书。



#### 预条件

通过使用预条件技术，可以使得诸如共扼梯度方法的迭代方法收敛速度加快。迭代方法的收敛率通常直接或者间接依赖于系数矩阵A的条件数。预条件方法的思想是降低问题中的条件数。

n×n的线性方程组Ax=b的预条件形式是
$$
M^{-1}Ax=M^{-1}b
$$
其中M是可逆的n×n矩阵，称为预条件子。我们所要做的就是在方程两侧左乘上该矩阵，预条件矩阵试图对矩阵A逆转，从而可以有效地降低问题的条件数。概念上来讲，它试图同时做两件事：矩阵M应该（1）和矩阵A足够接近，（2）容易求逆。这两个目的常常彼此对立。

一种特别简单的方式是雅可比预条件子M=D，其中D是A的对角线矩阵。D的逆矩阵是D的元素的倒数．例如在一个严格对角占优矩阵中，雅可比预条件子和A相似，同时非常容易求逆。注意到，对称正定矩阵的每个对角线元素都严格为正，因而计算倒数不是问题。

**预条件共轭梯度法**

> $x_0$=初始估计
>
> $r_0=b-A_0$
>
> $d_0=z_0=M^{-1}r_0$
>
> for k = 0,1,2,...,n-1
>
> ​	if $r_k$=0，stop，end
>
> ​	$a_k=r_k^Tz_k/d^T_kAd_k$
>
> ​	$x_{k+1}=x_k+\alpha_kd_k$
>
> ​	$r_{k+1}=r_k-\alpha_kAd_k$
>
> ​	$z_{k+1}=M^{-1}r_{k+1}$
>
> ​	$\beta_k=r_{k+1}^Tz_{k+1}/r_k^Tz_k$
>
> ​	$d_{k+1}=z_{k+1}+\beta_kd_k$
>
> end

在对称连续过松弛(SSOR)中，预条件子定义如下：
$$
M=(D+wL)D^{-1}(D+wU)
$$
其中A=L+D+U被分为下三角部分、对角线以及上三角部分。w是一个在0和2之间的常数。在特例中w=1，这被称为**高斯-塞德尔预条件子**。

注意到SSOR预条件子定义为下三角矩阵和上三角矩阵的乘积$M=(I+wLD^{-1})(D+wU)$，因而方程$z=M^{-1}v$可以通过两次回代求解。
$$
(I+wLD^{-1})c=v\\
(D+wU)z=c
$$

### 非线性方程组

#### 多元牛顿方法

多变量情况下函数导数f'对应的是雅可比矩阵，定义为
$$
DF(x) = \left[ {\matrix{
   {{{\partial {f_1}} \over {\partial u}}} & {{{\partial {f_1}} \over {\partial v}}} & {{{\partial {f_1}} \over {\partial w}}}  \cr 
   {{{\partial {f_2}} \over {\partial u}}} & {{{\partial {f_2}} \over {\partial v}}} & {{{\partial {f_2}} \over {\partial w}}}  \cr 
   {{{\partial {f_3}} \over {\partial u}}} & {{{\partial {f_3}} \over {\partial v}}} & {{{\partial {f_3}} \over {\partial w}}}  \cr 

 } } \right]
$$
**多变量牛顿方法**

> $x_0$=初始向量
>
> $x_{k+1}=x_k-(DF(x_k))^{-1}F(x_k)$，k=0,1,2,...

由于出现计算矩阵的逆，所以令$x_{k+1}=x_k-s$，其中s是$DF(x_k)s=F(x_k)$的解，所以迭代步骤为
$$
\left\{ \matrix{
  DF({x_k})s =  - F({x_k}) \hfill \cr 
  {x_{k + 1}} = {x_k} + s \hfill \cr}  \right.
$$

#### Broyden方法

如果可以计算雅可比矩阵，那么牛顿方法是一个好选择，那么最好的替代方法就是Broyden方法。

**Broyden方法**

> $x_0$=初始向量
>
> $A_0$=初始矩阵
>
> for i = 0,1,2,...
>
> ​	$x_{i+1}=x_i-A_i^-F(x_i)$
>
> ​	${A_{i + 1}} = {A_i} + {{\left( {{\Delta _{i + 1}} - {A_i}{\delta _{i + 1}}} \right){\delta _{i + 1}}^T} \over {{\delta _{i + 1}}^T{\delta _{i + 1}}}}$
>
> end

其中${\delta _{i + 1}} = {x_{i + 1}} - {x_i},\;{\Delta _{i + 1}} = F({x_{i + 1}}) - F({x_i})$

**Broyden方法Ⅱ**

> $x_0$=初始向量
>
> $A_0$=初始矩阵
>
> for i = 0,1,2,...
>
> ​	$x_{i+1}=x_i-B_iF(x_i)$
>
> ​	${B_{i + 1}} = {B_i} + {{\left( {{\delta _{i + 1}} - {B_i}{\Delta _{i + 1}}} \right){\delta _{i + 1}}^T{B_i}} \over {{\delta _{i + 1}}^T{B_i}{\delta _{i + 1}}}}$
>
> end

其中${\delta _{i + 1}} = {x_{i + 1}} - {x_i},\;{\Delta _{i + 1}} = F({x_{i + 1}}) - F({x_i})$

首先，需要初始向量$x_0$和初始估计$B_0$，如果难以计算导数，可以使用$B_0=I$。Broyden方法 Ⅱ的一个可以察觉的缺点是在一些应用中需要估计雅可比矩阵，但是这个矩阵并不容易得到。矩阵$B_i$是对雅可比矩阵逆的估计。而Broyden方法I正相反，一直记录了用来$A_i$估计雅可比。两个版本的Broyden方法都超线性收敛到单根，比牛顿方法的二次收敛要慢一些。



## 插值

### 数据和插值函数

**定义**：如果对于每个1≤i≤n，$P(x_i)=y_i$，则称函数y=P(x)插值数据点$(x_1,y_1)$，...，$(x_n,y_n)$。

#### 拉格朗日插值

假设给出三点($x_1$,$y_1$)，($x_2$,$y_2$)，($x_3$,$y_3$)，则多项式
$$
{P_2}(x) = {y_1}{{(x - {x_2})(x - {x_3})} \over {({x_1} - {x_2})({x_1} - {x_3})}} + {y_2}{{(x - {x_1})(x - {x_3})} \over {({x_2} - {x_1})({x_2} - {x_3})}} + {y_3}{{(x - {x_1})(x - {x_2})} \over {({x_3} - {x_1})({x_3} - {x_2})}}
$$
一般来说，如果有n个点$(x_1,y_1)$，...，$(x_n,y_n)$，对于1~n之间的每个k，定义n-1次多项式
$$
{L_k}(x) = {{(x - {x_1}) \cdots (x - {x_{k - 1}})(x - {x_{k + 1}}) \cdots (x - {x_n})} \over {({x_k} - {x_1}) \cdots ({x_k} - {x_{k - 1}})({x_k} - {x_{k + 1}}) \cdots ({x_k} - {x_n})}}
$$
${L_k}(x)$具有一个有趣的性质，即${L_k}(x_k)=1$，而${L_k}(x_j)=0\;j\neq k$。定义了n-1次多项式
$$
{P_{n - 1}}(x) = {y_1}{L_1}(x) +  \cdots {y_n}{L_n}(x)
$$
**定理（多项式插值的主定理）**：令$(x_1,y_1)$，...，$(x_n,y_n)$是平面中的n个点，具有不同的$x_i$坐标，则存在一个并且仅有一个n-1次或者更低次的多项式P满足$P(x_i)=y_i$，i=1,...,n。



#### 牛顿差商

**定义**：用 f[$x_1$...$x_n$] 表示（唯一）多项式的$x^{n-1}$项的系数，该多项式插值($x_1$,f($x_1$))，...，($x_n$,f($x_n$))。

**牛顿差商公式**：<span id="NewtonInterp"></span>
$$
\eqalign{
  & P(x) = f[{x_1}] + f[{x_1}\;{x_2}](x - {x_1}) + f[{x_1}\;{x_2}\;{x_3}](x - {x_1})(x - {x_2})  \cr 
  &  + f[{x_1}\;{x_2}\;{x_3}\;{x_4}](x - {x_1})(x - {x_2})(x - {x_3}) + ...  \cr 
  &  + f[{x_1}\; \cdots \;{x_4}](x - {x_1}) \cdots (x - {x_{n - 1}}) \cr} 
$$
**牛顿差商**

> 给定x=[$x_1$,...,$x_n$]，y=[$y_1$,...,$y_n$]
>
> for j = 1,...,n
>
> ​	f[$x_j$]=$y_j$
>
> end
>
> for i = 2,...,n
>
> ​	for j=1,...,n+1-i
>
> ​		f[$x_j$...$x_{j+i-1}$]=(f[$x_{j+1}$...$x_{j+i-1}$]-f[$x_{j}$...$x_{j+i-2}$])/($x_{j+i-1}$-$x_j$)
>
> ​	end
>
> end



#### 插值代码

```matlab
% 程序 3.1 牛顿差商插值方法
% 计算插值多项式的系数
% 输入：x 和y是包含 n 个数据点的 x 和 y坐 标的向量
% 输出：嵌套形式的插值多项式系数 c
% 使用 nes 七 ． m 计算插值 多项式
function c=newtdd(x, y,n)
for j =1:n
	v(j,1) =y (j) ; 	% 填入牛顿三角形的y列
end
for i =2:n 	% 对于第l.列
	for j =1 : n+1- i % 从顶端到底端填入列元素
		v (j, i) = (v (j + 1, i -1) - v(j, i -1)) / (x (j +i - 1) -x (j));
	end
end
for i = 1 : n
	c(i) =v(1, i) ; % 从三角形顶端读
end  % 输出系数
```



### 插值误差

不过多赘述。



### 切比雪夫插值

切比雪夫插值是一种特定最优的点间距选取方式。

#### 切比雪夫理论

切比雪夫插值的动机是在插值区间上，提高对如下插值误差的最大值的控制
$$
{{\left( {x - {x_1}} \right)\left( {x - {x_2}} \right) \cdots \left( {x - {x_n}} \right)} \over {n!}}{f^{(n)}}(c)
$$
从现在开始让我们把区间固定在 `[-1,1]`。多项式插值误差的分子
$$
(x-x_1)(x-x_2)\cdots(x-x_n)
$$
本身是一个关于x的n阶多项式，并在区间 `[-1,1]` 上具有极值。是否可能在区间`[-1,1]`找到特定的$x_1$，...，$x_n$使得多项式插值误差的分子的最大值足够小？这被称为插值误差的最小最大问题。

**定理**：选择实数$-1\le x_1,\cdots, x_n\le 1$，使得
$$
\mathop {\max }\limits_{ - 1 \le x \le 1} \left| {\left( {x - {x_1}} \right) \cdots \left( {x - {x_n}} \right)} \right|
$$
尽可能小，则
$$
{x_i} = \cos {{\left( {2i - 1} \right)\pi } \over {2n}},\quad i = 1, \cdots n
$$
对应的最小值是$1/2^{n-1}$，实际上，通过
$$
\left( {x - {x_1}} \right) \cdots \left( {x - {x_n}} \right) = {1 \over {{2^{n - 1}}}}{T_n}(x)
$$
可以得到极小值，其中$T_n(x)$表示n阶切比雪夫多项式。

#### 切比雪夫多项式

定义n阶切比雪夫多项式$T_n(x)=\cos (n\arccos x)$，对于每个n都是关于x的多项式。令 $y=\arccos x$，因而 $\cos y=x$。
$$
\eqalign{
  & {T_{n + 1}}(x) = \cos (n + 1)y = \cos (ny + y) = \cos ny\cos y - \sin ny\sin y  \cr 
  & {T_{n - 1}}(x) = \cos (n - 1)y = \cos (ny - y) = \cos ny\cos y + \sin ny\sin y  \cr 
  & {T_{n + 1}}(x) + {T_{n - 1}}(x) = 2\cos ny\cos y = 2x{T_n}(x)  \cr 
  & {T_{n + 1}}(x) = 2x{T_n}(x) - {T_{n - 1}}(x) \cr} 
$$
**事实**：$\deg ({T_n}) = n$，主导系数是$2^{n-1}$。

**事实**：$T_n(1)=1$，$T_n(-1)=(-1)^n$。

**事实**：$T_n(x)$的最大绝对值是1，-1≤x≤1，这由$T_n(x)=\cos y$的形式得到。

**事实**：$T_n(x)$的所有过零点都在-1~1之间，实际上，过零点是0=cos(n arccosx)的解，由于cosy=0，当且仅当y=奇数∙（π／2）。
$$
\eqalign{
  & n\arccos x = odd \cdot \pi /2  \cr 
  & x = \cos {{odd \cdot \pi } \over {2n}} \cr}
$$
**事实**：$T_n(x)$在-1和1之间一共往返变化n+1次，实际上，这发生在cos0，cosπ／n，...，cos(n-1)π/n，cosπ。



#### 区间的变化

关于切比雪夫插值的讨论局限在区间 `[-1,1]`，可以将方法推广到一般的区间 `[a,b]`，移动基点使得它们在区间 `[a,b]` 上的相对位置和在区间 `[-1,1]` 上一致。这可以通过如下两步实现：

（1）使用因子(b-a)/2拉伸点（这是两个区间长度的比值），

（2）将点平移(b+a)／2，使得中心从0移动到区间 `[a,b]` 的中心。 

换句话讲，从原始点
$$
\cos {{odd\pi } \over {2n}} \to {{b - a} \over 2}\cos {{odd\pi } \over {2n}} + {{b - a} \over 2}
$$
使用区间 `[a,b]` 上新的切比雪夫基点$x_1$，...，$x_n$，插值误差公式中分子部分的上界也发生了改变，这是由于对因子$x-x_i$拉伸(b-a)/2，结果最小最大值1/$2^{n-1}$必须被替换为${\left[ {\left( {b - a} \right)/2} \right]^n}/{2^{n - 1}}$。

**切比雪夫插值节点**

> 在区间 `[a,b]`
> $$
> {x_i} = {{b + a} \over 2} + {{b - a} \over 2}\cos {{\left( {2i - 1} \right)\pi } \over {2n}}
> $$
> i=1,...,n，不等式
> $$
> \left| {\left( {x - {x_1}} \right) \cdots \left( {x - {x_n}} \right)} \right| \le {{{{\left( {{{b - a} \over 2}} \right)}^n}} \over {{2^{n - 1}}}}
> $$
> 在区间 `[a,b]` 上成立。

**举例**：在区间 `[0,π/2]` 找出4个切比雪夫基点进行插值，找出切比雪夫插值误差的上界，在区间中f(x)=sinx。
$$
{x_i} = {{{\pi  \over 2} + 0} \over 2} + {{{\pi  \over 2} - 0} \over 2}\cos {{\left( {2i - 1} \right)\pi } \over 8},\quad i = 1,2,3,4
$$

$$
\eqalign{
  & {x_1} = {\pi  \over 4} + {\pi  \over 4}\cos {\pi  \over 8}\quad \;\;{x_2} = {\pi  \over 4} + {\pi  \over 4}\cos {{3\pi } \over 8}  \cr 
  & {x_3} = {\pi  \over 4} + {\pi  \over 4}\cos {{5\pi } \over 8}\quad {x_4} = {\pi  \over 4} + {\pi  \over 4}\cos {{7\pi } \over 8} \cr} 
$$

插值误差的最坏情况是
$$
\eqalign{
  & \left| {\sin x - {P_3}(x)} \right| = {{\left| {\left( {x - {x_1}} \right)\left( {x - {x_2}} \right)\left( {x - {x_3}} \right)\left( {x - {x_4}} \right)} \right|} \over {4!}}\left| {f''''(c)} \right|  \cr 
  &  \le {{{{\left( {{{{\pi  \over 2} - 0} \over 2}} \right)}^4}} \over {4!{2^3}}}1 \approx 0.00198 \cr} 
$$
这里获得的只是插值时使用的基点，即[牛顿差商](#NewtonInterp)中$x_1$，$x_2$，...。



### 三次样条

样条是另一种数据插值的方式，在多项式插值中，多项式给出的单一公式满足所有数据点，而样条 则使用多个公式，其中每个都是低阶多项式，来通过所有数据点。线性样条可以成功地对任意的n 个点集进行插值，但是线性样条函数缺乏平滑，三次样条则可以解决线性样条的这个缺点。三次样条在两个数据点之间使用3阶(cubic)多项式替换线性样条两点之间的线性函数。由于通过n个点的三次样条无穷多，所以需要添加额外条件。

#### 样条的性质

假设给定n个点$(x_1,y_1)$，...，$(x_n,y_n)$，其中$x_i$不同且升序。通过n个点的三次样条S(x)是一组三次多项式
$$
\eqalign{
  & {S_1}(x) = {y_1} + {b_1}(x - {x_1}) + {c_1}{(x - {x_1})^2} + {d_1}{(x - {x_1})^3}\quad x \in [{x_1},{x_2}]  \cr 
  & {S_2}(x) = {y_2} + {b_2}(x - {x_2}) + {c_2}{(x - {x_2})^2} + {d_2}{(x - {x_2})^3}\quad x \in [{x_2},{x_3}]  \cr 
  &  \vdots   \cr 
  & {S_{n - 1}}(x) = {y_{n - 1}} + {b_{n - 1}}(x - {x_{n - 1}}) + {c_{n - 1}}{(x - {x_{n - 1}})^2} + {d_{n - 1}}{(x - {x_{n - 1}})^3}\quad x \in [{x_{n - 1}},{x_n}] \cr} 
$$
并具有如下性质：

（1）$S_i(x_i)=y_i, S_i(x_{i+1})=y_{i+1}$，i=1,2,...,n-1。

（2）$S'_{i-1}(x_i)=S'_i(x_i)$，其中i=2,...,n-1。

（3）$S''_{i-1}(x_i)=S''_i(x_i)$，其中i=2,...,n-1。

性质1保证样条S(x)插值数据点；性质2使得相邻的样条段在它们相遇的地方斜率相同；性质3则保证在两条样条段相邻的地方曲率相同，该曲率由二阶导数表示。

**只有满足上面的这些条件才被称为三次样条**

**性质4a（自然样条）**：$S''_{1}(x_1)=S''_{n-1}(x_n)=0$

满足性质4a这两个附加条件的三次样条被称为**自然**三次样条，自然三次样条是唯一的。

$\delta_i=x_{i+1}-x_i$，$\Delta_i=y_{i+1}-y_i$

![image-20231103222300906](D:\TyporaImages\image-20231103222300906.png)

**自然三次样条**

> 给定$x=[x_1,\cdots,x_n]$，其中$x_1<\cdots<x_n$，$y=[y_1,\cdots,y_n]$
>
> for i = 1,...,n-1
>
> ​	$a_i=y_i$
>
> ​	$\delta_i=x_{i+1}-x_i$
>
> ​	$\Delta_i=y_{i+1}-y_i$
>
> end
>
> 求解3.24得到$c_1,\cdots,c_n$
>
> for i = 1,...,n-1
>
> ​	$d_i={c_{i+1}-c_i\over 3\delta_i}$
>
> ​	$b_i={\Delta_i\over \delta_i}-{\delta_i\over 3}(2c_{i}+c_{i+1})$
>
> end

除了性质4a给出的自然三次样条，还有曲率调整三次样条、钳制三次样条、抛物线端点的三次样条、非纽结三次样条。



### 贝塞尔曲线

贝塞尔样条是一个允许用户控制节点处斜率的样条。作为额外自由控制的代价，在节点处的一阶和二阶导数的平滑性不再能保证，而这种平滑性是前面三次样条本身就具有的性质。贝塞尔样条适合不时出现角点（一阶导数不连续）和曲率突变（二阶导数不连续）的情况。

平面贝塞尔样条的每一段由4个点$(x_1,y_1)$，$(x_2,y_2)$，$(x_3,y_3)$，$(x_4,y_4)$所确定，第一个点和最后一个点是样条的起点和终点，中间的两个点是控制点。如图所示，曲线以$(x_2-x_1,y_2-y_1)$离开$(x_1,y_1)$，并以切线方向$(x_4-x_3,y_4-y_3)$在$(x_4,y_4)$点结束。

<img src="D:\TyporaImages\image-20231103223127691.png" alt="image-20231103223127691" style="zoom: 80%;" />

**贝塞尔曲线**

> 给定端点$(x_1,y_1)$，$(x_4,y_4)$
>
> 控制点$(x_2,y_2)$，$(x_3,y_3)$
>
> 设
> $$
> \eqalign{
> & {b_x} = 3({x_2} - {x_1})  \cr 
> & {c_x} = 3({x_3} - {x_2}) - {b_x}  \cr 
> & {d_x} = {x_4} - {x_1} - {b_x} - {c_x}  \cr 
> & {b_y} = 3({y_2} - {y_1})  \cr 
> & {c_y} = 3({y_3} - {y_2}) - {b_y}  \cr 
> & {d_y} = {y_4} - {y_1} - {b_y} - {c_y} \cr} 
> $$
> 定义在0≤t≤1的贝塞尔曲线如下：
> $$
> \eqalign{
> & x(t) = {x_1} + {b_x}t + {c_x}{t^2} + {d_x}{t^3}  \cr 
> & y(t) = {y_1} + {b_y}t + {c_y}{t^2} + {d_y}{t^3} \cr}
> $$

计算机中的字母或者数字便是使用二维贝塞尔曲线画出来的，如Times Roman字体中大写T字符由下面的16条贝塞尔曲线构成，每条曲线包含$x_1$，$y_1$，$x_2$，$y_2$，$x_3$，$y_3$，$x_4$，$y_4$来定义每一段的贝塞尔样条。

237 620 237 620 237 120 237 120;
237 120 237 35 226 24 143 19;
143 19 143 19 143 0 143 0;
143 0 143 0 435 0 435 0;
435 0 435 0 435 19 435 19;
435 19 353 23 339 36 339 109;
339 109 339 108 339 620 339 620;
339 620 339 620 393 620 393 620;
393 620 507 620 529 602 552 492;
552 492 552 492 576 492 576 492;
576 492 576 492 570 662 570 662;
570 662 570 662 6 662 6 662;
6 662 6 662 0 492 0 492;
0 492 0 492 24 492 24 492;
24 492 48 602 71 620 183 620;
183 620 183 620 237 620 237 620;

画出来的曲线为：

<img src="D:\TyporaImages\image-20231103231902200.png" alt="image-20231103231902200" style="zoom:50%;" />

python代码： [BezierCurve[贝塞尔曲线].py](codes\BezierCurve[贝塞尔曲线].py) 



## 最小二乘

### 最小二乘与法线方程

在方程组一章中，给出了当方程解存在时，如何找到Ax＝b的解。我们将学习当解不存在的时候该怎么办。当方程不一致时，有可能方程的个数超过未知变量的个数，答案是找到第二可能好的解：即最小二乘近似．

在插值一章中给出了如何找出多项式，并精确拟合数据点。但是如果有大量的数据点，或者采集的数据点具有一定误差，使用高阶多项式精确拟合一般不是个好方法。在这种情况下，使用简单模型 近似数据是一种更合理的方式。

求解不一致的方程组以及近似拟合数据这两个问题一同驱动着最小二乘的发展．



#### 不一致的方程组

如果一个方程组无解，它被称为**不一致**。方程无解意味着什么？可能系数不十分精确，在很多情况下，方程的个数比未知变量的个数要多，使得解不可能满足所有的方程。但是还可以通过找出与解最相似的向量x。

如果我们选择“相似度”意味着欧氏距离相近，有一个直接的方法找到最接近的x，这个特殊的x称为 最小二乘解。

对于Ax=b这个方程而言，如果b不在Ax可以确定的平面（A中的每一列看成基向量，对应的x的一行（就是一个值）便可看成该方向的值）上，那么就是无解的，不过可以在这个平面上找到与b最接近的向量来近似。

最小二乘基于正交，从一点到一个平面的最短距离，由一个到平面的正交线段表示，法线方程可以确定该线段，这表示最小二乘的误差。

假设这个近似解为$\bar x$，则$b-A\bar x$代表余弦误差，让误差与平面正交，即
$$
\eqalign{
  & {A^T}(b - A\bar x) = 0  \cr 
  & {A^T}A\bar x = {A^T}b \cr}
$$
此时这个方程便是可解的，不过解出的结果与实际结果可能存在一定差距，有三种方式可以表示余项的大小。

（1）欧式长度：$||r||_2=\sqrt{r_1^2+\cdots+r_m^2}$

（2）2范数，平方误差：$SE=r_1^2+\cdots+r_m^2$

（3）平均平方根误差：$RMSE=\sqrt{SE/m}=\sqrt{(r_1^2+\cdots+r_m^2)/m}$

三种表达紧密相关。



#### 数据的拟合模型

**最小二乘数据拟合**

> 给定一组数据点$(t_1,y_1)$，...，$(t_m,y_m)$
>
> step1 选择模型，确定用于拟合数据的参数模型，例如y=a+bt
>
> step2 将数据点代入模型，每个数据点生成一个方程，其中的未知变量是模型参数，例如线性模型中的$c_1$和$c_2$为参数，这得到系统Ax=b，其中未知变量x表示未知参数。
>
> step3 参数的最小二乘解是法线方程${A^T}A\bar x = {A^T}b$的解。

最小二乘是数据压缩的经典例子，输入包含一组数据点，输出是一个尽可能好的数据拟合模型，其中具有较少的参数。通常，使用最小二乘的原因是使用合理的底层模型替换噪声数据，该模型通常用于信号预测以及分类。

#### 最小二乘的条件

当$A^TA$的条件数过大，难以在双精度算术中处理，即使原始的最小二乘问题条件还可以，但是这个法线方程是病态问题。



### 模型概述

#### 周期数据

周期数据使用周期模型。例如外层大气温度遵循大时间尺度的循环，包含由地球自转和绕太阳公转控制的每天和每年的循环。

如拟合一天为周期的温度变化，可以使用模型$y=c_1+c_2\cos 2\pi t+c_3 \sin 2\pi t + c_4 \cos 4\pi t$。

#### 数据线性化

以描述人口变化的指数模型为例：
$$
y=c_1e^{c_2t}
$$
不能直接进行最小二乘拟合，这是由于$c_2$在模型方程中不是线性的。一旦将数据点代入模型，求解的困难很明显：要求解的方程系数不是线性，不能表示为线性方程组Ax＝b。因而我们导出的法线方程不能使用。

不过有两种方法可以用来解决这个问题，第一种方法是直接求解非线性最小二乘方程（在后面介绍），第二种是通过取对数来使问题线性化。
$$
\ln y=\ln (c_1e^{c_2t})=\ln c_1+c_2t
$$
同样对于幂法则模型$y=c_1e^{c_2t}$，也可以采用这种方法解决。



### QR分解

#### 格拉姆-施密特正交与最小二乘

格拉姆－施密特方法是对一组向量正交化。给定一组输入的m维向量，目的是找出正交坐标系统，获取由这些向量张成的空间。更精确地讲，给定n个线性无关的输入向量，该方法计算n个彼此垂直的单位向量（单位长度由2范数定义），张成和输入向量相同的子空间。

格拉姆-施密特正交的思想是对向量减去其投影在其它已经正交好的向量上的分量，得到正交于这些正交好的向量，在进行单位化即可。令$r_{ij}=q_i^TA_j$表示第j个向量投影在第i个已正交化好的向量上的分量长度，再乘上$q_i$（表方向的单位向量）即表示第j个向量在第i个已正交化好的向量上的投影。定义$r_{jj}=||y_j||_2$，$r_{ij}=q_i^TA_j$，则可以将格拉姆-施密特正交换一种方式表达
$$
\eqalign{
  & {A_1} = {r_{11}}{q_1}  \cr 
  & {A_2} = {r_{12}}{q_1} + {r_{22}}{q_2}  \cr 
  &  \cdots   \cr 
  & {A_j} = {r_{1j}}{q_1} +  \cdots  + {r_{j - 1,j}}{q_{j - 1}} + {r_{jj}}{q_j} \cr} 
$$
或者写成矩阵形式
$$
\left( {\matrix{
   {{A_1}} &  \cdots  & {{A_n}}  \cr 

 } } \right) = \left( {\matrix{
   {{q_1}} &  \cdots  & {{q_n}}  \cr 

 } } \right)\left[ {\matrix{
   {{r_{11}}} & {{r_{12}}} &  \cdots  & {{r_{1n}}}  \cr 
   \; & {{r_{22}}} &  \cdots  & {{r_{2n}}}  \cr 
   \; & \; &  \ddots  &  \vdots   \cr 
   \; & \; & \; & {{r_{nn}}}  \cr 

 } } \right]
$$
或A=QR，其中A是包含列向量$A_j$的矩阵，我们将其称为**消减QR分解**。关于$A_j$线性无关的假设保证主对角线系数$r_{jj}$非0。

**经典格拉姆-施密特正交**

> 令$A_j$（j=1，...，n）为线性无关向量。
>
> for j = 1，2，...，n
>
> ​	$y=A_j$
>
> ​	for i = 1，2，...，j-1
>
> ​		$r_{ij}=q_i^TA_j$
>
> ​		$y=y-r_{ij}q_i$
>
> ​	end
>
> ​	$r_{jj}=||y||_2$
>
> ​	$q_j=y/r_{jj}$
>
> end

当成功分解后，通常会填满正交单位向量的矩阵，从而得到$R^m$完整的基。可以通过在$A_j$中加入m-n个额外的向量，因而m向量可以张成$R^m$，并实现格拉姆-施密特方法。如下所示
$$
\left( {\matrix{
   {{A_1}} &  \cdots  & {{A_n}}  \cr 

 } } \right) = \left( {\matrix{
   {{q_1}} &  \cdots  & {{q_m}}  \cr 

 } } \right)\left[ {\matrix{
   {{r_{11}}} & {{r_{12}}} &  \cdots  & {{r_{1n}}}  \cr 
   \; & {{r_{22}}} &  \cdots  & {{r_{2n}}}  \cr 
   \; & \; &  \ddots  &  \vdots   \cr 
   \matrix{
  \; \hfill \cr 
  0 \hfill \cr 
   \vdots  \hfill \cr 
  0 \hfill \cr}  & \matrix{
  \; \hfill \cr 
   \cdots  \hfill \cr 
  \; \hfill \cr 
   \cdots  \hfill \cr}  & \matrix{
  \; \hfill \cr 
   \cdots  \hfill \cr 
  \; \hfill \cr 
   \cdots  \hfill \cr}  & \matrix{
  {r_{nn}} \hfill \cr 
  0 \hfill \cr 
   \vdots  \hfill \cr 
  0 \hfill \cr}   \cr 

 } } \right]
$$
注意A是m×n矩阵，Q是m×m方阵，上三角矩阵R是m×n矩阵，m>n，这称为A的**完全QR分解**，Q在数值分析中具有特殊的地位，并具有一个特殊的定义。

**定义**：当$Q^T=Q^{-1}$，方阵Q正交。

**引理**：如果Q是m×m正交矩阵，x是m维向量，则
$$
||Qx||_2=||x||_2
$$
LU分解是对高斯消去中信息进行有效编码的方式。以相同方式，QR分解记录了矩阵正交化的信息 ，即构造一个正交集，张出由A的列向量构成的空间。使用正交矩阵计算更好的原因是

（1）根据定义它们很容易求逆，

（2）由引理知，它们不会放大误差。

QR分解可用于求解n个方程n个未知量的方程组，但是计算复杂度比LU分解大。

QR分解可以用于最小二乘，令A是m×n矩阵，其中m≥n。为了最小化$||Ax-b||_2$，使用上面的引理得到：
$$
\eqalign{
  & {\left\| {Qx} \right\|_2} = {\left\| x \right\|_2}  \cr 
  & {\left\| {QRx - b} \right\|_2} = {\left\| {{Q^{ - 1}}\left( {QRx - b} \right)} \right\|_2} = {\left\| {Rx - {Q^T}b} \right\|_2} \cr} 
$$
欧几里得范数中的向量是
$$
\left[ {\matrix{
   {{e_1}}  \cr 
    \vdots   \cr 
   {{e_n}}  \cr 
   {{e_{n + 1}}}  \cr 
    \vdots   \cr 
   {{e_m}}  \cr 

 } } \right] = \left[ {\matrix{
   {{r_{11}}} & {{r_{12}}} &  \cdots  & {{r_{1n}}}  \cr 
   \; & {{r_{22}}} &  \cdots  & {{r_{2n}}}  \cr 
   \; & \; &  \ddots  &  \vdots   \cr 
   \matrix{
  \; \hfill \cr 
  0 \hfill \cr 
   \vdots  \hfill \cr 
  0 \hfill \cr}  & \matrix{
  \; \hfill \cr 
   \cdots  \hfill \cr 
  \; \hfill \cr 
   \cdots  \hfill \cr}  & \matrix{
  \; \hfill \cr 
   \cdots  \hfill \cr 
  \; \hfill \cr 
   \cdots  \hfill \cr}  & \matrix{
  {r_{nn}} \hfill \cr 
  0 \hfill \cr 
   \vdots  \hfill \cr 
  0 \hfill \cr}   \cr 

 } } \right]\left[ {\matrix{
   {{x_1}}  \cr 
    \vdots   \cr 
   {{x_n}}  \cr 

 } } \right] - \left[ {\matrix{
   {{d_1}}  \cr 
    \vdots   \cr 
   {{d_n}}  \cr 
   {{d_{n + 1}}}  \cr 
    \vdots   \cr 
   {{d_m}}  \cr 

 } } \right]
$$
其中$d=Q^Tb$，假设$r_{ii}\neq0$，则误差向量$e$上面的部分$(e_1,\cdots,e_n)$可以通过回代变为0，选择$x_i$使得误差向量下面部分没有受到影响；显然$(e_{n+1},\cdots,e_m)=(-d_{n+1},\cdots,-d_m)$。因而，通过使用上面部分回代得到的x可以最小化最小二乘的解，最小二乘误差是$||e||_2^2=d_{n+1}^2+\cdots+d_m^2$。

**通过QR分解实现最小二乘**

> 给定m×n不一致系统
> $$
> Ax=b
> $$
> 找出完全QR分解A=QR，令
>
> $\hat R=R$的上n×n子矩阵
>
> $\hat d=d=Q^Tb$的上面的n个元素
>
> 求解$\hat R\bar x=\hat d$得到最小二乘解$\bar x$

相比于法线方程求解最小二乘，这个更精确。，可以更好处理病态问题。



#### 改进的格拉姆-施密特正交

**改进的格拉姆-施密特正交**

>令$A_j$（j=1，...，n）为线性无关向量。
>
>for j = 1，2，...，n
>
>​	$y=A_j$
>
>​	for i = 1，2，...，j-1
>
>​		$r_{ij}=q_i^Ty$
>
>​		$y=y-r_{ij}q_i$
>
>​	end
>
>​	$r_{jj}=||y||_2$
>
>​	$q_j=y/r_{jj}$
>
>end

与经典格拉姆-施密特唯一不同的是，$A_j$被y在最内层循环中替换。从几何上来讲，例如当减去$A_j$在$q_2$方向的投影时，应该减去$A_j$的余项y的投影，在余项中$q_1$部分已经减掉了，而不是将$A_j$自己投在$q_2$上。



#### 豪斯霍尔德反射子

尽管改进的格拉姆-施密特正交是计算矩阵的QR分解的有效方式，但是这不是最好方式。另一种方法是使用豪斯霍尔德反射，这种方式需要更少的计算，同时在舍入误差放大的意义上来讲这种方法也更稳定。

豪斯霍尔德反射子是正交矩阵，通过m-1维平面反射m维向量。这意味着当每个向量乘上矩阵后，长度保持不变，使得豪斯霍尔德反射被称为移动向量的完美形式。给定一个向量x，我们要重新找出一个相同长度的向量w，计算豪斯霍尔德反射得出矩阵H满足Hx=w。

如图所示，原始方法非常清晰。画出m-1维平面二分x和w，并和连接它们的向量垂直，然后通过该平面反射所有向量。

<img src="D:\TyporaImages\image-20231104104642216.png" alt="image-20231104104642216" style="zoom:67%;" />

**引理**：假设x和w是具有相同欧几里得长度的向量，$||x||_2=||w||_2$，则w-x和w+x正交。

定义向量v=w-x，考虑投影矩阵
$$
P = {{v{v^T}} \over {{v^T}v}}
$$
**投影矩阵**是一个满足$P^2=P$的矩阵。并且P满足Pv=v。几何上说，对于任何向量u，Pu是u在v上的投影。从上图可以看到，如果从x中两次减去Px，就可以得到w。令H=I-2P，则
$$
\eqalign{
  & Hx = x - 2Px = w - v - {{2v{v^T}x} \over {{v^T}v}} = w - v - {{v{v^T}x} \over {{v^T}v}} - {{v{v^T}(w - v)} \over {{v^T}v}}  \cr 
  &  = w - {{v{v^T}v} \over {{v^T}v}} - {{v{v^T}x} \over {{v^T}v}} - {{v{v^T}(w - v)} \over {{v^T}v}} = w - {{v{v^T}w} \over {{v^T}v}} - {{v{v^T}(w - v)} \over {{v^T}v}}  \cr 
  &  = w - {{v{v^T}(2w - v)} \over {{v^T}v}} = w - {{v{{(w - x)}^T}(w + x)} \over {{v^T}v}} = w \cr} 
$$
矩阵H被称为豪斯霍尔德反射子，注意到H是对称并正交的矩阵。

**定理（豪斯霍尔德反射子）**：令x和w是向量，$||x||_2=||w||_2$，并定义v=w-x，则$H=I-2vv^T/v^Tv$是对称正交矩阵，并且Hx=w。

使用豪斯霍尔德反射子推导出QR分解的一个新方式，使用反射子的目的是将列向量x移动到坐标轴，并以此将0放在矩阵中。

我们从矩阵A开始，希望写做形如A=QR的方程。令$x_1$是A的第一列，令w=$\pm$(${\left\| {{x_1}} \right\|_2}$，0，...，0)为第一个坐标轴上的向量，它们的欧几里得长度相同。为了数值稳定，一般将x的第一个元素选为正号，以避免在生成v的过程中出现两个近似相等的数字相减。生成豪斯霍尔德反射子并满足$H_1x=w$，在4×3的情况下，用A乘上$H_1$得到
$$
{H_1}A = {H_1}\left[ {\matrix{
    \times  &  \times  &  \times   \cr 
    \times  &  \times  &  \times   \cr 
    \times  &  \times  &  \times   \cr 
    \times  &  \times  &  \times   \cr 

 } } \right] = \left[ {\matrix{
    \times  &  \times  &  \times   \cr 
   0 &  \times  &  \times   \cr 
   0 &  \times  &  \times   \cr 
   0 &  \times  &  \times   \cr 

 } } \right]
$$
我们已经在A中引入一些0，我们希望继续这种方式直到A变为上三角；然后我们将得到QR分解中的R。对于一个3列的矩阵，还需要做两次这样的操作，即
$$
\eqalign{
  & {H_3}{H_2}{H_1}A = R  \cr 
  & A = {H_1}^{ - 1}{H_2}^{ - 1}{H_3}^{ - 1}R = {H_1}{H_2}{H_3}R = QR \cr} 
$$
**举例**：使用豪斯霍尔德反射子找到矩阵A的QR分解
$$
A = \left[ {\matrix{
   1 & { - 4}  \cr 
   2 & 3  \cr 
   2 & 2  \cr 

 } } \right]
$$
豪斯霍尔德反射子$H_1$将第一列x=[1,2,2]移动到$w=[||x||_2,0,0]$=[3,0,0]，v=w-x=[2,-2,-2]
$$
{H_1} = I - 2{{v{v^T}} \over {{v^T}v}} = \left[ {\matrix{
   1 & 0 & 0  \cr 
   0 & 1 & 0  \cr 
   0 & 0 & 1  \cr 

 } } \right] - {2 \over {12}}\left[ {\matrix{
   4 & { - 4} & { - 4}  \cr 
   { - 4} & 4 & 4  \cr 
   { - 4} & 4 & 4  \cr 

 } } \right] = \left[ {\matrix{
   {{1 \over 3}} & {{2 \over 3}} & {{2 \over 3}}  \cr 
   {{2 \over 3}} & {{1 \over 3}} & { - {2 \over 3}}  \cr 
   {{2 \over 3}} & { - {2 \over 3}} & {{1 \over 3}}  \cr 

 } } \right]
$$

$$
{H_1}A = \left[ {\matrix{
   {{1 \over 3}} & {{2 \over 3}} & {{2 \over 3}}  \cr 
   {{2 \over 3}} & {{1 \over 3}} & { - {2 \over 3}}  \cr 
   {{2 \over 3}} & { - {2 \over 3}} & {{1 \over 3}}  \cr 

 } } \right]\left[ {\matrix{
   1 & { - 4}  \cr 
   2 & 3  \cr 
   2 & 2  \cr 

 } } \right] = \left[ {\matrix{
   3 & 2  \cr 
   0 & { - 3}  \cr 
   0 & { - 4}  \cr 

 } } \right]
$$

下面使用$\hat H_2$将向量 $\hat x$=[-3,-4]移动$\hat w$=[5,0]，
$$
\eqalign{
  & {{\hat H}_2} = I - 2{{v{v^T}} \over {{v^T}v}} = \left[ {\matrix{
   1 & 0  \cr 
   0 & 1  \cr 

 } } \right] - {2 \over {80}}\left[ {\matrix{
   {64} & { - 32}  \cr 
   { - 32} & {16}  \cr 

 } } \right] = \left[ {\matrix{
   { - 0.6} & { - 0.8}  \cr 
   { - 0.8} & {0.6}  \cr 

 } } \right]  \cr 
  & {{\hat H}_2}{{\hat A}_2} = \left[ {\matrix{
   { - 0.6} & { - 0.8}  \cr 
   { - 0.8} & {0.6}  \cr 

 } } \right]\left[ {\matrix{
   { - 3}  \cr 
   { - 4}  \cr 

 } } \right] = \left[ \matrix{
  5 \hfill \cr 
  0 \hfill \cr}  \right] \cr} 
$$
因此，有
$$
{H_2}{H_1}A = \left[ {\matrix{
   1 & 0 & 0  \cr 
   0 & { - 0.6} & { - 0.8}  \cr 
   0 & { - 0.8} & {0.6}  \cr 

 } } \right]\left[ {\matrix{
   {{1 \over 3}} & {{2 \over 3}} & {{2 \over 3}}  \cr 
   {{2 \over 3}} & {{1 \over 3}} & { - {2 \over 3}}  \cr 
   {{2 \over 3}} & { - {2 \over 3}} & {{1 \over 3}}  \cr 

 } } \right]\left[ {\matrix{
   1 & { - 4}  \cr 
   2 & 3  \cr 
   2 & 2  \cr 

 } } \right] = \left[ {\matrix{
   3 & 2  \cr 
   0 & 5  \cr 
   0 & 0  \cr 

 } } \right] = R
$$

$$
A=H_1H_2R=QR
$$

与格拉姆-施密特正交相比，QR分解的计算代价更低，可以更好地得到单位正交向量以及具有更低的内存需求。



### 广义最小余项（GMRES）方法



#### Krylov方法

GMRES属于Krylov方法，这些方法依赖精确的Krylov空间的计算，该空间是向量{r，Ar，...，$A^kr$}所张的空间，其中$r=b-Ax_0$是初始估计的余项。由于向量$A^kr$对于大的k倾向于一个共有方向，Krylov空间的基必须认真计算。找出Krylov空间精确的基需要正交化计算方法，例如格拉姆-施密特或者豪斯霍尔德反射子方法。

GMRES的思想是在特殊矢量空间，即Krylov空间寻找初始估计的$x_0$的改进，Krylov空间由余项r和它与非奇异矩阵A的积所张成。在该方法的第k步，加入$A^kr$以扩大Krylov空间，重新对基进行正交化，然后通过最小二乘获取改进并加入到$x_0$中。

**广义最小余项方法**

> $x_0$=初始估计
>
> $r=b-Ax_0$
>
> $q_1=r/||r||_2$
>
> for k = 1,2,...,m
>
> ​	$y=Aq_k$
>
> ​	for j = 1,2,...,k
>
> ​		$h_{jk}=q_j^Ty$
>
> ​		$y=y-h_{jk}q_j$
>
> ​	end
>
> ​	$h_{k+1,k}=||y||_2$（如果$h_{k+1,k}=0$，跳过下一行，并在底端终止）
>
> ​	$q_{k+1}=y/h_{k+1,k}$
>
> ​	最小化 $||Hc_k-[||r||_2,0,0,...,0]^T||_2$得到$c_k$
>
> ​	$x_k=Q_kc_k+x_0$
>
> end

迭代的$x_k$是系统Ax=b的近似解，如果$h_{k+1,k}=0$，表示步骤k是最后一步。因为$x_k$在循环中没有使用，最小化得到$c_k$甚至可以放到循环外最后来做。如果条件数是一个重要问题，在内环进行的格拉姆-施密特正交步骤可以用豪斯霍尔德正交替换。GMRES的典型用途是用于大规模稀疏的n×n矩阵A。 理论上，算法经过n步终止，只要A是非奇异矩阵就可以得到解x。在大多数情况下，目标是仅仅运行k步，k比n要小得多。注意到矩阵$Q_k$是n×k矩阵，并不保证是稀疏矩阵。因而内存也可能限制GMRES方法中的步数k。如果k步迭代后没能足够趋近解，而且如果n×k矩阵$Q_k$变得大得难以处理，有一个简单想法：扔掉$Q_k$重新开始GMRES方法，使用当前的最优估计$x_k$作为新的$x_0$，这种方法被称为**重启GMRES**。



#### 预条件GMRES

在预条件GMRES背后的概念和共枙梯度法非常相似，从非对称的线性方程组Ax=b开始，我们试图求解$M^{-1}Ax=M^{-1}b$，其中M是方程组中讨论的预条件子。

**预条件GMRES**

>$x_0$=初始估计
>
>$r=M^{-1}(b-Ax_0)$
>
>$q_1=r/||r||_2$
>
>for k = 1,2,...,m
>
>​	$w=M^{-1}Aq_k$
>
>​	for j = 1,2,...,k
>
>​		$h_{jk}=w^Tq_j$
>
>​		$w=w-h_{jk}q_j$
>
>​	end
>
>​	$h_{k+1,k}=||w||_2$
>
>​	$q_{k+1}=w/h_{k+1,k}$
>
>​	最小化 $||Hc_k-[||r||_2,0,0,...,0]^T||_2$得到$c_k$
>
>​	$x_k=Q_kc_k+x_0$
>
>end



### 非线性最小二乘

#### 高斯-牛顿方法

首先会议多变量的牛顿方法，并将其应用在列向量函数$F(x)^T=(r^TDr)^T=(Dr)^Tr$，
$$
DF(x)^T=D((Dr)^Tr)=(Dr)^T\cdot Dr+\sum_{i=1}^mr_iDc_i
$$
其中$c_i$是Dr的第i列，注意到$Dc_i=Hr_i$，$r_i$的2阶偏导数矩阵或称为海森矩阵如下：
$$
{H_{{r_i}}} = \left[ {\matrix{
   {{{{\partial ^2}{r_i}} \over {\partial {x_1}\partial {x_1}}}} &  \cdots  & {{{{\partial ^2}{r_i}} \over {\partial {x_1}\partial {x_n}}}}  \cr 
    \vdots  & \; &  \vdots   \cr 
   {{{{\partial ^2}{r_i}} \over {\partial {x_n}\partial {x_1}}}} &  \cdots  & {{{{\partial ^2}{r_i}} \over {\partial {x_n}\partial {x_n}}}}  \cr 

 } } \right]
$$
**高斯牛顿方法**

> 为了最小化
> $$
> r_1(x)^2+\cdots+r_m(x)^2
> $$
> 令$x^0$=初始向量，
>
> for k = 0,1,2,...
>
> ​	$A=Dr(x^k)$	
>
> ​	$A^TAv^k=-A^Tr(x^k)$
>
> ​	$x^{k+1}=x^k+v^k$
>
> end

$$
Dr(x,y) = \left[ {\matrix{
   {{{\partial r} \over {\partial {x_1}}}} & {{{\partial r} \over {\partial {y_1}}}}  \cr 
    \vdots  &  \vdots   \cr 
   {{{\partial r} \over {\partial {x_n}}}} & {{{\partial r} \over {\partial {y_n}}}}  \cr 

 } } \right]
$$

#### 具有非线性参数的模型

定义矩阵$D_r$是误差$r_i$关于参数$c_j$偏导数矩阵
$$
(Dr)_{ij}={\part r_i \over \part c_j}=f_{c_j}(t_i)
$$


**举例**：拟合模型$c_1e^{c_2t}$

定义余项
$$
r = \left[ \matrix{
  {c_1}{e^{{c_2}{t_1}}} - {y_1} \hfill \cr 
   \cdots  \hfill \cr 
  {c_1}{e^{{c_2}{t_m}}} - {y_m} \hfill \cr}  \right]
$$
并相对$c_1$和$c_2$计算偏导数得到
$$
\eqalign{
  & r = \left[ \matrix{
  {c_1}{e^{{c_2}{t_1}}} - {y_1} \hfill \cr 
   \cdots  \hfill \cr 
  {c_1}{e^{{c_2}{t_m}}} - {y_m} \hfill \cr}  \right]  \cr 
  & Dr = \left[ {\matrix{
   {{e^{{c_2}{t_1}}}} & {{c_1}{t_1}{e^{{c_2}{t_1}}}}  \cr 
    \vdots  &  \vdots   \cr 
   {{e^{{c_2}{t_m}}}} & {{c_1}{t_m}{e^{{c_2}{t_m}}}}  \cr 

 } } \right] \cr} 
$$
最小二乘问题中的非线性带来额外的挑战。法线方程以及QR方法只要系数矩阵A满秩都可以找到唯一解。而对于非线性问题的高斯-牛顿迭代可能收敛到多个极小平方误差中的一个。尽可能使用初始向量的合理近似，有助于收敛到绝对极小。



#### Levenberg-Marquardt方法

非线性最小二乘法遇到病态系数矩阵时，会面临巨大的挑战，Levenberg-Marquardt方法使用“正则化项”部分修复这个问题。

**Levenberg-Marquardt方法**

> 最小化
> $$
> r_1(x)^2+\cdots+r_m(x)^2
> $$
> 令$x^0$=初始向量，$\lambda$=常数
>
> for k = 0,1,2,...
>
> ​	$A=Dr(x^k)$	
>
> ​	$(A^TA+\lambda diag(A^TA))v^k=-A^Tr(x^k)$
>
> ​	$x^{k+1}=x^k+v^k$
>
> end

提高正则化参数λ可以增强矩阵$A^TA$对角线元素的作用，这通常可以改善条件数，允许方法从一个比高斯-牛顿更宽的初始估计$x_0$开始，并实现收敛。



## 数值微分和积分

计算微积分解决的主要问题是计算函数的导数和积分。

### 数值微分

#### 有限差分公式

**二点前向差分公式**
$$
f'(x) = {{f(x + h) - f(x)} \over h} - {h \over 2}f''(c)
$$
其中c在x和x+h之间，$- {h \over 2}f''(c)$是误差项。

**定理（推广中值定理）**：令f是区间 `[a,b]` 上的连续函数，令$x_1$，...，$x_n$是区间 `[a,b]` 上的点，$a_1$，...，$a_n$>0，则在a和b之间存在数字c满足
$$
(a_1+\cdots+a_n)f(c)=a_1f(x_1)+\cdots+a_nf(x_n)
$$
**三点中心差分公式**
$$
f'(x) = {{f(x + h) - f(x - h)} \over {2h}} - {{{h^2}} \over 6}f'''(c)
$$
其中x-h<c<x+h。

**二阶导数的三点中心差分公式**
$$
f''(x) = {{f(x - h) - 2f(x) + f(x + h)} \over {{h^2}}} - {{{h^2}} \over {12}}{f^{(4)}}(c)
$$
其中x-h<c<x+h。

当h->0时，误差会先减小再增大，后增大的原因是计算时有效数字的丢失。对于三点中心差分公式而言，f'(x)的及其近似误差的绝对值上界是
$$
E(h) \equiv {{{h^2}} \over 6}f'''(c) + {{{\varepsilon _{mach}}} \over h}
$$

#### 外推

假设我们有n阶公式F(h)近似一个给定量Q，这个阶数意味着
$$
Q \approx F(h) + Kh''
$$
其中K大约是我们感兴趣的h区间上的一个常数，相关的例子是
$$
f'(x) = {{f(x + h) - f(x - h)} \over {2h}} - {{f'''({c_h})} \over 6}{h^2}
$$
$c_h$的位置在x+h和x-h之间，只要f合理光滑，h不太大，那么$f'''(c_h)$和$f'''(x)$相差不会太大。这样我们就可以把n阶公式变换成更高阶的公式，我们期望
$$
Q-F(h/2)\approx {1\over 2^n}(Q-F(h))
$$
**n阶公式的外推**
$$
Q \approx {{{2^n}F(h/2) - F(h)} \over {{2^n} - 1}}
$$
**举例**：应用外推公式
$$
\eqalign{
  & {F_4}(x) = {{{2^2}{F_2}(h/2) - {F_2}(h)} \over {{2^2} - 1}}  \cr 
  &  = \left[ {4{{f(x + h/2) - f(x - h/2)} \over h} - {{f(x + h) - f(x - h)} \over {2h}}} \right]/3  \cr 
  &  = {{f(x - h) - 8f(x - h/2) + 8f(x + h/2) - f(x + h)} \over {6h}} \cr} 
$$

### 数值积分的牛顿-科特斯公式

**梯形法则**
$$
\int_{{x_0}}^{{x_1}} {f(x)} dx = {h \over 2}\left( {{y_0} + {y_1}} \right) - {{{h^3}} \over {12}}f''(c)
$$
其中$h=x_1-x_0$，c在$x_0$和$x_1$之间。



**辛普森法则**
$$
\int_{{x_0}}^{{x_2}} {f(x)} dx = {h \over 3}\left( {{y_0} + 4{y_1} + {y_2}} \right) - {{{h^5}} \over {90}}{f^{(4)}}(c)
$$
其中$h=x_2-x_1=x_1-x_0$，c在$x_0$和$x_2$之间。

**定义**：数值积分方法的**精度**是最大的整数k，使用该积分方法可以得到所有k阶或者更低阶多项式积分的精确值。

梯形法则的精度是1，辛普森法则的精度是3。

#### 复合牛顿-科特斯公式

由于积分在区间的所有子区间上具有可加性，我们可以通过除法把整个区间变为很多小区间再计算积分，在每个小区间上使用法则，然后再求和。这种策略被称为**复合数值积分**。

**复合梯形法则**
$$
\int_a^b {f(x)} dx = {h \over 2}\left( {{y_0} + {y_m} + 2\sum\limits_{i = 1}^{m - 1} {{y_i}} } \right) - {{(b - a){h^2}} \over {12}}f''(c)
$$
其中h=(b-a)/m，c在a和b之间。

**复合辛普森公式**
$$
\int_a^b {f(x)} dx = {h \over 3}\left( {{y_0} + {y_{2m}} + 4\sum\limits_{i = 1}^m {{y_{2i - 1}}}  + 2\sum\limits_{i = 1}^{m - 1} {{y_{2i}}} } \right) - {{(b - a){h^4}} \over {180}}{f^{(4)}}(c)
$$
其中c在a和b之间。

#### 开牛顿-科特斯方法

**中点法则**
$$
\int_{{x_0}}^{{x_1}} {f(x)} dx = hf(w) + {{{h^3}} \over {24}}f''(c)
$$
其中$h=(x_1-x_0)$，w是中点$x_0+h/2$，c在$x_0$和$x_1$之间。

中点法则也可用于减少所需函数求值的次数与梯形法则相比，闭牛顿-科特斯方法具有相同的阶数，只需要一次函数求值，而不是两次。而且误差项是梯形法则误差项大小的一半。

**复合中点法则**
$$
\int_{{x_0}}^{{x_1}} {f(x)} dx = h\sum\limits_{i = 1}^m {f({w_i})}  + {{(b - a){h^2}} \over {24}}f''(c)
$$
其中h=(b-a)/m，c在a和b之间，$w_i$是 `[a,b]` 中 m 个相等子区间的中点。



### 龙贝格积分

龙贝格积分是对复合梯形法则应用外推的结果，给定近似M的法则N(h)，该法则依赖步长h，如果知道法则的阶，则可以对法则进行外推。

**龙贝格积分**

> $R_{11}=(b-a){f(a)+f(b)\over 2}$
>
> for j = 2,3,...n
>
> ​	$h_j={b-a\over 2^{j-1}}$
>
> ​	${R_{j1}} = {1 \over 2}{R_{j - 1,1}} + {h_j}\sum\limits_{i = 1}^{{2^{j - 2}}} {f(a + (2i - 1){h_j})}$
>
> ​	for k = 2,...,j
>
> ​		${R_{jk}} = {{{4^{k - 1}}{R_{j,k - 1}} - {R_{j - 1,k - 1}}} \over {{4^{k - 1}} - 1}}$
>
> ​	end
>
> end

最后的结果便是$R_{jj}$，龙贝格积分的常用停止条件是计算新的一行直到相邻的对角线元素$R_{jj}$差异小于当前的容差。



### 自适应积分

到目前我们学到的积分近似方法都使用相等的步长。一般来说，小步长可以提高精度变化剧烈的函数将需要更多步，因而带来更多的计算时间，这是由于需要更小的步长跟踪函数所有的变化。函数可能在定义域某些部分变化剧烈，而在另外一些部分变化缓慢，满足前一部分的容差的步长在后一部分就显得过于精细了。

通过使用积分误差公式，可以在运算中推出一个标准，其步长对于特定的子空间适合。这种方法称为**自适应积分**。

**自适应积分**

> 对给定容差TOL近似$\int_a^bf(x)dx$
>
> c = (a+b)/2
>
> $S_{[a,b]}$=(b-a)(f(a)+f(b))/2
>
> if $S_{[a,b]}-S_{[a,c]}-S_{[c,b]}<3\cdot TOL\cdot ({b-a \over b_{orig}-a_{orig}})$
>
> ​	接受$S_{[a,c]}+S_{[c,b]}$作为区间 `[a,b]` 上的近似
>
> else
>
> ​	对于区间`[a,c]`和`[c,b]`递归重复上面的步骤
>
> end

```matlab
% 程序 5 . 2 自适应积分
% 计算定积分的近似
% 输入： MATLAB函数f，区间 [ ao, bo]
% 容差 tol0
% 输出：近似定积分
function int=adapquad(f, a0, b0, tol0)
int = 0; n=1; a(1) = a0; b(1)=b0; tol(1)=tol0; 
app(1)=trap(f,a,b);                         % trap求梯形积分
while n >0                                  % n是当前列表结束的位置。
    c = (a(n) + b(n)) / 2; oldapp = app(n);
    app(n) = trap(f, a(n), c);
    app(n+1) = trap(f, c, b(n));
    if abs(oldapp - (app(n) + app(n+1))) < 3 * tol(n)
        int = int + app(n) + app(n+1);      % 成功
        n = n-1;                            % 该区间操作完成
    else                                    % 分成两个子区间
        b(n+1) = b(n); b(n) = c;            % 设置新区间
        a(n+1) = c;
        tol(n) = tol(n) / 2; tol(n+1) = tol(n);
        n = n + 1;                          % 到列表尾端，重复
    end
end
```

其中tol0是自己设置的参数，如设置为0.005。



### 高斯积分

**定义**：一组在区间`[a,b]`上的非0函数{$p_0$，...，$p_n$}在区间`[a,b]`**正交**，当
$$
\int_a^b {{p_j}(x){p_k}(x)} dx = \left\{ \matrix{
  0\quad \quad j \ne k \hfill \cr 
   \ne 0\quad j = k \hfill \cr}  \right.
$$
**定理**：如果{$p_0$，...，$p_n$}是区间`[a,b]`上多项式的正交集，其中$\deg p_i=i$，则{$p_0$，...，$p_n$}是在区间`[a,b]`由区间`[a,b]`由至多n个多项式所张的向量空间的基。

**定理**：如果{$p_0$，...，$p_n$}是区间`[a,b]`上多项式的正交集，且$\deg p_i=i$，则$p_i$在区间(a,b)上有i个不同的根。

**高斯积分**
$$
\int_{ - 1}^1 {f(x)} dx \approx \sum\limits_{i = 1}^n {{c_i}f({x_i})} 
$$
其中
$$
{c_i} = \int_{ - 1}^1 {{L_i}(x)} dx,i = 1, \cdots ,n
$$

$$
{L_i} = {{(x - {x_1}) \cdots \overline {(x - {x_i})}  \cdots (x - {x_n})} \over {({x_i} - {x_1}) \cdots \overline {({x_i} - {x_i})}  \cdots ({x_i} - {x_n})}}
$$

上面带横线的项表示不进行计算，$c_i$可以查表获得(包括n=1,2,3,4)

**定理**：高斯积分方法，在区间`[-1,1]`上使用n阶勒让德多项式，具有2n-1阶精度。

一般区间`[a,b]`的积分可以通过替代t=(2x-a-b)/(b-a)得到
$$
\int_a^b {f(x)} dx = \int_{ - 1}^1 {f\left( {{{(b - a)t + b + a} \over 2}} \right)} {{b - a} \over 2}dt
$$


## 常微分方程

### 初值问题

#### 欧拉方法

通过跟随箭头计算“求解”微分方程。从初始条件($t_0$，$y_0$)开始，然后沿着在那里指定的方向。在移动很小的距离后，在新点($t_1$，$y_1$)重新计算斜率，根据新的斜率继续移动，重复这个过程。

**欧拉方法**

> $w_0=y_0$
>
> $w_{i+1}=w_i+hf(t_i,w_i)$

h是步长，如取0.2，y'=f(t,w)。

**举例**：用欧拉方法求解如下问题
$$
\left\{ \matrix{
  y' = ty + {t^3} \hfill \cr 
  y(0) = {y_0} \hfill \cr 
  t \in [0,1] \hfill \cr}  \right.
$$
欧拉方法则对应如下迭代，h设置为0.1
$$
\eqalign{
  & {w_0} = 1  \cr 
  & {w_{i + 1}} = {w_i} + h({t_i}{w_i} + t_i^3) \cr}
$$

#### 解的存在性、唯一性和连续性

**定义**：当存在常数L（称为**利普希茨常数**）对矩形 S=[a,b]×[α,β]中的每对(t,$y_1$)，(t,$y_2$)满足
$$
\left| {f(t,{y_1}) - f(t,{y_2})} \right| \le L\left| {{y_1} - {y_2}} \right|
$$
函数f(t,y)相对于变量y在S上**利普西茨连续**。

**定理**：假设f(t,y)定义在集合[a,b]×[α,β]并且α<$y_a$<β，函数对于变量y是利普希茨连续，则在a与 b之间存在c，使得初值问题
$$
\left\{ \matrix{
  y' = f(t,y) \hfill \cr 
  y(a) = {y_a} \hfill \cr 
  t \in [a,c] \hfill \cr}  \right.
$$
有唯一解y(t)。而且，如果f在[a,b]×(-∞，∞)是利普西茨连续，则它在[a,b]上存在唯一解。

**定理**：假设f(t,y)在集合S=[a,b]×[α,β]上关于y是连续的，如果Y(t)和Z(t)是微分方程
$$
y'=f(t,y)
$$
在S上的解，分别具有初值条件Y(z)和Z(a)，则
$$
\left| {Y(t) - Z(t)} \right| \le {e^{L(t - a)}}\left| {Y(a) - Z(a)} \right|
$$

#### 一阶线性方程

一组容易求解的特定的常微分方程提供了ODE求解问题中具有指导意义的例子，这组方程是一阶方程，其右侧是关于y的线性函数。考虑初值问题
$$
\left\{ \matrix{
  y' = g(t)y + h(t) \hfill \cr 
  y(a) = {y_a} \hfill \cr 
  t \in [a,b] \hfill \cr}  \right.
$$
首先注意到如果g(t)在区间[a,b]上连续，唯一解存在，使用$L=\max_{[a,b]}g(t)$作为利普希茨常数可以使用技巧找到解，即对方程乘上“积分因子”。

积分因子是${e^{ - \int {g(t)} dt}}$，在方程两侧同时乘这个因子得到
$$
\eqalign{
  & \left( {y' - g(t)y} \right){e^{ - \int {g(t)} dt}} = {e^{ - \int {g(t)} dt}}h(t)  \cr 
  & (y{e^{ - \int {g(t)} dt}})' = {e^{ - \int {g(t)} dt}}h(t)  \cr 
  & y{e^{ - \int {g(t)} dt}} = \int {{e^{ - \int {g(t)} dt}}h(t)} dt  \cr 
  & y(t) = {e^{\int {g(t)} dt}}\int {{e^{ - \int {g(t)} dt}}h(t)} dt \cr} 
$$

### IVP求解器的分析

#### 显式梯形方法

**显式梯形方法**

>$w_0=y_0$
>
>${w_{i + 1}} = {w_i} + {h \over 2}\left( {f({t_i},{w_i}) + f({t_i} + h,{w_i} + hf({t_i},{w_i}))} \right)$

#### 泰勒方法

泰勒方法基本思想是直接利用泰勒展开，假设解y(t)是(k+1)阶连续可微的函数。给定在解曲线上的当前点(t,y(t))，目标是对于某个步长h，使用微分方程的信息，用y(t)来表达y(t+h)。

**K阶泰勒方法**

>$w_0=y_0$
>
>${w_{i + 1}} = {w_i} + hf({t_i},{w_i}) + {{{h^2}} \over 2}f'({t_i},{w_i}) +  \cdots  + {{{h^k}} \over {k!}}{f^{(k - 1)}}({t_i},{w_i})$

**举例**：对于一阶线性方程确定二阶泰勒方法
$$
\left\{ \matrix{
  y' = ty + {t^3} \hfill \cr 
  y(0) = {y_0} \hfill \cr}  \right.
$$
由于$f(t,y)=ty+t^3$，
$$
f'(t,y) = {f_t} + {f_y}f = y + 3{t^2} + t(ty + {t^3})
$$
所以二阶泰勒方法的迭代公式为
$$
{w_{i + 1}} = {w_i} + h({t_i}{w_i} + t_i^3) + {{{h^2}} \over 2}\left( {{w_i} + 3{t_i}^2 + {t_i}({t_i}{w_i} + {t_i}^3)} \right)
$$

### 常微分方程组

#### 高阶方程

单个的高阶微分方程可以转化为一个方程组，令
$$
y^{(n)}=f(t,y,y',y'',\cdots,y^{(n-1)})
$$
是n阶常微分方程，定义新的变量
$$
\eqalign{
  & {y_1} = y  \cr 
  & {y_2} = y'  \cr 
  & {y_3} = y''  \cr 
  &  \vdots   \cr 
  & {y_n} = {y^{(n - 1)}} \cr} 
$$
注意到原始的常微分方程写成
$$
y{'_n} = f(t,{y_1},{y_2}, \cdots ,{y_n})
$$
两者放在一起
$$
\eqalign{
  & {y_1}' = {y_2}  \cr 
  & {y_2}' = {y_3}  \cr 
  & {y_3}' = {y_4}  \cr 
  &  \vdots   \cr 
  & {y_{n - 1}}' = {y_n}  \cr 
  & y{'_n} = f(t,{y_1},{y_2}, \cdots ,{y_n}) \cr}
$$
这些方程将n阶的微分方程转化为一阶微分方程组，该方程组可以使用欧拉或者梯形方法求解。



### 龙格-库塔方法和应用

#### 龙格-库塔家族

我们已经知道一阶的欧拉方法以及二阶的梯形方法。除了梯形方法外，还有其他龙格-库塔类型的二阶方法。

**中点方法**

> $w_0=y_0$
>
> ${w_{i + 1}} = {w_i} + hf\left( {{t_i} + {h \over 2},{w_i} + {h \over 2}f({t_i},{w_i})} \right)$

**4阶龙格-库塔方法（RK4）**

> ${w_{i + 1}} = {w_i} + {h \over 6}\left( {{s_1} + 2{s_2} + 2{s_3} + {s_4}} \right)$
>
> 其中
> $$
> \eqalign{
> & {s_1} = f({t_i},{w_i})  \cr 
> & {s_2} = f\left( {{t_i} + {h \over 2},{w_i} + {h \over 2}{s_1}} \right)  \cr 
> & {s_3} = f\left( {{t_i} + {h \over 2},{w_i} + {h \over 2}{s_2}} \right)  \cr 
> & {s_4} = f\left( {{t_i} + h,{w_i} + h{s_3}} \right) \cr}
> $$



### 可变步长方法

直到现在，步长h在ODE求解器实现中一直是一个常数。但是，并没有原因说h在求解过程中不能改变。我们希望改变步长的一个原因是，问题的解会在缓慢变化的周期和迅速变化的周期之间移动。将固定步长变得足够小使其精确跟踪快速变化，意味着解的其他部分求解都会是令人难以忍受的缓慢。

#### 龙格-库塔嵌入对

可变步长方法的关键思想是检测当前步生成的误差。用户设置的容差在当前步必须能够满足．然后设计方法(1)如果超出容差，必须拒绝误差并将步长减小，或者(2)如果满足容差，接受步长并选择对于下一步适合的步长h。关键是近似在每步中产生的误差。首先假设我们已经找到这样的方式并解释如何改变步长。

有RK2/3（龙格-库塔2阶/3阶嵌入对）、RK4/5，不过多赘述。



### 隐式方法和刚性方程

**后向欧拉方法**

> $w_0=y_0$
>
> $w_{i+1}=w_i+hf(t_{i+1},w_{i+1})$

**举例**：对初值问题使用后向欧拉方法
$$
\left\{ \matrix{
  y' = y + 8{y^2} - 9{y^3} \hfill \cr 
  y(0) = 1/2 \hfill \cr 
  t \in [0,3] \hfill \cr}  \right.
$$
使用后向欧拉方法
$$
{w_{i + 1}} = {w_i} + hf({t_{i + 1}},{w_{i + 1}}) = {w_i} + h\left( {{w_{i + 1}} + 8w_{i + 1}^2 - 9w_{i + 1}^3} \right)
$$
令$z=w_{i+1}$，必须求解方程$z=w_i+h(z+8z^2-9z^3)$，或者对于未知的z求解
$$
9h{z^3} - 8h{z^2} + (1 - h)z - {w_i} = 0
$$
可以使用牛顿方法进行估计
$$
{z_{new}} = z - {{9h{z^3} - 8h{z^2} + (1 - h)z - {w_i}} \over {27h{z^2} - 16hz + 1 - h}}
$$
用$z_{new}$替换z并重复进行，对于每个后向欧拉步，使用牛顿方法直到$z_{new}-z$小于容差（比近似微分方程解的误差还要小）。



### 多步方法

我们已经研究过的龙格-库塔方法家族包含单步方法，意味着新一步的$w_{i+1}$可基于微分方程和上一步的$w_i$得到。 多步方法则给出不同的方式：使用除了$w_i$之外的知识帮助下一步的求解。这将带来OD E求解器的阶数和单步方法的阶数相同，但是大量必要的计算会被替换为求解过程中已经计算的值 的插值。

#### 构造多步方法

**Adams-Bashforth两步方法**
$$
{w_{i + 1}} = {w_i} + h\left[ {{3 \over 2}f({t_i},{w_i}) - {1 \over 2}f({t_{i - 1}},{w_{i - 1}})} \right]
$$


#### 显式多步方法

**Adams-Bashforth三步方法（三阶）**
$$
{w_{i + 1}} = {w_i} + {h \over {12}}\left[ {23{f_i} - 16{f_{i - 1}} + 5{f_{i - 2}}} \right]
$$
**Adams-Bashforth四步方法（四阶）**
$$
{w_{i + 1}} = {w_i} + {h \over {24}}\left[ {55{f_i} - 59{f_{i - 1}} + 37{f_{i - 2}} - 9{f_{i - 3}}} \right]
$$

#### 隐式多步方法

**隐式梯形方法（二阶）**
$$
{w_{i + 1}} = {w_i} + {h \over 2}\left[ {{f_{i + 1}} + {f_i}} \right]
$$
**Adams-Moulton两步方法（三阶）**
$$
{w_{i + 1}} = {w_i} + {h \over {12}}\left[ {5{f_{i + 1}} + 8{f_i} - {f_{i - 1}}} \right]
$$
**Milne-Simpson方法** 
$$
{w_{i + 1}} = {w_i} + {h \over 3}\left[ {{f_{i + 1}} + 4{f_i} + {f_{i - 1}}} \right]
$$
**Adams-Moulton三步方法（四阶）**
$$
{w_{i + 1}} = {w_i} + {h \over {24}}\left[ {9{f_{i + 1}} + 19{f_i} - 5{f_{i - 1}} + {f_{i - 2}}} \right]
$$
**Adams-Moulton四步方法（五阶）**
$$
{w_{i + 1}} = {w_i} + {h \over {720}}\left[ {251{f_{i + 1}} + 646{f_i} - 246{f_{i - 1}} + 106{f_{i - 2}} - 19{f_{i - 3}}} \right]
$$



## 边值问题

### 有限差分方法

#### 线性边值问题

令y(t)是一个至少四阶连续的函数，可以得到一阶导数的离散近似为
$$
y'(t) = {{y(t + h) - y(t - h)} \over {2h}} - {{{h^2}} \over 6}y'''(c)
$$
二阶导数的近似为
$$
\eqalign{
  & y'(t) = {{y(t + h) - y(t - h)} \over {2h}} - {{{h^2}} \over 6}y'''(c)  \cr 
  & y''(t) = {{y(t + h) - 2y(t) + y(t - h)} \over {{h^2}}} + {{{h^2}} \over {12}}y''''(c) \cr} 
$$
有限差分方法包含使用离散版本替换微分方程中的导数，并求解得到的更简单的代数方程获取正确值$y_i$的近似$w_i$，边界条件则在需要的时候代入方程组中。

**举例**：使用有限差分求解
$$
\left\{ \matrix{
  y'' = 4y \hfill \cr 
  y(0) = 1 \hfill \cr 
  y(1) = 3 \hfill \cr}  \right.
$$
使用二阶导数的中心差分形式，在$t_i$处的有限差分形式如下
$$
{{{w_{i + 1}} - 2{w_i} + {w_{i - 1}}} \over {{h^2}}} - 4{w_i} = 0
$$
或者等价地
$$
{w_{i - 1}} + ( - 4{h^2} - 2){w_i} + {w_{i + 1}} = 0
$$
当n=3时，区间大小h=1/(n+1)=1/4，再插入边界条件$w_0=1$，$w_4=3$，给出方程
$$
\left\{ \matrix{
  1 + ( - 4{h^2} - 2){w_1} + {w_2} = 0 \hfill \cr 
  {w_1} + ( - 4{h^2} - 2){w_2} + {w_3} = 0 \hfill \cr 
  {w_2} + ( - 4{h^2} - 2){w_3} + 3 = 0 \hfill \cr}  \right.
$$
代入h得到三对角线矩阵方程
$$
\left[ {\matrix{
   { - {9 \over 4}} & 1 & 0  \cr 
   1 & { - {9 \over 4}} & 1  \cr 
   0 & 1 & { - {9 \over 4}}  \cr 

 } } \right]\left[ {\matrix{
   {{w_1}}  \cr 
   {{w_2}}  \cr 
   {{w_3}}  \cr 

 } } \right] = \left[ \matrix{
   - 1 \hfill \cr 
  0 \hfill \cr 
   - 3 \hfill \cr}  \right]
$$

#### 非线性边值问题

对于非线性边值问题，使用多变量牛顿方法进行迭代，迭代公式为
$$
{w^{k + 1}} = {w^k} - DF{({w^k})^{ - 1}}F({w^k})
$$
一般地，完成迭代的最好方式是求解方程$DF({w^k})\Delta w =  - F({w^k})$中的$\Delta w = {w^{k + 1}} - {w^k}$。

**举例**：求解非线性边值问题
$$
\left\{ \matrix{
  y'' = y - {y^2} \hfill \cr 
  y(0) = 1 \hfill \cr 
  y(1) = 4 \hfill \cr}  \right.
$$
离散化后方程为
$$
{w_{i - 1}} - (2 + {h^2}){w_i} + {h^2}w_i^2 + {w_{i + 1}} = 0
$$
其中$w_0=y_a=1$，$w_{n+1}=y_b=4$。

函数F(w)如下
$$
F\left[ \matrix{
  {w_1} \hfill \cr 
  {w_2} \hfill \cr 
   \vdots  \hfill \cr 
  {w_{n - 1}} \hfill \cr 
  {w_n} \hfill \cr}  \right] = \left[ {\matrix{
   \matrix{
  {y_a} - (2 + {h^2}){w_1} + {h^2}w_1^2 + {w_2} \hfill \cr 
  {w_1} - (2 + {h^2}){w_2} + {h^2}w_2^2 + {w_3} \hfill \cr}   \cr 
    \vdots   \cr 
   {{w_{n - 2}} - (2 + {h^2}){w_{n - 1}} + {h^2}w_{n - 1}^2 + {w_n}}  \cr 
   {{w_{n - 1}} - (2 + {h^2}){w_n} + {h^2}w_n^2 + {y_b}}  \cr 

 } } \right]
$$

### 排列与有限元方法

和有限差分方法相似，排列和有限元方法背后的思想是将边值问题消减为一组可求解的代数方程。但是，并不是通过使用有限差分替换微分方程中的导数进行离散化，而是对于解给出函数形式，其对应的参数使用该方法拟合。

选择一组基函数${\phi _1}(t)$，...，${\phi _n}(t)$，它们可能是多项式、三角函数、样条，或者其他简单函数，然后考虑可能的解
$$
y(t) = {c_1}{\phi _1}(t) +  \cdots  + {c_n}{\phi _n}(t)
$$
找出近似解的问题被简化为确定$c_i$的值。



#### 排列

考虑BVP问题
$$
\left\{ \matrix{
  y'' = f(t,y,y') \hfill \cr 
  y(a) = {y_a} \hfill \cr 
  y(b) = {y_b} \hfill \cr}  \right.
$$
选择n个点，从边值点a开始，在b点结束，即
$$
a = {t_1} < {t_2} <  \cdots  < {t_n} = b
$$
排列方法将的候选解代入微分方程中，并估计微分方程在n个点上的值，得到关于n个未知变量$c_1$，...，$c_n$的方程，$t_i$的值为${t_i} = a + {{i - 1} \over {n - 1}}\left( {b - a} \right)$。

选择基函数$\phi_j(t)=t^{j-1}$，其中1≤j≤n。解的形式如下
$$
y(t) = \sum\limits_{j = 1}^n {{c_j}{\phi _j}(t)}  = \sum\limits_{j = 1}^n {{c_j}{t^{j - 1}}}
$$
所以可以写出n个未知变量$c_1$，...，$c_n$的n个方程，其中第一个和最后一个方程是边值条件
$$
\eqalign{
  & \sum\limits_{j = 1}^n {{c_j}{a^{j - 1}}}  = y(a)  \cr 
  & \sum\limits_{j = 1}^n {{c_j}{b^{j - 1}}}  = y(b) \cr}
$$
余下的n-2个方程来自微分方程在$t_i$上的求值，其中2≤i≤n-1。将微分方程y"=f(t,y,y')应用到$y(t)=\sum_{j=1}^nc_jt^{j-1}$得到
$$
\sum\limits_{j = 1}^n {\left( {j - 1} \right)\left( {j - 2} \right){c_j}} {t^{j - 3}} = f\left( {t,\sum\limits_{j = 1}^n {{c_j}} {t^{j - 1}},\sum\limits_{j = 1}^n {\left( {j - 1} \right){c_j}} {t^{j - 2}}} \right)
$$
在$t_i$点求值得到n个方程，然后求解$c_i$。如果微分方程是线性的，则关于$c_i$的方程组也是线性的，并可以求解。

在排列方法中选择三角函数作为基带来了傅里叶分析以及谱方法，这在边值方程和偏微分方程中都大量应用。这是一个“全局的方法，其中对于t的一个非常大的范围中，基函数非0，但是具有很好的正交性质。



#### 有限元以及Galerkin方法

选择样条函数作为基函数带来有限元方法．在这种方法中，每个基函数仅在t的一个小区间上非0。有限元方法大量用于高维情况下的BVP和PDE，特别是当存在不规则的边界条件的情况下，其中使用标准的基函数做参数化变得困难。Galerkin方法最小化微分方程在解上的平方误差。这带来关于$c_i$的一个不同的方程组。

考虑一个BVP问题
$$
\left\{ \matrix{  y'' = f(t,y,y') \hfill \cr   y(a) = {y_a} \hfill \cr   y(b) = {y_b} \hfill \cr}  \right.
$$
解为y，余项r=y"-f，需要使得微分方程两侧的差异尽可能小。和最小二乘方法类似，这可以通过选择y使得余项与潜在解的向量空间正交得到。

对于区间[a,b]，定义平方可积分的函数对应的向量空间
$$
{L^2}[a,b] = \left\{ {区间[a,b]上的函数\quad y(t)|\int_a^b {y{{(t)}^2}} dt存在并有穷} \right\}
$$
$L^2$函数空间具有内积
$$
\left\langle {{y_1},{y_2}} \right\rangle  = \int_a^b {{y_1}(t){y_2}(t)} dt
$$
当$<y_1,y_2>=0$时，两个函数$y_1$和$y_2$在$L^2[a,b]$上**正交**。由于$L^2[a,b]$是无限维的向量空间，通过有限计算，不能得到余项r与所有$L^2[a,b]$正交，但是可以使用已有的计算资源，选择基和$L^2$张得尽可能一致，令n+2个基函数的集合表示为$\phi_0(t)$，...，$\phi_{n+1}(t)$。

Galerkin方法包含两个主要思想，第一个是最小化r，其中强制其在$L^2$内积的意义上与基函数正交，这意味着强制$\int_a^b {\left( {y'' - f} \right){\phi _i}(t)} dt=0$，或
$$
\int_a^b { y'' {\phi _i}(t)} dt = \int_a^b {f(t,y,y'){\phi _i}(t)} dt
$$
其中0≤i≤n+1，该形式被称为边值问题的弱形式。

Galerkin方法的第二个思想是部分使用积分消去二阶导数，注意到
$$
\eqalign{
  & \int_a^b {y''{\phi _i}(t)} dt = \left. {{\phi _i}(t)y'(t)} \right|_a^b - \int_a^b {y'(t){\phi _i}'(t)} dt  \cr 
  &  = {\phi _i}(b)y'(b) - {\phi _i}(a)y'(a) - \int_a^b {y'(t){\phi _i}'(t)} dt \cr} 
$$
可以得到
$$
\int_a^b {f(t,y,y'){\phi _i}(t)} dt = {\phi _i}(b)y'(b) - {\phi _i}(a)y'(a) - \int_a^b {y'(t){\phi _i}'(t)} dt
$$
以如下函数形式求解$c_i$
$$
y(t) = \sum\limits_{i = 0}^{n + 1} {{c_i}{\phi _i}(t)}
$$
Galerkin方法的两个思想使其可以方便地使用极简单的函数作为有限元$\phi_i(t)$，我们将仅仅介绍分段线性B样条函数，从t轴上的数据格点${t_0} < {t_1} <  \cdots < {t_{n-1}}  < {t_n}$开始，对于i=1，...，n定义
$$
{\phi _i}(t) = \left\{ \matrix{
  {{t - {t_{i - 1}}} \over {{t_i} - {t_{i - 1}}}}\quad {t_{i - 1}} < t \le {t_i} \hfill \cr 
  {{{t_{i + 1}} - t} \over {{t_{i + 1}} - {t_i}}}\quad {t_i} < t \le {t_{i + 1}} \hfill \cr 
  0\quad \quad \quad \;otherwise \hfill \cr}  \right.
$$
同时定义
$$
{\phi _0}(t) = \left\{ \matrix{
  {{{t_1} - t} \over {{t_1} - {t_0}}}\quad {t_0} \le t < {t_1} \hfill \cr 
  0\quad \quad \quad \;otherwise \hfill \cr}  \right.\quad {\phi _{n + 1}}(t) = \left\{ \matrix{
  {{t - {t_n}} \over {{t_{n + 1}} - {t_n}}}\quad {t_n} < t \le {t_{n + 1}} \hfill \cr 
  0\quad \quad \quad \;otherwise \hfill \cr}  \right.
$$
对于$\phi_i(t_j)$而言，只有当i=j时，才为1，否则为0。对于一组数据点$(t_i,c_i)$，定义**分段线性B样条**
$$
S(t) = \sum\limits_{i = 0}^{n + 1} {{c_i}{\phi _i}(t)}
$$
同时有$S(t) = \sum\limits_{i = 0}^{n + 1} {{c_i}{\phi _i}(t_j)}=c_j$。

第一个和最后一个$c_i$通过排列方法得到
$$
\eqalign{
  & y(a) = \sum\limits_{i = 0}^{n + 1} {{c_i}{\phi _i}(a)}  = {c_0}{\phi _0}(a) = {c_0}  \cr 
  & y(b) = \sum\limits_{i = 0}^{n + 1} {{c_i}{\phi _i}(b)}  = {c_{n + 1}}{\phi _{n + 1}}(b) = {c_{n + 1}} \cr} 
$$
对于i=1，...，n，使用有限元方程
$$
\int_a^b {f(t,y,y'){\phi _i}(t)} dt + \int_a^b {y'(t){\phi _i}'(t)} dt = 0
$$
代入$y(t)=\sum c_i\phi_i(t)$，利用$\phi$函数的性质可以求解。



## 偏微分方程

#### 前向差分方法

定义偏微分方程（PDE）如下
$$
\left\{ \matrix{
  {u_t} = D{u_{xx}}\quad a \le x \le b,t \ge 0 \hfill \cr 
  u(x,0) = f(x)\quad a \le x \le b \hfill \cr 
  u(a,t) = f(t)\quad t \ge 0 \hfill \cr 
  u(b,t) = r(t)\quad t \ge 0 \hfill \cr}  \right.
$$


使用数值微分和积分的离散公式近似在x和t方向的导数。例如，使用中心差分公式，关于x的二阶导数如下
$$
{u_{xx}}(x,t) \approx {1 \over {{h^2}}}\left( {u(x + h,t) - 2u(x,t) + u(x - h,t)} \right)
$$
误差为${h^2}{u_{xxxx}}({c_1},t)/12$，对于时间变量的一阶导数的前向差分公式得到
$$
{u_t}(x,t) = {1 \over k}\left( {u(x,t + k) - u(x,t)} \right)
$$
其中误差$ku_{tt}(x,c_2)/2$，$x-h<c_1<x+h$，$t<c_2<t+h$。带入点$(x_i,t_j)$处的偏微分方程中得到
$$
{D \over {{h^2}}}\left( {{w_{i + 1,j}} - 2{w_{ij}} + {w_{i - 1,j}}} \right) \approx {1 \over k}\left( {{w_{i,j + 1}} - {w_{ij}}} \right)
$$
通过时间步进可以得到（定义$\sigma={DK \over h^2}$）
$$
\eqalign{
  & {w_{i,j + 1}} = {w_{ij}} + {{DK} \over {{h^2}}}\left( {{w_{i + 1,j}} - 2{w_{ij}} + {w_{i - 1,j}}} \right)  \cr 
  &  = \sigma {w_{i + 1,j}} + (1 - 2\sigma ){w_{ij}} + \sigma {w_{i - 1,j}} \cr} 
$$
写成矩阵形式
$$
\left[ {\matrix{
   {{w_{1.j + 1}}}  \cr 
    \vdots   \cr 
   {{w_{m,j + 1}}}  \cr 

 } } \right] = \left[ {\matrix{
   {1 - 2\sigma } & \sigma  & 0 &  \cdots  & 0  \cr 
   \sigma  & {1 - 2\sigma } & \sigma  &  \ddots  & 0  \cr 
   0 & \sigma  & {1 - 2\sigma } &  \ddots  & 0  \cr 
    \vdots  &  \ddots  &  \ddots  &  \ddots  & \sigma   \cr 
   0 &  \cdots  & 0 & \sigma  & {1 - 2\sigma }  \cr 

 } } \right]\left[ {\matrix{
   {{w_{1.j}}}  \cr 
    \vdots   \cr 
   {{w_{m,j}}}  \cr 

 } } \right] + \sigma \left[ {\matrix{
   {{w_{0,j}}}  \cr 
   0  \cr 
    \vdots   \cr 
   \matrix{
  0 \hfill \cr 
  {w_{m + 1,j}} \hfill \cr}   \cr 

 } } \right]
$$

#### 后向差分方法

此时离散的差分公式为（定义$\sigma={DK \over h^2}$）
$$
\eqalign{
  & {1 \over k}\left( {{w_{ij}} - {w_{i,j - 1}}} \right) = {D \over {{h^2}}}\left( {{w_{i + 1,j}} - 2{w_{ij}} + {w_{i - 1,j}}} \right)  \cr 
  &  - \sigma {w_{i + 1,j}} + (1 + 2\sigma ){w_{ij}} - \sigma {w_{i - 1,j}} = {w_{i,j - 1}} \cr} 
$$
矩阵形式为
$$
\left[ {\matrix{
   {1 + 2\sigma } & { - \sigma } & 0 &  \cdots  & 0  \cr 
   { - \sigma } & {1 + 2\sigma } & { - \sigma } &  \ddots  & 0  \cr 
   0 & { - \sigma } & {1 + 2\sigma } &  \ddots  & 0  \cr 
    \vdots  &  \ddots  &  \ddots  &  \ddots  & { - \sigma }  \cr 
   0 &  \cdots  & 0 & { - \sigma } & {1 + 2\sigma }  \cr 

 } } \right]\left[ {\matrix{
   {{w_{1.j}}}  \cr 
    \vdots   \cr 
   {{w_{m,j}}}  \cr 

 } } \right] = \left[ {\matrix{
   {{w_{1.j - 1}}}  \cr 
    \vdots   \cr 
   {{w_{m,j - 1}}}  \cr 

 } } \right] + \sigma \left[ {\matrix{
   {{w_{0,j}}}  \cr 
   0  \cr 
    \vdots   \cr 
   \matrix{
  0 \hfill \cr 
  {w_{m + 1,j}} \hfill \cr}   \cr 

 } } \right]
$$


#### Crank-Nicolson方法

Crank-Nicolson对于时间导数使用后向差分公式，并均匀组合前向差分近似和后向差分近似。

使用如下的后向差分公式替换$u_t$
$$
{1 \over k}\left( {{w_{ij}} - {w_{i,j - 1}}} \right)
$$
使用混合差分替换$u_{xx}$
$$
{1 \over 2}\left( {{{{w_{i + 1,j}} - 2{w_{ij}} + {w_{i - 1,j}}} \over {{h^2}}}} \right) + {1 \over 2}\left( {{{{w_{i + 1,j - 1}} - 2{w_{i,j - 1}} + {w_{i - 1,j - 1}}} \over {{h^2}}}} \right)
$$
可以重新组织热方程近似
$$
2{w_{ij}} - 2{w_{i,j - 1}} = \sigma \left[ {{w_{i + 1,j}} - 2{w_{ij}} + {w_{i - 1,j}} + {w_{i + 1,j - 1}} - 2{w_{i,j - 1}} + {w_{i - 1,j - 1}}} \right]
$$
或者
$$
 - \sigma {w_{i - 1,j}} + (2 + 2\sigma ){w_{ij}} - \sigma {w_{i + 1,j}} = \sigma {w_{i - 1,j - 1}} + (2 - 2\sigma ){w_{i,j - 1}} + \sigma {w_{i + 1,j - 1}}
$$




**本章中有各种偏微分方程（抛物线方程，双曲线方程，椭圆方程，非线性偏微分方程）的解法。**



## 随机数和应用

### 随机数

#### 伪随机数

**定义**：线性同余生成器（LCG）表示为以下形式（$u_i$为生成的[0,1]随机数）
$$
x_i = ax_{i-1}+b(\bmod m)\\
u_i={x_i \over m}
$$
其中a为乘子，b为偏移，m为模数。

**最小标准随机数生成器**

> $x_i=ax_{i-1}(\bmod m)$
>
> $u_i={x_i\over m}$
>
> 其中$m=2^{31}-1$，$a=7^5=16807$，$b=0$

按照现在计算机每秒指令数的增加，这个数目显得太少了。

**randu生成器**

>$x_i=ax_{i-1}(\bmod m)$
>
>$u_i={x_i\over m}$
>
>其中$a=65539=2^{16}+3$，$m=2^{31}$

不满足随机数的独立性要求。

最新版的matlab已经不再使用LCG作为随机数生成器，从matlab5开始，使用的是延迟斐波那契生成器。



#### 指数和正态随机数

指数随机变量的取值满足概率分布函数$p(x)=ae^{-ax},a>0$，使用均匀分布随机数，可以很容易地生成指数随机数，累计分布函数为
$$
P(x) = Prob(V \le x) = \int_0^x {p(x)} dx = 1 - {e^{ - ax}}
$$
算法的核心思想是要找到指数随机变量的值，使得概率Prob(V≤x)是[0,1]区间上的均匀分布，即给定一个满足均匀分布的变量u之后，要使得
$$
u=Prob(V\le x)=1-e^{-ax}
$$
求解x得到
$$
x = {{ - \ln (1 - u)} \over a}
$$
把输入的均匀随机数u转换为指数随机数。

对于一般的概率分布，也可以用这种方法处理。假设P(x)是需要生成的随机变量对应的概率分布函数。$Q(x)=P^{-1}(x)$是对应的反函数。若U[0,1]是[0,1]区间上的均匀分布的随机数，则Q(U[0,1])就是满足分布P的随机数，剩下的问题就是如何高效地计算函数Q。

标准正态分布也可以采用反函数的方式计算，不过还有一种更有效的方法，可以同时生成两个正态分布随机数。二维标准正态分布的概率密度函数为$p(x,y)=(1/2\pi)e^{-(x^2+y^2)/2}$，在极坐标系下又可写成$p(r)=(1/2\pi)e^{-r^2/2}$的形式。由于p(r)是对极坐标轴对称的，我们只需要考虑半径r，而角度θ可以用[0,2π]区间上的均匀分布。由于p(r)是对于变量$r^2$的指数分布，其中参数a=1/2，可以得到$r^2$
$$
r^2={-\ln(1-u_1)\over {1/2}}
$$
其中$u_1$是均匀分布随机数，进而生成两个满足标准正态分布且完全独立的随机数
$$
\eqalign{
  & {n_1} = r\cos 2\pi {u_2} = \sqrt { - 2\ln (1 - {u_1})} \cos 2\pi {u_2}  \cr 
  & {n_2} = r\sin 2\pi {u_2} = \sqrt { - 2\ln (1 - {u_1})} \sin 2\pi {u_2} \cr} 
$$
这种算法称为Box-Muller方法，此外还有一种更加高效的方法
$$
{n_1} = {x_1}\sqrt {{{ - 2\ln ({u_1})} \over {{u_1}}}} \quad {n_2} = {x_2}\sqrt {{{ - 2\ln ({u_1})} \over {{u_1}}}} 
$$
该方法可以避免使用三角函数，但存在一定的拒绝率（21%左右），如果$x_1^2+x_2^2<1$就接受这个值，反之重新选择。



### 蒙特卡洛模拟

#### 幂律和蒙特卡洛模拟

Ⅰ型蒙特卡罗问题是用随机采样点计算方程的均值，再乘以积分区间的体积。计算函数均值可以看成是计算一个符合同样分布函数的概率的均值。我们用E(X)表示随机变量X的期望随机变量X的方差为$E[(E-E(X))^2]$，而**X标准差**为方差的平方根。在测量过程中，误差的期望会随着n的增加而下降，并且符合如下的规律：

**Ⅰ型或Ⅱ型蒙特卡罗模拟具有伪随机数字。**
$$
Error \propto {n^{ - {1 \over 2}}}
$$

#### 拟随机数

拟随机数的理念是在条件允许的情况下，放弃对随机数独立性的要求。放弃独立性意味着拟随机数不再是随机的，而且也不是像伪随机数那样看上去是随机的。通过这种方式，可以使得蒙特卡罗方法能很快地收敛到正确解。拟随机数的设计是要使得生成的序列是自避(Self-avoiding)的，而不一定要保证独立性。自避定义为，在生成随机数序列的过程中，新的数会填充到比较稀疏的区域，而不会聚集到一起。

**Halton序列**：设p为一个素数，例如p=2。按p进制表示写出1~n的自然数，假设第i个数字的p进制表示为$b_kb_{k-1}\cdots b_2b_1$，则我们要生成的随机数就是$0.b_1b_2\cdots b_{k-1}b_k$。换句话说，整个生成过程就是先写出第i个整数，然后把位数反过来写，再放到小数点的另一侧，最后输出[0,1]区间上均匀分布的第i个随机数。

如p=3生成的前8个随机数

| i    | $(i)_3$ | $(u_i)_3$ | $u_i$      | i    | $(i)_3$ | $(u_i)_3$ | $u_i$      |
| ---- | ------- | --------- | ---------- | ---- | ------- | --------- | ---------- |
| 1    | 1       | 0.1       | $0.\bar 3$ | 5    | 12      | 0.21      | $0.\bar 7$ |
| 2    | 2       | 0.2       | $0.\bar 6$ | 6    | 20      | 0.02      | $0.\bar 2$ |
| 3    | 10      | 0.01      | $0.\bar 1$ | 7    | 21      | 0.12      | $0.\bar 5$ |
| 4    | 11      | 0.11      | $0.\bar 4$ | 8    | 22      | 0.22      | $0.\bar 8$ |

拟随机数的优势在于它的收敛速度更快。如果把估计误差写成计算次数n的函数的话，使用拟随机数会以更高阶的速度收敛。以下是采用拟随机数的误差收敛函数，d表示将要生成的随机数的维度：

**1型蒙特卡洛问题（拟随机数）**
$$
Error \propto {\left( {\ln n} \right)^d}{n^{ - 1}}
$$
**2型蒙特卡洛问题（拟随机数）**
$$
Error \propto {n^{ - {1 \over 2} - {1 \over {2d}}}}
$$


### 离散和连续布朗运动

#### 随机游走

随机游走模型，又称离散布朗运动。随机游走$W_t$定义在实数轴上，起始位置$W_0=0$，在每个整数时刻i，都会移动一个距离$s_i$，其中$s_i$是独立同分布的随机变量。这里，我们假设每个$s_i$只能是+1或-1，且概率均为1/2。离散布朗运动定位为按如下公式生成的随机游走序列
$$
W_t=W_0+s_1+s_2+\cdots+s_t
$$
其中t=0，1，2，...。

每个时刻t，$W_t$是一个随机变量，把这些随机变量连起来得到的{$W_0$，$W_1$，$W_2$，...}就称为随机过程，$W_t=s_0+s_1+\cdots+s_t$的期望为0，方差为t。

很多随机游走模型的应用都是关于逃逸时间，又称为首次到达时间。设a，b为正整数，一个初始位置为0的随机游走序列，首次达到[-b,a]区间边缘的时刻，就称为逃逸时间。理论证明在a处（而不是-b处）逃逸的概率为b/(a+b)。对于从区间[-b,a]逃逸所需的时间，其期望值为ab。

#### 连续布朗运动

标准随机游走过程在时刻t的期望为0，方差为t。假设在每个单位时间内都可以进行两次位移，且每次位移的时间为1/2个单位时间，则在时刻t的期望仍然会是0，但是方差会变为
$$
V({W_t}) = V({s_1} +  \cdots  + {s_{2t}}) = 2t
$$
这是因为一共进行了2t次位移操作。如果我们把移动次数增加到原来的k倍，我们就必须把每次移动的距离缩小到原来的$1/\sqrt k$，这样才能保证方差不变。

因此，$W_t^k$表示的随机游走过程，其在水平方向上的步长为原来的1/k，在垂直方向上的步长为±1/$\sqrt k$的等概率分布，则期望仍然为0，方差为
$$
V({W_t}^k) = \sum\limits_{i = 1}^{kt} {V(s_i^k)}  = \sum\limits_{i = 1}^{kt} {\left[ {{1 \over 2}{{\left( {{1 \over {\sqrt k }}} \right)}^2} + {1 \over 2}{{\left( { - {1 \over {\sqrt k }}} \right)}^2}} \right]}  = kt{1 \over k} = t
$$
当k趋向∞，极限$W_t^{\infty}$就成为了连续布朗运动，$B_t=W_t^{\infty}$是在t≥0上的随机变量，有三个重要特性

（1）对任意t，随机变量$B_t$的均值为0，方差为t；

（2）对任意$t_1<t_2$，随机变量$B_{t_2}-B_{t_1}$是正态分布，且独立于$B_{t_1}$的取值，实际上独立于所有$B_s$，0≤s≤$t_1$；

（3）布朗运动$B_t$是一条连续路径。



### 随机微分方程

#### 有噪声的微分方程

**定义**：以实数t≥0为索引的随机变量集合$x_t$称为**时域连续随机过程**。

考虑如下的有噪声的微分方程（SDE）问题：
$$
\left\{ \matrix{
  dy = rdt + \sigma d{B_t} \hfill \cr 
  y(0) = 0 \hfill \cr}  \right.
$$
其中r和σ为常数，这个概率过程为$y(t)=rt+\sigma B_t$。

SDE是以微分的形式给出的，而在ODE中使用的是导数形式。这是因为像包括布朗运动在内的很多随机过程，它们是连续但不可微的。因此，SDE
$$
dy=f(t,y)dt+g(t,y)dB_t
$$
等价于积分形式的表示
$$
y(t) = y(0) + \int_0^t {f(s,y)} ds + \int_0^t {g(s,y)} d{B_s}
$$
公式中第二个积分项称为Ito积分。

设$a=t_0<t_1<\cdots<t_{n-1}<t_n=b$是在区间[a,b]上的网格点，黎曼积分定义为如下的极限
$$
\int_a^b {f(t)} dt = \mathop {\lim }\limits_{\Delta t \to 0} \sum\limits_{i = 1}^n {f({t_i}')\Delta {t_i}}
$$
其中$\Delta {t_i} = {t_i} - {t_{i - 1}},{t_{i - 1}} \le {t_i}' \le {t_i}$。类似地，Ito积分定义为如下的极限：
$$
\int_a^b {f(t)} d{B_t} = \mathop {\lim }\limits_{\Delta t \to 0} \sum\limits_{i = 1}^n {f({t_{i - 1}})\Delta {B_i}} 
$$
其中$\Delta {B_i} = {B_{{t_i}}} - {B_{{t_{i - 1}}}}$，是布朗运动中单次位移的距离。

布朗运动$B_t$的微分$dB_t$称为白噪声。

**Ito公式**

> 若y=f(t,x)，则
> $$
> dy = {{\partial f} \over {\partial t}}(t,x)dt + {{\partial f} \over {\partial x}}(t,x)dx + {1 \over 2}{{{\partial ^2}f} \over {\partial {x^2}}}(t,x)dxdx
> $$

有许多数值方法用于求解SDE，有Euier-Maruyania方法，Milstein方法，一阶随机龙格-库塔方法，不过多赘述。





## 三角插值和FFT

DFT和快速傅里叶变换的内容不过多赘述。

### 三角插值

#### DFT插值定理

设[c,d]为参数区间，n为正整数。定义$\Delta t=(d-c)/n$，$t_j=c+j\Delta t$，其中j=0，...，n-1。这是在区间[c,d]上均匀分布的采样点，向量x为傅里叶变换的输入，$x_j$可以理解为一个信号的第j
个分量。例如，我们可以把x想象成为一系列的测量，每个$x_j$都测量一个离散的采样点$t_j$。

设y=$F_n$x是x的傅里叶变换，由于x是y的逆傅里叶变换，x可以表示为
$$
\eqalign{
  & {x_j} = {1 \over {\sqrt n }}\sum\limits_{k = 0}^{n - 1} {{y_k}{{\left( {{w^{ - k}}} \right)}^j}}  = {1 \over {\sqrt n }}\sum\limits_{k = 0}^{n - 1} {{y_k}{e^{i2\pi kj/n}}}   \cr 
  &  = \sum\limits_{k = 0}^{n - 1} {{y_k}{{{e^{i2\pi kj/n}}} \over {\sqrt n }}}  = \sum\limits_{k = 0}^{n - 1} {{y_k}{{{e^{{{i2\pi k({t_j} - c)} \over {d - c}}}}} \over {\sqrt n }}}  \cr}
$$
以上公式可以看成是对采样点$(t_j,x_j)$的插值，其中使用的是三角函数作为基函数，而插值系数为$y_k$。而$y_k=F_nx$，所以可以说傅里叶变换$F_n$可以将数据$\{x_j\}$变换为插值系数。

假设$y_k=a_k+ib_k$，可以将插值函数写成
$$
Q(t) = {1 \over {\sqrt n }}\sum\limits_{k = 0}^{n - 1} {({a_k} + i{b_k})\left( {\cos {{2\pi k(t - c)} \over {d - c}} + i\sin {{2\pi k(t - c)} \over {d - c}}} \right)}
$$
把插值函数Q(t)=P(t)+I(t)分成实部和虚部两个部分，由于$x_j$是实数，只需用到实数部分就可以插值得到$x_j$，实部为
$$
P(t) = {P_n}(t) = {1 \over {\sqrt n }}\sum\limits_{k = 0}^{n - 1} {\left( {{a_k}\cos {{2\pi k(t - c)} \over {d - c}} - {b_k}\sin {{2\pi k(t - c)} \over {d - c}}} \right)} 
$$
下标n表示三角插值公式中的项数，有时可以把$P_n$称为n阶三角函数。

当n为偶数时，有
$$
{P_n}(t) = {{{a_0}} \over {\sqrt n }} + {2 \over {\sqrt n }}\sum\limits_{k = 1}^{n/2 - 1} {\left( {{a_k}\cos {{2\pi k(t - c)} \over {d - c}} - {b_k}\sin {{2\pi k(t - c)} \over {d - c}}} \right)}  + {{{a_{n/2}}} \over {\sqrt n }}\cos {{n\pi (t - c)} \over {d - c}}
$$
满足$P_n(t_j)=x_j$，j=0，...，n-1。

当n为奇数时，有
$$
{P_n}(t) = {{{a_0}} \over {\sqrt n }} + {2 \over {\sqrt n }}\sum\limits_{k = 1}^{(n - 1)/2} {\left( {{a_k}\cos {{2\pi k(t - c)} \over {d - c}} - {b_k}\sin {{2\pi k(t - c)} \over {d - c}}} \right)} 
$$


### FFT和信号处理

使用三角函数计算最小二乘近似问题

#### 正交性和插值

因为$F_n^{-1}=\bar{F_n}^T=\bar{F_n}$（$\bar F$表示共轭），即$F_n$是酉阵，可以给出一种特殊形式的正交阵，可以用来进行插值计算。

**定理**：令$f_0(t)$，...，$f_{n-1}(t)$为关于时间t的函数，$t_0$，...，$t_{n-1}$为实数。设n×n矩阵
$$
A = \left[ {\matrix{
   {{f_0}({t_0})} & {{f_0}({t_1})} &  \cdots  & {{f_0}({t_{n - 1}})}  \cr 
   {{f_1}({t_0})} & {{f_1}({t_1})} &  \cdots  & {{f_1}({t_{n - 1}})}  \cr 
    \vdots  &  \vdots  & \; &  \vdots   \cr 
   {{f_{n - 1}}({t_0})} & {{f_{n - 1}}({t_1})} &  \cdots  & {{f_{n - 1}}({t_{n - 1}})}  \cr 

 } } \right]
$$
为实数正交阵，对y=Ax，以下函数
$$
F(t)=\sum_{k=0}^{n-1}y_kf_k(t)
$$
是经过采样点$(t_0,x_0)$，...，$(t_{n-1},x_{n-1})$的插值函数，即$F(t_j)=x_j$，j=0，...，n-1。

令y=Ax，可以直接给出插值函数
$$
\eqalign{
  & F(t) = {1 \over {\sqrt n }}{y_0} + \sqrt {{2 \over n}} {y_1}\cos {{2\pi (t - c)} \over {d - c}} + \sqrt {{2 \over n}} {y_2}\sin {{2\pi (t - c)} \over {d - c}}  \cr 
  &  + \sqrt {{2 \over n}} {y_3}\cos {{4\pi (t - c)} \over {d - c}} + \sqrt {{2 \over n}} {y_4}\sin {{4\pi (t - c)} \over {d - c}} +  \cdots  + \sqrt {{1 \over n}} {y_{n - 1}}\cos {{n\pi (t - c)} \over {d - c}} \cr} 
$$


#### 用三角函数进行最小二乘拟合

DFT方法可以对区间[0,1]上的均匀采样点进行三角插值，假设n为偶数
$$
{P_n}(t) = {{{a_0}} \over {\sqrt n }} + {2 \over {\sqrt n }}\sum\limits_{k = 1}^{n/2 - 1} {\left( {{a_k}\cos 2\pi kt - {b_k}\sin 2\pi kt} \right)}  + {{{a_{n/2}}} \over {\sqrt n }}\cos n\pi t
$$
公式中的项数为n，等于采样点的数目。采样点越多，需要用到的sin和cos函数项就越多。

当样本点数目 n 很大时，一般不会去精确地拟合模型函数．
实际上，在一般应用中，需要丢失一些信息（有损压缩）而使得模型比较简单；此外，由于采样点本身就是不准确的，因此强制要求插值函数经过所有点是不恰当的。对于这两种情况，可以使用上式进行最小二乘拟合，由于在这个模型中$a_k$和$b_k$都是线性的，所以可以用法线方程求解。

令n表示样本点$x_j$的数目，$x_j$是[0,1]区间上的均匀时刻$t_j=j/n$的采样点。我们要引入正偶数m， 表示在最小二乘拟合中要用到的基函数个数，即要拟合到前m个基函数，$f_0(t)$，...，$f_{m-1}(t)$，拟合的结果如下式
$$
{P_m}(t) = \sum\limits_{k = 0}^{m - 1} {{c_k}{f_k}(t)}
$$
当m<n时，就变成了一个压缩问题，我们希望用$P_m$在最小平方误差的标准下拟合采样点。

最小二乘问题需要找到系数$c_0$，...，$c_{m-1}$，使得方程
$$
\sum\limits_{k = 0}^{m - 1} {{c_k}{f_k}({t_j})}  = {x_j}
$$
的误差尽可能小，用矩阵形式表示，为
$$
A_m^Tc=x
$$
其中$A_m$是矩阵A的前m行，假设$A_m^T$的各列是相互正交的，对于变量c的法线方程
$$
{A_m}A_m^Tc = {A_m}x
$$
${A_m}A_m^T$是单位阵。因此，最小二乘的解为
$$
c=A_mx
$$
n个点的插值多项式可以表示为
$$
{F_n}(t) = \sum\limits_{k = 0}^{n - 1} {{y_k}{f_k}(t)}
$$
则解可以表示为
$$
{F_m}(t) = \sum\limits_{k = 0}^{m - 1} {{y_k}{f_k}(t)}
$$
这个结论说明，对给定的n个数据点，用m<n个三角函数进行最小二乘拟合，只需计算n阶插值，再取前m个项即可。即x的插值系数Ax，在丢弃高频分量后，仍然是x的最佳拟合函数。n阶展开里的前m项，就是使用m个低阶频率可以达到的最佳拟合。这个性质反映了基函数的“正交性”。

**推论**：令[c,d]为一区间，m<n为正偶数，x=($x_0$，...，$x_{n-1}$)为n维实数向量，$t_j=c+j(d-c)/n$，j=0，...，n-1。令{$a_0$，$a_1$，$b_1$，$a_2$，$b_2$，...，$a_{n/2-1}$，$b_{n/2-1}$，$a_{n/2}$}=$F_nx$为x的插值系数，且
$$
{P_n}(t) = {{{a_0}} \over {\sqrt n }} + {2 \over {\sqrt n }}\sum\limits_{k = 1}^{n/2 - 1} {\left( {{a_k}\cos {{2\pi k(t - c)} \over {d - c}} - {b_k}\sin {{2\pi k(t - c)} \over {d - c}}} \right)}  + {{{a_{n/2}}} \over {\sqrt n }}\cos {{n\pi (t - c)} \over {d - c}}
$$
其中j=0，...，n-1，则有（注意$P_m$中$a_{m/2}$的那一项中有2，而$P_n$中这一项没有2，这与原书423页的引理10.10有关）
$$
{P_m}(t) = {{{a_0}} \over {\sqrt n }} + {2 \over {\sqrt n }}\sum\limits_{k = 1}^{m/2 - 1} {\left( {{a_k}\cos {{2\pi k(t - c)} \over {d - c}} - {b_k}\sin {{2\pi k(t - c)} \over {d - c}}} \right)}  + {{2{a_{m/2}}} \over {\sqrt n }}\cos {{n\pi (t - c)} \over {d - c}}
$$
是数据$(t_j,x_j)$，j=0，...，n-1的m阶最优最小二乘拟合。



#### 声音、噪声和滤波

滤波的用法有两种。第一种是用另一个简单的函数尽可能地拟合原始音频．这是信号的压缩，我们可以只储存m个低频分量。滤波的另一个重要应用是去噪，在一个音乐文件里，音乐和语音中可能会混有高频噪声，需要消除这些高频分量来提高音频的质量。



### 维纳滤波

设c为一个没有噪声的音频信号，与一个同样长度的向量r相加。得到的结果x=c+r是否是有噪声的 ？若r=c，我们认为r不是噪声，因为相加的结果只是增大c的音量，但信号是同样干净的。根据定
义，噪声是与信号无关的，即若r为噪声，则内积$c^Tr$的期望应为0。

在一个典型应用中，我们需要从含噪声的信号x中恢复出c。信号c可能是一个重要的系统变量，但是在噪声环境下进行测量。或者如下面例子，c可能是音频采样，我们想从中去除噪声。在20世纪中期Norbert Wiener建议，以最小平方误差为目标，寻找一种最优滤波器来去掉x中的噪声。他建议寻找一个对角阵Φ，使得公式
$$
{F^{ - 1}}\Phi Fx - c
$$
的模尽可能小，其中F表示DFT变换。它的核心思想是通过傅里叶变换后，在频域进行乘以Φ，再进行逆傅里叶变换。这又称为频域滤波，因为我们处理的是经过傅里叶变换的信号x而不是x本身。为了找到最佳的对角阵Φ，注意到
$$
\eqalign{
  & {\left\| {{F^{ - 1}}\Phi Fx - c} \right\|_2} = {\left\| {\Phi Fx - Fc} \right\|_2} = {\left\| {\Phi F(c + r) - Fc} \right\|_2}  \cr 
  &  = {\left\| {\left( {\Phi  - I} \right)C + \Phi R} \right\|_2} \cr} 
$$
令C=Fc和R=Fr为傅里叶变换，同时注意到根据噪声的定义
$$
{{\bar C}^T}R = {\overline {Fc} ^T}Fr = {c^T}{{\bar F}^T}Fr = {c^T}r = 0
$$
因此模可以写成
$$
\eqalign{
  & {\left\| {\left( {\Phi  - I} \right)C + \Phi R} \right\|_2} = {\left( {\overline {\left( {\Phi  - I} \right)C + \Phi R} } \right)^T}\left( {\left( {\Phi  - I} \right)C + \Phi R} \right)  \cr 
  &  = \left( {{{\bar C}^T}\left( {\Phi  - I} \right) + {{\bar R}^T}\Phi } \right)\left( {\left( {\Phi  - I} \right)C + \Phi R} \right)  \cr 
  &  \approx {{\bar C}^T}{\left( {\Phi  - I} \right)^2}C + {{\bar R}^T}{\Phi ^2}R  \cr 
  &  = \sum\limits_{i = 1}^n {{{\left( {{\phi _i} - 1} \right)}^2}{{\left| {{C_i}} \right|}^2} + {\phi _i}^2{{\left| {{R_i}} \right|}^2}}  \cr} 
$$
为了确定对角元素$\phi_i$能使上式最小，对每个$\phi_i$求导，得到
$$
2\left( {{\phi _i} - 1} \right){\left| {{C_i}} \right|^2} + 2{\phi _i}{\left| {{R_i}} \right|^2} = 0
$$
对每个i，解得$\phi_i$
$$
{\phi _i} = {{{{\left| {{C_i}} \right|}^2}} \over {{{\left| {{C_i}} \right|}^2} + {{\left| {{R_i}} \right|}^2}}}
$$
但一般情况下我们不知道C或R，所以需要某种近似方法，令X=Fx为傅里叶变换，沿用关于信号与噪声的独立性假设，近似地有
$$
{\left| {{X_i}} \right|^2} \approx {\left| {{C_i}} \right|^2} + {\left| {{R_i}} \right|^2}
$$
可以把最优参数选为
$$
{\phi _i} \approx {{{{\left| {{X_i}} \right|}^2} - {{\left| {{R_i}} \right|}^2}} \over {{{\left| {{X_i}} \right|}^2}}}
$$
这里需要用到对噪声等级的先验知识，例如，如果噪声是无关的高斯噪声（独立于信号的正态高斯分布随机数，与信号值相加），可以把$|R_i|^2$项替换为常数$(p\sigma)^2$，其中$\sigma$为噪声的标准差，p为可调节的接近1的参数。



## 压缩

### 离散余弦变换

#### 一维DCT

设n为正整数，一维的n阶DCT变换定义为n×n矩阵C，其元素为
$$
{C_{ij}} = {{\sqrt 2 } \over {\sqrt n }}{a_i}\cos {{i(2j + 1)\pi } \over {2n}}
$$
其中i，j=0，...，n-1，
$$
{a_i} = \left\{ \matrix{
  1/\sqrt 2 \quad if\;i = 0 \hfill \cr 
  1\quad \quad \;\;\;if\;i = 1, \cdots ,n - 1 \hfill \cr}  \right.
$$
**定义**：设C为如下矩阵，向量x=$[x_0,...,x_{n-1}]^T$的离散余弦变换（DCT）是n维向量$y=[y_0,...,y_{n-1}]^T$，其中
$$
y=Cx
$$

$$
C = \sqrt {{2 \over n}} \left[ {\matrix{
   {{1 \over {\sqrt 2 }}} & {{1 \over {\sqrt 2 }}} &  \cdots  & {{1 \over {\sqrt 2 }}}  \cr 
   {\cos {\pi  \over {2n}}} & {\cos {{3\pi } \over {2n}}} &  \cdots  & {\cos {{(2n - 1)\pi } \over {2n}}}  \cr 
   {\cos {{2\pi } \over {2n}}} & {\cos {{6\pi } \over {2n}}} &  \cdots  & {\cos {{2(2n - 1)\pi } \over {2n}}}  \cr 
    \vdots  &  \vdots  & \; &  \vdots   \cr 
   {\cos {{(n - 1)\pi } \over {2n}}} & {\cos {{(n - 1)3\pi } \over {2n}}} &  \cdots  & {\cos {{(n - 1)(2n - 1)\pi } \over {2n}}}  \cr 

 } } \right]
$$

注意到C是实数正交矩阵，即转置与逆矩阵相同。



**定理（DCT插值定理）**：令$x=[x_0,...,x_{n-1}]^T$为n维实数向量，定义$y=[y_0,...,y_{n-1}]^T=Cx$，其中C是n阶DCT变换矩阵，则实数函数
$$
{P_n}(t) = {1 \over {\sqrt n }}{y_0} + \sqrt {{2 \over n}} \sum\limits_{k = 1}^{n - 1} {{y_k}\cos {{k(2t + 1)\pi } \over {2n}}} 
$$
满足$P_n(j)=x_j$，j=0，...，n-1。



**举例**：用DCT变换计算以下点的插值，(0,1)，(1,0)，(2,-1)，(3,0)。

先计算4×4的C矩阵，x=$[1,0,-1,0]^T$，计算Cx得到y即为插值函数的系数，计算得到的y为
$$
y = \left[ \matrix{
  0.0000 \hfill \cr 
  0.9239 \hfill \cr 
  1.0000 \hfill \cr 
   - 0.3827 \hfill \cr}  \right]
$$
所以n=4时得到的插值函数为
$$
{P_4}(t) = {1 \over {\sqrt 2 }}\left[ {0.9239\cos {{(2t + 1)\pi } \over 8} + \cos {{2(2t + 1)\pi } \over 8} - 0.3827\cos {{3(2t + 1)\pi } \over 8}} \right]
$$

#### DCT变换和最小二乘近似

**定理（DCT最小二乘近似定理）**：设$x=[x_0,...,x_{n-1}]^T$为n维实数向量，令$y=[y_0,...,y_{n-1}]^T=Cx$，其中C是DCT变换矩阵，则对于任意正整数m≤n，通过选择系数$y_0$，...，$y_{m-1}$，得到的函数
$$
{P_m}(t) = {1 \over {\sqrt n }}{y_0} + \sqrt {{2 \over n}} \sum\limits_{k = 1}^{m - 1} {{y_k}\cos {{k(2t + 1)\pi } \over {2n}}}
$$
可以使平方误差$\sum\limits_{j = 0}^{n - 1} {{{\left( {{P_m}(j) - {x_j}} \right)}^2}}$最小化。



### 二维DCT和图像压缩

#### 二维DCT

二维DCT变换实际上只是把一维DCT先后应用到二维数据的各个维度上，二维DCT可以用来进行二维网格数据的插值和拟合，计算过程与一维的情形类似。把纵坐标作为第一维，横坐标作为第二维，二维DCT的目的是构造插值函数F(s,t)拟合$n^2$个点$(s_i,t_j,x_{ij})$，其中i，j=0，...，n-1。从最小二乘的观点来看，二维DCT用最优的方法实现了以上的拟合。

二维DCT是将一维DCT变换先后应用到水平和竖直方向上。考虑二维数据$x_{ij}$组成的矩阵X，先在水平s方向上应用一维DCT变换，首先需要把X转置，再乘以变换矩阵C。结果中的每一列就是X中每行的一维DCT结果，$CX^T$的每一列对应固定的$t_i$，对t方向进行一维DCT变换需要按每行来进行；因此再次转置并乘以C，得到
$$
C{(C{X^T})^T} = CX{C^T}
$$
**定义**：n×n矩阵X的二维DCT变换(2D-DCT)定义为$Y=CXC^T$。

**定义**：n×n矩阵Y的二维DCT逆变换是矩阵$X=C^TYC$。

正交变换（例如2D-DCT) 与插值有密切的关系，插值的过程是为了恢复原始数据点，先通过DCT变换得到插值系数，然后用这些系数构造插值函数，再在函数上采样得到数据点。由于C是正交阵，$C^{-1}=C^T$，2D-DCT的逆变换可以写成插值的形式，$X=C^TYC$，因为在这个方程中的$x_{ij}$实际上是余弦函数的乘积。

对于$X = {C^T}YC$，2D-DCT插值如下
$$
\eqalign{
  & {P_n}(i,j) = {x_{ij}} = \sum\limits_{k = 0}^{n - 1} {\sum\limits_{t = 0}^{n - 1} {C_{ik}^T{y_{kl}}{C_{lj}}} }  = \sum\limits_{k = 0}^{n - 1} {\sum\limits_{t = 0}^{n - 1} {{C_{ki}}{y_{kl}}{C_{lj}}} }   \cr 
  &  = {2 \over n}\sum\limits_{k = 0}^{n - 1} {\sum\limits_{t = 0}^{n - 1} {{y_{kl}}{a_k}{a_l}\cos {{k(2i + 1)\pi } \over {2n}}\cos {{l(2j + 1)\pi } \over {2n}}} }  \cr} 
$$
$P_n(i,j)=x_{ij}$，其中i,j=0，...，n-1。



#### 图像压缩

一般对8×8的图像块进行DCT变换，将图像中的能量集中到图像块的左上角。



#### 量化

**模q量化**

> 量化：$z = round\left( {{y \over q}} \right)$
>
> 反量化：$\bar y=qz$

**线性量化**定义为矩阵
$$
q_{kl}=8p(k+l+1)
$$
其中0≤k，l≤7，其中常量p称为损失参数。



RGB -> GRAY
$$
X_{gray}=0.2126R+0.7152G+0.0722B
$$
RGB -> YUV
$$
Y=0.299R+0.587G+0.114B \\
U = B-Y \\
V = R-Y
$$

### 霍夫曼编码

不过多赘述



### 改进的DCT和音频压缩

人的听觉系统对频率非常敏感，在压缩和解压过程中产生的任何瑕疵都能被听出来，基于这个原因，音频压缩中通常会用到一些复杂的技巧来掩盖压缩带来的影响。

#### 改进的DCT

**定义**：离散余弦变换版本4（DCT4）是指对n维向量$x=(x_0,...,x_{n-1})^T$，其变换为
$$
y=Ex
$$
其中E为n×n矩阵
$$
{E_{ij}} = \sqrt {{2 \over n}} \cos {{\left( {i + {1 \over 2}} \right)\left( {j + {1 \over 2}} \right)\pi } \over n}
$$
**引理**：设$c_j$为（扩展的）DCT4矩阵的第j列，则有

（1）$c_j=c_{-1-j}$（各列关于j=-1/2对称），

（2）$c_j=-c_{2n-1-j}$（各列关于j=n-1/2反对称）。

我们将用DCT4对应的矩阵E来构造MDCT变换，设n为偶数，我们要用列$c_{n\over 2}$，...，$c_{{5\over 2}n-1}$构造一个新的矩阵。上面的引理说明，对任意整数j，$c_j$都可以用DCT4中的某一列来表示，即0≤i≤n-1中的某个$c_i$，如下图所示。

![image-20231105185249948](D:\TyporaImages\image-20231105185249948.png)

**定义**：令n为正偶数，向量$x=(x_0,\cdots,x_{2n-1})^T$的**修正的DCT**（MDCT）为n维向量
$$
y=Mx
$$
其中M是n×2n矩阵
$$
{M_{ij}} = \sqrt {{2 \over n}} \cos {{\left( {i + {1 \over 2}} \right)\left( {j + {n \over 2} + {1 \over 2}} \right)\pi } \over n}
$$
其中，0≤i≤n-1，0≤j≤2n-1。

与前面讲到的DCT变换的最大区别是： MDCT把2n维的向量转换为了n维的向量。基于这个原因， MDCT不是可逆的，但是通过重叠这些长度为2n的向量，也可以达到可逆的目的。

我们可以把MDCT的变换矩阵M用DCT4的列来表示，然后用上面的引理来简化，得到
$$
\eqalign{
  & M = \left[ {{c_{{n \over 2}}} \cdots {c_{{5 \over 2}n - 1}}} \right] = \left[ {{c_{{n \over 2}}} \cdots {c_{n - 1}}|{c_n} \cdots {c_{{{3n} \over 2} - 1}}|{c_{{{3n} \over 2}}} \cdots {c_{2n - 1}}|{c_{2n}} \cdots {c_{{5 \over 2}n - 1}}} \right]  \cr 
  &  = \left[ {{c_{{n \over 2}}} \cdots {c_{n - 1}}| - {c_{n - 1}} \cdots  - {c_{{n \over 2}}}| - {c_{{n \over 2} - 1}} \cdots  - {c_0}| - {c_0} \cdots {-c_{{n \over 2} - 1}}} \right] \cr}
$$
对n=4的MDCT矩阵
$$
M = \left[ {{c_2}{c_3}|{c_4}{c_5}|{c_6}{c_7}|{c_8}{c_9}} \right] = \left[ {{c_2}{c_3}| - {c_3}-{c_2}| - {c_1} - {c_0}|{-c_0}{-c_1}} \right]
$$
令A和B表示DCT4矩阵的左半部分和右半部分，则E=[A|B]。定义一个置换矩阵R，可以把矩阵的各列顺序颠倒：
$$
R = \left[ {\matrix{
   {} & {} & 1  \cr 
   {} &  {\mathinner{\mkern2mu\raise1pt\hbox{.}\mkern2mu
 \raise4pt\hbox{.}\mkern2mu\raise7pt\hbox{.}\mkern1mu}}  & {}  \cr 
   1 & {} & {}  \cr 

 } } \right]
$$
当一个矩阵右乘以矩阵R，则它的各列的顺序会颠倒。若矩阵左乘以R，则其各行的顺序会颠倒。

利用置换矩阵，可以进一步简化MDCT矩阵的表示
$$
M=(B|-BR|-AR|-A)
$$
其中AR和BR是列序颠倒后的A和B矩阵。

MDCT变换可以用DCT4来表示，令
$$
x = \left[ \matrix{
  {x_1} \hfill \cr 
  {x_2} \hfill \cr 
  {x_3} \hfill \cr 
  {x_4} \hfill \cr}  \right]
$$
为一个2n维的向量，其中$x_i$是一个n/2维的向量（注意n为偶数）
$$
Mx = B{x_1} - B{x_2} - AR{x_3} - A{x_4} = [A|B]\left[ \matrix{
   - R{x_3} - {x_4} \hfill \cr 
  {x_1} - R{x_2} \hfill \cr}  \right] = E\left[ \matrix{
   - R{x_3} - {x_4} \hfill \cr 
  {x_1} - R{x_2} \hfill \cr}  \right]
$$
其中E是一个n×n的DCT4变换矩阵，$Rx_2$和$Rx_3$表示把$x_2$和$x_3$的元素顺序颠倒。这样我们就可以用正交矩阵E来表示M。

由于MDCT变换的矩阵M是一个n×2n矩阵，而不是方阵，因此它是不可逆的。但是，两个相邻的MDCT 的总阶数为2n，因此若把它们组合起来，就可以完美地重建出输入数据x。

MDCT的“逆”表示为一个2n×n矩阵$N=M^T$，是M的转置。
$$
{N_{ij}} = \sqrt {{2 \over n}} \cos {{\left( {j + {1 \over 2}} \right)\left( {i + {n \over 2} + {1 \over 2}} \right)\pi } \over n}
$$

$$
N = \left[ \matrix{
  {B^T} \hfill \cr 
   - R{B^T} \hfill \cr 
   - R{A^T} \hfill \cr 
   - {A^T} \hfill \cr}  \right]
$$

因为E是正交阵，可以得到
$$
\eqalign{
  & {A^T}A = I  \cr 
  & {B^T}B = I  \cr 
  & {A^T}B = {B^T}A = 0 \cr
  & {R^2=I} \cr
  }
$$

$$
NM\left[ \matrix{
  {x_1} \hfill \cr 
  {x_2} \hfill \cr 
  {x_3} \hfill \cr 
  {x_4} \hfill \cr}  \right] = \left[ \matrix{
  {B^T} \hfill \cr 
   - R{B^T} \hfill \cr 
   - R{A^T} \hfill \cr 
   - {A^T} \hfill \cr}  \right]\left[ {B{x_1} - B{x_2} - AR{x_3} - A{x_4}} \right] = \left[ \matrix{
  {x_1} - R{x_2} \hfill \cr 
   - R{x_1} + {x_2} \hfill \cr 
  {x_3} + R{x_4} \hfill \cr 
  R{x_3} + {x_4} \hfill \cr}  \right]
$$

在音频压缩中，MDCT变换的向量对应于重叠的数据段。压缩误差会使每段向量之间的连接处产生不连续，而由于向量的长度是固定的，因此这种误差会带来固定频率的噪声。而听觉系统对周期性错误的敏感度要高于视觉系统；实际上，固定周期的错误就是那个频率上的一个声调，而人耳正是感知的各种声调的。如果数据表示采用重叠的方式，令n为正偶数，
$$
{Z_1} = \left[ \matrix{
  {x_1} \hfill \cr 
  {x_2} \hfill \cr 
  {x_3} \hfill \cr 
  {x_4} \hfill \cr}  \right],\quad {Z_2} = \left[ \matrix{
  {x_3} \hfill \cr 
  {x_4} \hfill \cr 
  {x_5} \hfill \cr 
  {x_6} \hfill \cr}  \right]
$$
为两个2n维的向量，其中每个$x_i$是一个长度为n/2的向量，向量$Z_1$和$Z_2$有一半长度是重叠的，
$$
NM{Z_1} = \left[ \matrix{
  {x_1} - R{x_2} \hfill \cr 
   - R{x_1} + {x_2} \hfill \cr 
  {x_3} + R{x_4} \hfill \cr 
  R{x_3} + {x_4} \hfill \cr}  \right],\quad NM{Z_2} = \left[ \matrix{
  {x_3} - R{x_4} \hfill \cr 
   - R{x_3} + {x_4} \hfill \cr 
  {x_5} + R{x_6} \hfill \cr 
  R{x_5} + {x_6} \hfill \cr}  \right]
$$
把$NMZ_1$的下半部分和$NMZ_2$的上半部分求和，可以重建出n维向量[$x_3$，$x_4$]：
$$
\left[ \matrix{
  {x_3} \hfill \cr 
  {x_4} \hfill \cr}  \right] = {1 \over 2}{\left( {NM{Z_1}} \right)_{n \cdots 2n - 1}} + {1 \over 2}{\left( {NM{Z_2}} \right)_{0 \cdots n - 1}}
$$

#### 位量化

通过对信号的MDCT进行量化，可以实现音频信号的有损压缩。通过推广图像压缩中使用的量化方法，可以更好地控制信号表示所需的位数。

首先确定一个实数开区间(-L，L)，假设我们要用b位来表示(-L，L)上的一个数，而且可以接受一定程度的误差。我们可以用一个位来表示符号，并用b—1位来对数值进行量化。公式为:

**(-L,L)的b位量化**

> 量化：$z=round({y\over q})$，其中$q={2L\over 2^b-1}$
>
> 反量化：$\bar y = qz$

## 特征值与奇异值

### 幂迭代方法

在求解方程中遇到的威尔金森多项式，难以寻找特征值。本节介绍的方法基于对矢量乘上矩阵的高阶幂，这个向量随着幂的升高会变成特征向量，在后面我们将对这个方法进行精化，但是这就是最复杂方法的主要思想。



#### 幂迭代

幂迭代的动机是与矩阵相乘可以将向量推向主特征向量的方向。

**定义**：令A是一个m×m矩阵，A的**占优特征值**是量级比矩阵A所有其他特征值都大的特征值λ，如果这样的特征值存在，与λ相关特征向量被称为**占优特征向量**。

幂迭代本质上是一个在每步上进行归一化的不动点迭代。和FPI一样，该方法线性收敛，意味着在收敛过程中，在每个迭代步骤中误差以常数因子降低。在本节的随后部分，将讨论幂迭代具有二阶收敛的变种，该方法称为瑞利商迭代。

考虑特征值方程xλ=Ax，其中x是特征向量的近似，λ未知，在这种情况下，系数矩阵是n×1矩阵x，法线方程指出最小二乘解是$x^Tx\lambda=x^TAx$的解，或者
$$
\lambda  = {{{x^T}Ax} \over {{x^T}x}}
$$
这就是瑞利商，给定特征向量近似，瑞利商是特征值的最优近似。

**幂迭代**

> 给定初始向量$x_0$
>
> for j = 1，2，3，...
>
> ​	$u_{j-1}=x_{j-1}/||x_{j-1}||_2$
>
> ​	$x_j=Au_{j-1}$
>
> ​	$\lambda_j=u_{j-1}^TAu_{j-1}$
>
> end
>
> $u_j=x_j/||x_j||_2$



#### 幂迭代的收敛

**定理**：令A是一个m×m矩阵，特征值为$\lambda_1$，...，$\lambda_m$，并满足$|\lambda_1|>|\lambda_2|\ge|\lambda_3|\ge\cdots\ge|\lambda_m|$。假设矩阵A的特征向量张成$R^m$空间，对于几乎所有的初始向量，幂迭代线性收敛到和$\lambda_1$相关的特征向量，收敛常数为$S=|\lambda_2/\lambda_1|$。



#### 幂迭代的逆

幂迭代局限于求解最大（绝对值最大）的特征值，如果幂迭代用于矩阵的逆矩阵，可以找到最小的特征值。

**逆向幂迭代**

> 给定初始向量$x_0$以及平移s
>
> for j = 1,2,3,...
>
> ​	$u_{j-1}=x_{j-1}/||x_{j-1}||_2$
>
> ​	求解$(A-sI)x_j=u_{j-1}$
>
> ​	$\lambda_j=u_{j-1}^Tx_j$
>
> end
>
> $u_j=x_j/||x_j||_2$

为了找出矩阵A在实数s附近的特征值，对$(A-sI)^{-1}$使用幂迭代得到$(A-sI)^{-1}$的最大特征值b。幂迭代可以通过对$(A-sI)y_{k+1}=x_k$进行高斯消去得到。则$\lambda=b^{-1}+s$为矩阵A在s附近的特征值．和λ相关的特征向量可由计算直接给出。



#### 瑞利商迭代

瑞利商可以同逆向幂迭代同时使用，我们知道它会收敛到和s最接近的特征值对应的特征向量，如果这个距离很小则收敛速度很快。如果在这个过程中任何步骤里，近似的特征值已知，可以使用该特征值作为s，以加速收敛。使用瑞利商作为逆向幂迭代中更新的平移会得到瑞利商迭代(RQI)方法。

**瑞利商迭代**

> 给定初值$x_0$
>
> for j=1,2,3,...
>
> ​	$u_{j-1}=x_{j-1}/||x_{j-1}||$
>
> ​	$\lambda_{j-1}=u_{j-1}^TAu_{j-1}$
>
> ​	求解$(A-\lambda_{j-1}I)x_j=u_{j-1}$
>
> end
>
> $u_j=x_j/||x_j||_2$



### QR算法

本节的目标是推导可以一次找出所有特征值的方法，我们从一个用于对称矩阵的方法开始，然后对该方法进行补充，并用于一般矩阵的特征值求解。对称矩阵容易处理是因为它的特征值为实数，其特征向量构成$R^m$空间（见附录A）的一组单位正交基。这激发了同时对m个向量使用幂迭代，其中我们需要保持每个向量都与其他向量正交。

#### 同时迭代

假设开始有m个两两正交的初始向量，对每个向量使用幂方法一步后得到的向量不再保证彼此正交，所以在每步后重新对m个向量正交化，正交步骤可以看做对结果进行QR分解，如果使用初等基向量作为初始向量，则重新正交化后幂迭代的第一步是$AI=\bar Q_1R_1$或者
$$
\left[ {\matrix{
   {\left. {A\left[ \matrix{
  1 \hfill \cr 
  0 \hfill \cr 
   \vdots  \hfill \cr 
  0 \hfill \cr}  \right]} \right|} & {\left. {A\left[ \matrix{
  0 \hfill \cr 
  1 \hfill \cr 
   \vdots  \hfill \cr 
  0 \hfill \cr}  \right]} \right|} & {\left.  \cdots  \right|} & {\left. {A\left[ \matrix{
  0 \hfill \cr 
  0 \hfill \cr 
   \vdots  \hfill \cr 
  1 \hfill \cr}  \right]} \right|}  \cr 

 } } \right] = \left[ {{{\overline {{q_1}} }^1}| \cdots |{{\overline {{q_m}} }^1}} \right]\left[ {\matrix{
   {r_{11}^1} & {r_{12}^1} &  \cdots  & {r_{1m}^1}  \cr 
   {} & {r_{22}^1} & {} &  \vdots   \cr 
   {} & {} &  \ddots  & {}  \cr 
   {} & {} & {} & {r_{mm}^1}  \cr 

 } } \right]
$$
$\bar {q_i}^1$是幂迭代过程中新的正交单位向量集，其中i=1，...，m。

**归一化同时迭代（NSI）**

> 设 $\bar{Q_0}=I$
>
> for j=1,2,3,...
>
> ​	$A\bar{Q}_j=\bar{Q}_{j+1}R_{j+1}$
>
> end

在第j步，$\bar {Q}_j$的列是A的特征向量的近似，对角线元素$r_{11}^j$，...，$r_{mm}^k$是近似的特征值。

```matlab
function [lam, Q] = nsi(A,k)
[m,n] = size(A);
Q=eye(m,m);
for j = 1:k
	[Q,R] = qr(A*Q)		% QR分解
end
lam = diag(Q'*A*Q);		 % 瑞利商
```



**无移动的QR算法**

用$R_j'Q_j$替代$A_j$

```matlab
function [lam, Q] = unshiftedqr(A,k)
[m,n] = size(A);
Q=eye(m,m);
Qbar = Q; R=A;
for j = 1:k
	[Q,R] = qr(R*Q)		% QR分解
	Qbar = Qbar * Q;
end
lam = diag(R*Q);		 % 瑞利商
```



**定理**：假设A是对称m×m矩阵，特征值为$\lambda_i$，满足$|\lambda_1|>|\lambda_2|>>\cdots>|\lambda_m|$。无移动的QR算法可以线性收敛到A的特征值和特征向量。当j→∞时，$A_j$收敛到对角线矩阵，主对角线上包含所有的特征值，$\bar Q_j=Q_1\cdots Q_j$收敛到正交矩阵，对应的列是特征向量。



#### 实数舒尔形式和QR算法

**定义**：如果矩阵T为上三角矩阵，且在主对角线上可以出现2×2的块，该矩阵具有**实数舒尔形式**。

**定理**：令A是实数元素的方阵，则存在正交矩阵Q以及实数舒尔形式的矩阵T，满足$A=Q^TTQ$。

完整的QR算法迭代通过一系列相似变换，将任意矩阵A移动到它的舒尔分解。我们将使用两个步骤，首先将移动的逆向幂迭代思想植入，并加入收缩技术推出移动QR算法，然后推出改进版本允许处理复数特征值。平移版本可以非常直观地写出。每步中都需要使用平移，完成QR分解，然后再平移回来，使用符号表示：
$$
\eqalign{
  & {A_0} - sI = {Q_1}{R_1}  \cr 
  & {A_1} = {R_1}{Q_1} + sI \cr} 
$$
注意到
$$
{A_1} - sI = {R_1}{Q_1} = Q_1^T({A_0} - sI){Q_1} = Q_1^T{A_0}{Q_1} - sI
$$
意味着$A_1$和$A_0$相似，因而具有相同的特征值，重复这个步骤，生成一系列$A_k$矩阵，都与$A=A_0$相似。什么是最优的平移？这带来了特征值计算中的收缩概念。我们将选择平移量为矩阵$A_k$的右下角元素，这将导致迭代随着矩阵收敛为实数舒尔形式，而将矩阵的最下面一行除了最右边元素之外全部变为0。在这个元素收敛为特征值后，我们去掉矩阵的最后一行和最后一列进行收缩，然后继续寻找其他的特征值。

**原书中有平移QR算法和改进版本的matlab代码**



#### 上海森伯格形式

**定义**：如果$a_{ij}=0$，i>j+1，m×n矩阵A是**上海森伯格形式**。

**定理**：令A是一个方阵，存在正交矩阵Q，满足$A=QBQ^T$，并且B是上海森伯格形式。

可以使用用于构造QR分解的豪斯霍尔德反射构造B，但是在这里有一个主要的差别：现在我们关注在矩阵的左侧和右侧乘上反射子H，这是由于最后想得到一个具有相同的特征值的相似矩阵。正因为如此，我们必须逐步地将0放入矩阵A中。

**原书中给出了对应的matlab代码**



### 奇异值分解

在$R^m$中对单位球施加m×m矩阵变换得到一个椭球，这个有趣的事实是奇异值分解的基础。对于每个m×n的矩阵A，具有单位正交集{$u_1$，...，$u_m$}和{$v_1$，...，$v_n$}，以及非负数字$s_1\ge\cdots s_n\ge 0$，满足
$$
\eqalign{
  & A{v_1} = {s_1}{u_1}  \cr 
  & A{v_2} = {s_2}{u_2}  \cr 
  &  \vdots   \cr 
  & A{v_n} = {s_n}{u_n} \cr} 
$$
$v_i$被称为矩阵A的右奇异向量，$u_i$是矩阵A的左奇异向量，$s_i$是矩阵A的奇异值。

**举例**：找出矩阵A的奇异值和奇异向量
$$
A = \left[ {\matrix{
   0 & { - {1 \over 2}}  \cr 
   3 & 0  \cr 
   0 & 0  \cr 

 } } \right]
$$

$$
\eqalign{
  & A{v_1} = A\left[ \matrix{
  1 \hfill \cr 
  0 \hfill \cr}  \right] = 3\left[ \matrix{
  0 \hfill \cr 
  1 \hfill \cr 
  0 \hfill \cr}  \right] = {s_1}{u_1}  \cr 
  & A{v_2} = A\left[ \matrix{
  0 \hfill \cr 
  1 \hfill \cr}  \right] = {1 \over 2}\left[ \matrix{
   - 1 \hfill \cr 
  0 \hfill \cr 
  0 \hfill \cr}  \right] = {s_2}{u_2} \cr} 
$$

在对m×n矩阵A分解的过程中，有一个标准的方式来记录所有的信息，生成一个m×m矩阵U，它的列为左奇异向量$u_i$，一个n×n矩阵V的列为右奇异向量$v_i$，以及一个m×n的对角线矩阵S，其对角线元素是奇异值$s_i$，m×n矩阵A的**奇异值分解（SVD）**为
$$
A=USV^T
$$

#### 找出一般的SVD

**引理**：令A是一个m×n矩阵，$A^TA$的特征值非负。

对于一个m×n的矩阵A，n×n矩阵$A^TA$对称，因而它的特征向量正交，特征值是实数。又特征值是非负实数，因而可以表达为$s_1^2$≥...≥$s_n^2$，其对应的正交特征向量集为{$v_1$，...，$v_n$}，使用下面的方向找出$u_i$，其中1≤i≤m：

如果$s_i\neq0$，使用方程$s_iu_i=Av_i$定义$u_i$；

如果$s_i=0$，选择一个任意的单位向量$u_i$，与$u_1$，...，$u_{i-1}$正交。

注意$u_i$是两两正交的单位向量（当$s_i=0$时，根据定义可得，当$s_i\neq0$，有
$$
\eqalign{
  & {\left( {{s_i}{u_i}} \right)^T}{s_j}{u_j} = u_i^Ts_i^T{s_j}{u_j} = s_i^T{s_j}u_i^T{u_j}  \cr 
  & {\left( {A{v_i}} \right)^T}A{v_j} = v_i^T{A^T}A{v_j} = \lambda v_i^T{v_j} = 0  \cr 
  & s_i^T{s_j}u_i^T{u_j} = \lambda v_i^T{v_j} = 0 \cr}
$$
）因此是$R^m$的一个正交基。事实上，$u_1$，...，$u_m$构成了$AA^T$的正交特征向量。

**定理**：令A是m×n矩阵，则存在两个正交基：$R^m$的{$v_1$，...，$v_n$}，与$R^m$的{$u_1$，...，$u_m$}，以及实数$s_1$≥...≥$s_n$≥0，满足$Av_i=s_iu_i$，1≤i≤min{m,n}，$V=[v_1|\cdots|v_n]$的列是右奇异向量，并构成$A^TA$的单位正交向量；$U=[u_1|\cdots|u_m]$的列是左奇异向量，构成了$A^TA$的单位正交向量。

**举例**：找出2×2矩阵A的奇异值和奇异向量
$$
A = \left[ {\matrix{
   0 & 1  \cr 
   0 & { - 1}  \cr 

 } } \right]
$$

$$
{A^T}A = \left[ {\matrix{
   0 & 0  \cr 
   0 & 2  \cr 

 } } \right]
$$

的奇异值以降序排列为$v_1=[0,1]$，$s_1^2=2$，$v_2=[1,0]$，$s_2^2=0$。奇异值为$\sqrt 2$与0，根据前面的方向，$u_1$定义为
$$
\sqrt 2 {u_1} = A{v_1} = \left[ \matrix{
  1 \hfill \cr 
   - 1 \hfill \cr}  \right]
$$
求解$u_1=[1/\sqrt2, -1/\sqrt 2]^T$，因为$s_2$为0，所以选择$u_2$与$u_1$正交即可，$u_2=[1/\sqrt2,1/\sqrt2]^T$，SVD为
$$
\left[ {\matrix{
   0 & 1  \cr 
   0 & { - 1}  \cr 

 } } \right] = \left[ {\matrix{
   {\sqrt 2 /2} & {\sqrt 2 /2}  \cr 
   { - \sqrt 2 /2} & {\sqrt 2 /2}  \cr 

 } } \right]\left[ {\matrix{
   {\sqrt 2 } & 0  \cr 
   0 & 0  \cr 

 } } \right]\left[ {\matrix{
   0 & 1  \cr 
   1 & 0  \cr 

 } } \right]
$$
注意SVD分解并不唯一，还有另外一种分解
$$
\left[ {\matrix{
   0 & 1  \cr 
   0 & { - 1}  \cr 

 } } \right] = \left[ {\matrix{
   { - \sqrt 2 /2} & {\sqrt 2 /2}  \cr 
   {\sqrt 2 /2} & {\sqrt 2 /2}  \cr 

 } } \right]\left[ {\matrix{
   {\sqrt 2 } & 0  \cr 
   0 & 0  \cr 

 } } \right]\left[ {\matrix{
   0 & { - 1}  \cr 
   1 & 0  \cr 

 } } \right]
$$
对于对称矩阵，只需计算自身的特征值和特征向量即可。



### SVD的应用

#### SVD的性质

假设$A=USV^T$是奇异值分解，m×n矩阵A的秩是线性无关行的个数（也可以是线性无关列的个数）

**性质1**：矩阵$A=USV^T$的秩是S中非0元素的个数。

**性质2**：如果A为n×n矩阵，则$\left| {\det (A)} \right| = {s_1} \cdots {s_n}$

**性质3**：如果A为可逆的m×m矩阵，则${A^{ - 1}} = V{S^{ - 1}}{U^T}$

**性质4**：m×n矩阵A可以写成秩为1的矩阵的和
$$
A = \sum\limits_{i = 1}^r {{s_i}{u_i}v_i^T}
$$
其中r为A的秩，$u_i$和$v_i$分别是U和V的第i列。该性质是SVD的低秩近似性质，对于A的最优最小二乘近似为保留的前p项，p为矩阵A的秩，p≤r。



#### 降维

降维思想是将数据投影到低维空间。假设$a_1$，...，$a_n$包含一组m维的向量，在富含数据的应用中，m远小于n，降维的目标是使用n个向量替换$a_1$，...，$a_n$，其中新向量的维数p<m，同时最小化该过程中引入的误差。通常我们从一组均值为0的向量开始，如果向量的均值不为0，我们可以减去均值实现，并在后面的阶段再把均值加回去。

SVD则给出了一种直接完成降维的方式，考虑数据向量为m×n矩阵$A=[a_1|\cdots|a_n]$，计算奇异值分解$A=USV^T$，令$e_j$表示第j个初等基向量（除了第j个元素为1，其他元素都为0），则$Ae_j=a_j$，使用性质4的秩近似
$$
A \approx {A_p} = \sum\limits_{i = 1}^p {{s_i}{u_i}v_i^T}
$$
我们可以将$a_j$投影到p维空间，该空间由U的列向量$u_1$，...，$u_p$张成
$$
{a_j} = A{e_j} \approx {{\rm{A}}_p}{e_j}
$$
由于矩阵与$e_j$相乘仅捡出第j列，我们可以这样描述：空间$<u_1,\cdots,u_p>$由左奇异向量$u_1$，...，$u_p$张成，这是对于$a_1$，...，$a_n$的p维子空间在最小二乘意义上的近似，A的列$a_i$在该空间上的正交投影对应$A_p$的列。换句话说，一组向量$a_1$，...，$a_n$到其最优的最小二乘p维子空间的投影就是矩阵最优的秩p近似矩阵$A_p$。



#### 压缩

可以利用性质4来压缩矩阵信息，注意到在性质4中的秩1展开中的每一项使用两个向量$u_i$，$v_i$；以及另一个数字$s_i$定义。如果A是一个n×n矩阵，我们可以尝试矩阵A的有损压缩，其中扔掉性质4求和后面的几项，它们具有较小的$s_i$。每一项在展开中需要2n+1个数字保存或者传输。



#### 计算SVD

如果A是一个实数对称矩阵，SVD可以消减为本章前面讨论的特征值计算问题。在这种情况下单位特征向量构成正交基。如果我们定义矩阵V将单位特征向量保存在列上，则AV=US表示特征向量方程，其中S是对角线矩阵，保存特征值的绝对值。对于U的操作和V一样，但是如果特征值是负，则要变换列的符号，由于U和V是正交矩阵，
$$
A=USV^T
$$
是矩阵A的奇异值分解。

对于一个一般的非对称m×n矩阵A，确定SVD有两种不同的方法。最明显的方法是构造$A^TA$，但当A的条件数很大时，该方法可能不适合使用。

考虑如下矩阵
$$
B = \left[ {\matrix{
   0 & {{A^T}}  \cr 
   A & 0  \cr 

 } } \right]
$$
注意，B是对称的(m+n)×(m+n)矩阵，因而，它具有实数特征值和作为基的特征向量。令[v,w]表示一个(m+n)向量，对应B的特征向量，则
$$
\left[ \matrix{
  {A^T}w \hfill \cr 
  Av \hfill \cr}  \right] = \left[ {\matrix{
   0 & {{A^T}}  \cr 
   A & 0  \cr 

 } } \right]\left[ \matrix{
  v \hfill \cr 
  w \hfill \cr}  \right] = \lambda \left[ \matrix{
  v \hfill \cr 
  w \hfill \cr}  \right]
$$
或者Av=λw，左侧乘上$A^T$得到
$$
{A^T}Av = \lambda {A^T}w = {\lambda ^2}v
$$
表明w是$A^TA$的特征向量，对应的特征值为$\lambda^2$，注意到我们可以确定$A^TA$的特征值和特征向量，但是不需要构造$A^TA$。

因而，另一种更好的计算奇异值和奇异向量的方法是，将对称矩阵B转化为上海森伯格形式。由于对称，上海森伯格形式的矩阵和三对角线矩阵等价，则该方法与平移QR算法一样可以找出特征值，这是奇异值的平方，特征向量中n个最大的项是奇异向量v、尽管这种方式看起来使得矩阵的大小加倍，但是它避免对条件数的不必要提高。



## 最优化

最优化指的是找出实数函数的极大或者极小值，该函数称为目标函数。由于定位函数f(z)的极大值与找出函数-f(x)的极小值等价，在推导计算方式时仅考虑最小化问题就足够了。



### 不使用导数的无约束优化



#### 黄金分割搜索

**定义**：当区间[a,b]上只有一个极大或者极小值，并且f在其他点上严格升高或降低，连续函数f(x)被称为区间[a,b]上的**单峰函数**。

**定理**：从初始区间[a,b]开始，黄金分割搜索k步之后，最后区间中点和最小值之间的差异为$g^k(b-a)/2$，其中$g = (\sqrt 5  - 1)/2 \approx 0.618$。

**黄金分割搜索**

> 给出单峰函数f，在区间[a,b]上具有极小值
>
> for i = 1,2,3,...
>
> ​	$g=(\sqrt 5-1)/2$
>
> ​	if f(a+(1-g)(b-a)) < f(a+g(b-a))
>
> ​		b = a+g(b-a)
>
> ​	else
>
> ​		a = a+(1-g)(b-a)
>
> ​	end
>
> end
>
> 最终区间[a,b]中包含极小值



#### 持续的抛物线插值

**持续抛物线插值**

> 从近似极小值r，s，t开始
> for i = 1,2,3,...
> ​	$x = {{r + s} \over 2} - {{\left( {f(s) - f(r)} \right)\left( {t - r} \right)\left( {t - s} \right)} \over {2\left[ {\left( {s - r} \right)\left( {f(t) - f(s)} \right) - \left( {f(s) - f(r)} \right)\left( {t - s} \right)} \right]}}$
> ​	t = s
> ​	s = r
> ​	r = x
> end

x是对极小值的新的近似。在SPI中，新的x可以替换r，s，t中离当前最远，或者最差的一个点，并重复进行该步骤。对于SPI不能保证收敛，这与黄金分割搜索不同。但是，如果收敛的话，由于该方法更好地使用了函数求值的信息，所以速度会更快。



#### Nelder-Mead搜索

Nelder-Mead搜索试图将一个多面体滚到一个尽可能低的水平。由于这个原因，它也被称为单纯型下山法。它没有使用目标函数的导数信息。假设需要最小化的函数f具有n个变量，该方法首先需要n+1个属于$R^m$的初始估计向量$x_1$，...，$x_{n+1}$，这些点构成n维的单纯型。测试单纯型的顶点，并根据它们的值以升序排列$y_1$<$y_2$<...<$y_{n+1}$=$y_h$。最差的单纯型向量$x_h=x_{n+1}$根据下面的流程图进行替换。首先我们定义略去$x_h$的单纯型平面的重心$\bar x$。然后我们测试在反射点$x_r=2\bar x-x_h$上的函数值$y_r=f(x_r)$，如图（a）所示，如果新的值$y_r$在$y_1<y_2<y_n$范围中，我们使用$x_r$替换最差的点$x_n$，使用函数值对顶点排序，并重复前面的步骤。

<img src="D:\TyporaImages\image-20231106101117408.png" alt="image-20231106101117408" style="zoom:80%;" />

具体代码见原书P503页



### 使用导数的无约束优化



#### 牛顿方法

如果函数连续可微，可以对导数求导，则优化问题可以表示为求解根的问题。

在一个连续可微函数f(x)的局部极小值点$x^*$处，一阶导数必然为0。可以使用求解方程组的方法求解f'(x)=0。如果目标函数单峰，在区间中具有极小值，则使用最小值$x^*$附近的初始估计开始牛顿方法的计算，这将会收敛到$x^*$。对f'(x)=0应用牛顿方法得到如下迭代
$$
{x_{k + 1}} = {x_k} - {{f'({x_k})} \over {f''({x_k})}}
$$
当遇到一个多变量函数时，可以使用多变量的牛顿方法，设置导数为0，有
$$
\nabla f = 0
$$
其中
$$
\nabla f = \left[ {{{\partial f} \over {\partial {x_1}}}\left( {{x_1}, \cdots ,{x_n}} \right)\;,\; \cdots \;,\;{{\partial f} \over {\partial {x_n}}}\left( {{x_1}, \cdots ,{x_n}} \right)} \right]
$$
表示梯度。

使用方程组一章中的牛顿方法可以求解这个问题，设$F(x)=\nabla f(x)$，牛顿方法的迭代步中将设置$x_{k+1}=x_k+v$，其中v是$DF(x_k)v=-F(x_k)$的解。梯度的雅可比矩阵DF为
$$
{H_f} = DF = \left[ {\matrix{
   {{{{\partial ^2}f} \over {\partial {x_1}\partial {x_2}}}} &  \cdots  & {{{{\partial ^2}f} \over {\partial {x_1}\partial {x_n}}}}  \cr 
    \vdots  & {} &  \vdots   \cr 
   {{{{\partial ^2}f} \over {\partial {x_n}\partial {x_1}}}} &  \cdots  & {{{{\partial ^2}f} \over {\partial {x_n}\partial {x_n}}}}  \cr 

 } } \right]
$$
这是函数f的海森矩阵，因而牛顿步骤为
$$
\left\{ \matrix{
  {H_f}({x_k})v =  - \nabla f({x_k}) \hfill \cr 
  {x_{k + 1}} = {x_k} + v \hfill \cr}  \right.
$$


#### 最速下降

最速下降，也称为梯度搜索，背后的基本想法是将当前点在在最速下降的方向上移动以找出函数最小值。由于$\nabla f$指向f的最速生长方向，反方向$-\nabla f$就是最速下降方向。

**最速下降**

> for i = 0,1,2,...
>
> ​	$v = \nabla f({x_i})$
>
> ​	对于标量$s=s^*$最小化$f(x-sv)$
>
> ​	$x_{i+1}=x_i-s^*v$
>
> end



#### 共轭梯度搜索

当A为对称正定矩阵时，求解Ax=w等价于找出抛物面的极小值，如在二维中，线性方程组
$$
\left[ {\matrix{
   a & b  \cr 
   b & c  \cr 

 } } \right]\left[ \matrix{
  {x_1} \hfill \cr 
  {x_2} \hfill \cr}  \right] = \left[ \matrix{
  e \hfill \cr 
  f \hfill \cr}  \right]
$$
的解是如下抛物面的极小值
$$
f({x_1},{x_2}) = {1 \over 2}ax_1^2 + b{x_1}{x_2} + {1 \over 2}cx_2^2 - e{x_1} - f{x_2}
$$
这是由于f的梯度可以表示为线性方程组
$$
\left[ {\matrix{
   a & b  \cr 
   b & c  \cr 

 } } \right]\left[ \matrix{
  {x_1} \hfill \cr 
  {x_2} \hfill \cr}  \right] - \left[ \matrix{
  e \hfill \cr 
  f \hfill \cr}  \right] = \left[ \matrix{
  a{x_1} + b{x_2} - e \hfill \cr 
  b{x_1} + c{x_2} - f \hfill \cr}  \right] = \nabla f = \left[ \matrix{
  {{\partial f} \over {\partial {x_1}}} \hfill \cr 
  {{\partial f} \over {\partial {x_2}}} \hfill \cr}  \right]
$$
在极小值处的梯度为0，正定意味着抛物面的凹面向上。

一个关键的观测时线性方程组的余项r=w-Ax为$- \nabla f(x)$，这是函数f在x点的最速下降方向。假设以及选择了一个搜索方向，使用向量d标记。为了在那个方向找到最小值，需要找出α使得函数h(α)=f(x+αd)最小。将导数设置为0以找出最小：
$$
0 = \nabla f \cdot d = \left( {A(x + \alpha d) - {{(e,f)}^T}} \right) \cdot d = {(\alpha Ad - r)^T}d
$$
这意味着
$$
\alpha  = {{{r^T}d} \over {{d^T}Ad}} = {{{r^T}r} \over {{d^T}Ad}}
$$
**共轭梯度搜索**

> 令$x_0$为初始估计，设$d_0=r_0=-\nabla f$
>
> for i = 1,2,3,...
>
> ​	$\alpha_i=\alpha$使得$f(x_{i-1}+\alpha d_{i-1})$最小
>
> ​	$x_i = x_{i-1}+\alpha_i d_{i-1}$
>
> ​	${r_i} =  - \nabla f({x_i})$
>
> ​	${\beta _i} = {{r_i^T{r_i}} \over {r_{i - 1}^T{r_{i - 1}}}}$
>
> ​	${d_i} = {r_i} + {\beta _i}{d_{i - 1}}$
>
> end



## 附录

### 分块乘法

分块计算A×B，分块的唯一要求是：对A矩阵的各列的分组方法要与B矩阵各行的分组方法一致，如

<img src="D:\TyporaImages\image-20231106114503654.png" alt="image-20231106114503654" style="zoom: 50%;" />


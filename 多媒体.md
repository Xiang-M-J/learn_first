

# 基本知识

## 图像

### IPB帧

许多视频编码方案中包括了I-frame（关键帧）、P-frame（预测帧）和 B-frame（双向预测帧）。I-frame保存了整张图片，P-frame只保存当前图片和前一张图片的差别，B-frame则保存前一张或者后一张的差别。这三种帧构成了一个GOP（Group of pictures），GOP是一组连续的画面，由一张 I 帧和数张 B/P 帧组成，是视频图像编码器和解码器存取的基本单位。GOP越大，中间的P帧和B帧越多，解码出来的视频质量越高，但是会影响编码效率。


### YUV

YUV是一种颜色编码方式，Y是亮度，UV为色度。常见的有4: 4: 4，4:2:2 和 4:2:0。YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV444意味着三个分量的采样比例相等，

```text
假如图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]

那么采样的码流为：Y0 U0 V0 Y1 U1 V1 Y2 U2 V2 Y3 U3 V3 

最后映射出的像素点依旧为 [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3] 
```

YUV422意味着UV分量是Y分量的一半，Y 分量和 UV 分量按照 2 : 1 的比例采样。

```text
 假如图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]

 那么采样的码流为：Y0 U0 Y1 V1 Y2 U2 Y3 V3 

 其中，每采样过一个像素点，都会采样其 Y 分量，而 U、V 分量就会间隔一个采集一个。

 最后映射出的像素点为 [Y0 U0 V1]、[Y1 U0 V1]、[Y2 U2 V3]、[Y3 U2 V3]
```

YUV420在每一行扫描时，只扫描一种色度分量（U 或者 V），和 Y 分量按照 2 : 1 的方式采样。

```text
假设图像像素为：
 
[Y0 U0 V0]、[Y1 U1 V1]、 [Y2 U2 V2]、 [Y3 U3 V3]
[Y5 U5 V5]、[Y6 U6 V6]、 [Y7 U7 V7] 、[Y8 U8 V8]
 
那么采样的码流为：Y0 U0 Y1 Y2 U2 Y3 Y5 V5 Y6 Y7 V7 Y8
 
其中，每采样过一个像素点，都会采样其 Y 分量，而 U、V 分量就会间隔一行按照 2 : 1 进行采样。
 
最后映射出的像素点为：

[Y0 U0 V5]、[Y1 U0 V5]、[Y2 U2 V7]、[Y3 U2 V7]
[Y5 U0 V5]、[Y6 U0 V5]、[Y7 U2 V7]、[Y8 U2 V7]
```



# FFmpeg



## 编译 FFmpeg

### 使用 Visual studio 进行编译

参考链接：[CompilationGuide/MSVC – FFmpeg](https://trac.ffmpeg.org/wiki/CompilationGuide/MSVC)
[Compiling FFmpeg with X264 on Windows 10 using MSVC | ROXLU](https://www.roxlu.com/2019/062/compiling-ffmpeg-with-x264-on-windows-10-using-msvc)


首先需要下载 [MSYS2](https://www.msys2.org/) 和 [Yasm](https://yasm.tortall.net/Download.html)，并且为它们配置环境变量，将 yasm 的可执行文件命名为 yasm.exe。对于 msys2，修改目录下的 msys2_shell.cmd 中的

```bat
rem set MSYS2_PATH_TYPE=inherit
```

为

```bat
set MSYS2_PATH_TYPE=inherit
```


打开visual studio 自带的cmd，`Tools` -> `Command Line` -> `Developer Command Prompt`。为了编译 64 位，需要执行下面的命令来启用 64 位编译器（ vcvarsall.bat 的位置根据安装位置改变，建议直接使用 everything 进行搜索）

```cmd
D:\VisualStudio\Community\VC\Auxiliary\Build\vcvarsall.bat amd64
```

打开 msys2 中的 msys2_shell.cmd，在弹出的窗口进行如下操作 。。。


## ffmpeg C 编程


### 开发环境搭建

注意从[Releases · BtbN/FFmpeg-Builds (github.com)](https://github.com/BtbN/FFmpeg-Builds/releases)下载-shared.zip版本，下面是解压后的文件夹

├─bin
├─doc
├─include
└─lib

#### Visual Studio

先设置项目，在 `c/c++` -> `general` -> `Additional Include Directories` 中添加头文件目录 include，同时设置 `SDL checks` 为 No。再在 `Linker` -> `General` -> `Additional Library Directories` 添加 lib 目录，最后在执行时，需要将 bin 文件夹的 dll 文件拷贝至执行文件路径中。

下面是一个简单示例

```c
#include<stdio.h>

// 声明链接库，否则会报错
#pragma comment (lib, "avcodec.lib")
#pragma comment (lib, "avdevice.lib")
#pragma comment (lib, "avfilter.lib")
#pragma comment (lib, "avformat.lib")
#pragma comment (lib, "avutil.lib")
#pragma comment (lib, "swresample.lib")
#pragma comment (lib, "swscale.lib")

#include "libavcodec/avcodec.h"
#include "libavformat/avformat.h"

void main() {
	printf("%s\n", avcodec_configuration());
}
```

注意如果不使用 `#pragma` 声明链接库，需要在 `Linker` -> `Input` -> `Additional Dependencies` 中添加

```
avcodec.lib
avdevice.lib
avfilter.lib
avformat.lib
avutil.lib
postproc.lib
swscale.lib
swresample.lib
```


#### Cmake

在 CmakeLists.txt 中添加以下内容

```cmake
# 设置 FFMPEG 的目录  
set(FFMPEG_DEV_ROOT "D:/ffmpeg-7.0.2")  
  
set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)  
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)  
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)  
  
  
# set ffmpeg develop environment  
include_directories(${FFMPEG_DEV_ROOT}/include)  # 头文件
link_directories(${FFMPEG_DEV_ROOT}/lib)         # 链接动态链接库
link_libraries(  
        avcodec  
        avformat        avfilter        avdevice        swresample        swscale        avutil)  
  
# copy dlls  
file(GLOB ffmpeg_shared_libries ${FFMPEG_DEV_ROOT}/bin/*dll)  
file(COPY ${ffmpeg_shared_libries} DESTINATION ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})

add_executable(ffmpeg_libav_test main.cpp)
```



### 解码视频

> 使用的 ffmpeg 版本为 N-109449-gb92260f70a-20221224

下面的代码需要的头文件如下

```c
#include "libavcodec/avcodec.h"
#include "libavformat/avformat.h"
#include "libavutil/imgutils.h"
#include "libswscale/swscale.h"
```

首先先声明一个 Format I/O上下文，然后使用 `avformat_open_input` 打开视频

```c
AVFormatContext* fc = NULL;
const char* path = "record.mp4";
// 后两个参数可以为 NULL
int ret = avformat_open_input(&fc, path, NULL, NULL);
if (ret)
{
	printf("open error");
	return;
}
```

然后从媒体文件中读取一部分获取流信息

```c
ret = avformat_find_stream_info(fc, NULL);
if ret)
{
	printf("find stream info error");
}
int time = fc->duration;   // 返回时间单位为 us
int minute = (time / 1000000) / 60;
int second = (time / 1000000) % 60;

av_dump_format(fc, NULL,  path, 0);  // 打印流信息
```

然后从Format I/O上下文中找到视频流的索引（一个媒体文件可能包括视频流，音频流和字幕流等）

```c
int VideoStream = -1, AudioStream = -1; // 寻找流
VideoStream = av_find_best_stream(fc, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, -1);
AudioStream = av_find_best_stream(fc, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, -1);
```

通过索引定位视频流，然后找到解码器和 codec 上下文，并且打开 codec 上下文

```c
AVCodec* codec = avcodec_find_decoder(fc->streams[VideoStream]->codecpar->codec_id);
AVCodecContext* cc = avcodec_alloc_context3(codec);
// 将解码参数复制到解码上下文中
avcodec_parameters_to_context(cc, fc->streams[VideoStream]->codecpar);
ret = avcodec_open2(cc, codec, NULL);
if (ret)
{
	printf("codec open error");
}
printf("codec open success");
```

解码视频是一帧一帧解码

```c
AVFrame* frame = av_frame_alloc();  // 储存原始帧
AVFrame* frameYUV = av_frame_alloc(); // 储存转换后的帧
int width = fc->streams[VideoStream]->codecpar->width;   // 获取视频宽度
int height = fc->streams[VideoStream]->codecpar->height; // 获取视频高度
int fmt = fc->streams[VideoStream]->codecpar->format;	 // 获取视频格式
int size = av_image_get_buffer_size(AV_PIX_FMT_YUV420P, width, height, 1); // 获取一帧数据大小
```

`av_image_fill_arrays` 为目标帧 `frameYUV` 填充一帧空间

```c
uint8_t* buff = NULL;
buff = (uint8_t*)av_malloc(size);

// 一帧数据
av_image_fill_arrays(frameYUV->data, frameYUV->linesize, buff, AV_PIX_FMT_YUV420P, width, height, 1);
```

声明转换上下文和数据包

```c
AVPacket* packet = (AVPacket*)av_malloc((sizeof(AVPacket)));

// 转换上下文
struct SwsContext* swsCtx = NULL;
swsCtx = sws_getContext(width, height, fmt, width, height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL);
```

现在开始转换

```c
while (av_read_frame(fc, packet) >= 0)
{
// 视频流才处理
	if (packet->stream_index == AVMEDIA_TYPE_VIDEO)
	{
		// 解码视频
		avcodec_send_packet(cc, packet);
		avcodec_receive_frame(cc, frame);
		
		// 转换视频
		sws_scale(swsCtx, frame->data, frame->linesize, 0, height, frameYUV->data, frameYUV->linesize);
		
		// 这里保存数据流
		fwrite(frameYUV->data[0], 1, width * height, fp1);
		fwrite(frameYUV->data[1], 1, width * height / 4, fp2);
		fwrite(frameYUV->data[2], 1, width * height / 4, fp3);
	}
}
```

最后是收尾工作，释放申请的资源

```c
av_packet_free(packet);
av_frame_free(&frame);
av_frame_free(&frameYUV);
avcodec_close(cc);
sws_freeContext(swsCtx);
```


### 重采样音频

重采样音频与解码视频前半部分类似，重采样使用的是 libswresample，转换上下文和数据包的声明如下

```c
struct SwrContext* swr_ctx = swr_alloc();

/* set options */
av_opt_set_chlayout(swr_ctx, "in_chlayout", &src_ch_layout, 0);
av_opt_set_int(swr_ctx, "in_sample_rate", src_rate, 0);
av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", src_sample_fmt, 0);

av_opt_set_chlayout(swr_ctx, "out_chlayout", &dst_ch_layout, 0);
av_opt_set_int(swr_ctx, "out_sample_rate", dst_rate, 0);
av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", dst_sample_fmt, 0);

swr_init(swr_ctx);

AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket));
```

在重采样音频时，同样也是一帧一帧处理，首先定义源文件的一帧的大小，然后计算目标文件一帧的大小

```c
int src_nb_samples = 1024;

// 源文件的一帧
AVFrame* frame = av_frame_alloc();
frame->sample_rate = src_rate;
frame->ch_layout = src_ch_layout;
frame->format = src_sample_fmt;
frame->nb_samples = src_nb_samples;

av_frame_get_buffer(frame, 0);

int max_dst_nb_samples, dst_nb_samples;
max_dst_nb_samples = dst_nb_samples = av_rescale_rnd(src_nb_samples, dst_rate, src_rate, AV_ROUND_UP);
AVFrame* e_frame = av_frame_alloc();
e_frame->nb_samples = dst_nb_samples;
e_frame->format = e_cc->sample_fmt;
e_frame->sample_rate = dst_rate;
e_frame->ch_layout = dst_ch_layout;

av_frame_get_buffer(e_frame, 0);
```

现在开始重采样，注意可能需要重新分配源文件的帧大小

```c
 while (av_read_frame(fc, packet) >= 0)
 {
     //fprintf("package idx : %d \n", packet->stream_index);
     if (packet->stream_index == AudioStream)
     {
         avcodec_send_packet(cc, packet);
         avcodec_receive_frame(cc, frame);

         dst_nb_samples = av_rescale_rnd(swr_get_delay(swr_ctx, src_rate) + src_nb_samples, dst_rate, src_rate, AV_ROUND_UP);
         if (dst_nb_samples > max_dst_nb_samples) {
             e_frame->nb_samples = dst_nb_samples;
             av_freep(&e_frame->data[0]);
             av_frame_get_buffer(e_frame, 0);
             max_dst_nb_samples = dst_nb_samples;
         }
         av_frame_make_writable(e_frame);  // 不一定需要
         ret = swr_convert(swr_ctx, e_frame->data, dst_nb_samples, frame->data, src_nb_samples);
         if (ret < 0) {
             fprintf(stderr, "Error while converting\n");
             goto end;
         }
     }
 }
```

此时如果直接将 `e_frame->data` 会无法播放，因此需要对重采样后的数据进行编码。


### 编码音频

和解码时类似，定义编码上下文，找到编码器

```c
AVCodec* e_codec;
AVCodecContext* e_cc = NULL;
e_codec = avcodec_find_encoder(AV_CODEC_ID_PCM_S16LE);

e_cc = avcodec_alloc_context3(e_codec);
e_cc->bit_rate = 16000 * 16 * 2;  // bit rate = 采样率 * 比特位数 * channels
e_cc->sample_rate = dst_rate;
e_cc->sample_fmt = AV_SAMPLE_FMT_S16;
```

还需要定义编码帧和数据包

```c
AVFrame* e_frame = av_frame_alloc();
e_frame->nb_samples = dst_nb_samples;
e_frame->format = e_cc->sample_fmt;
e_frame->sample_rate = dst_rate;
av_channel_layout_copy(&e_frame->ch_layout, &e_cc->ch_layout);

av_frame_get_buffer(e_frame, 0);

AVPacket* e_pkt = av_packet_alloc();

```

开始编码

```c
while (av_read_frame(fc, packet) >= 0)
{
    //fprintf("package idx : %d \n", packet->stream_index);
    if (packet->stream_index == AudioStream)
    {
        // 一些处理
        encode(e_cc, e_frame, e_pkt, dst_file);
    }
}
encode(e_cc, NULL, e_pkt, dst_file);
```

encode 函数如下

```c
static void encode(AVCodecContext* ctx, AVFrame* frame, AVPacket* pkt,
    FILE* output)
{
    int ret;

    /* send the frame for encoding */
    ret = avcodec_send_frame(ctx, frame);
    // 错误处理
    /* read all the available output packets (in general there may be any
     * number of them */
    while (ret >= 0) {
        ret = avcodec_receive_packet(ctx, pkt);
        if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)
            return;
        else if (ret < 0) {
            fprintf(stderr, "Error encoding audio frame\n");
            exit(1);
        }
        fwrite(pkt->data, 1, pkt->size, output);
        av_packet_unref(pkt);  // 重置数据包
    }
}
```


### 缩放视频

基本思路和解码视频一致，只不过在 `sws_getContext` 中设置参数时额外设置目标宽度和高度

```c
int dst_w = 160, dst_h = 120;
enum AVPixelFormat src_pix_fmt = codecpar->format;
enum AVPixelFormat dst_pix_fmt = AV_PIX_FMT_RGB8;
swsCtx = sws_getContext(src_w, src_h, src_pix_fmt, dst_w, dst_h, dst_pix_fmt, SWS_BILINEAR, NULL, NULL, NULL);
```

得到的结果可以使用 ffplay 播放，命令如下

```sh
ffplay -f rawvideo -pixel_format rgb8 -video_size 160x120 out.raw
```


> [!NOTE]
> 注意 ffplay 的参数可能随着版本的变化有所改变



## ffmpeg android 编程

在 [Releases · arthenica/ffmpeg-kit (github.com)](https://github.com/arthenica/ffmpeg-kit/releases) 中下载需要的 aar 文件。

在 android studio 创建项目，设置 `gradle/wrapper/gradle-wrapper.properties` 中设置 

```properties
distributionUrl=https\://mirrors.cloud.tencent.com/gradle/gradle-8.7-bin.zip
```

此外设置 `build.gradle` 

```gradle
buildscript{
    repositories{
        maven { url 'https://maven.aliyun.com/repository/public/' }
        maven { url 'https://maven.aliyun.com/repository/google' }
        maven {
            url 'https://maven.aliyun.com/repository/central'
        }
        mavenLocal()
        mavenCentral()
        google()
        jcenter()
    }
}
```


在 `app` 文件夹中创建一个 `libs` 文件夹，将 aar 文件放在该文件夹中，在 `app/build.gradle` 中设置

```gradle
dependencies {
    implementation 'com.arthenica:smart-exception-java:0.2.1'
    implementation files("libs/ffmpeg-kit-min-gpl-6.0-2.aar")
	...
}
```

使用时参考命令行的使用方式，如下所示

```kotlin
import com.arthenica.ffmpegkit.FFmpegKit
import com.arthenica.ffmpegkit.ReturnCode

// ...
val session = FFmpegKit.execute("-version")
var output = ""
if (ReturnCode.isSuccess(session.returnCode)) {
	output = session.output
}
```




## ffmpeg 命令行

ffmpeg下载：[ffmpeg第一弹:ffmpeg介绍和开发环境搭建 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/336663258)

> 注意从[Releases · BtbN/FFmpeg-Builds (github.com)](https://github.com/BtbN/FFmpeg-Builds/releases)下载-shared.zip版本


1. ffmpeg 录制桌面： 

```powershell
ffmpeg -f gdigrab -i desktop -f mp4 d:/out.mp4
```
    
2. ffmpeg 剪辑视频

```powershell
ffmpeg -ss 00:00:10 -t 30 -i src.mp4 -codec copy cut.mp4
```

这条命令可以从源文件src.mp4中剪切出一个视频片段，并存储成out.mp4
    
+ -ss: 起始时间戳
+ -t: 持续时间，单位为秒
+ -i: 源文件名字，这里是src.mp4
+ -codec copy: 沿用原来的编码格式，cut.mp4为目标文件的文件名
    
3. ffmpeg 视频抽帧： 

```powershell
ffmpeg -i test.mp4 %05d.jpg
```
这条命令可以将test.mp4文件抽帧，抽出来的图片帧在当前目录下，名字为"帧号.jpg",不足5位则补0,如00001.jpg
    
4. ffmpeg提取视频中的音频： 

```powershell
ffmpeg -i test.mp4 -f mp3 -vn test.mp3
```

 + -i 表示input，即输入文件
 + -f 表示format，即输出格式
 + -vn表示vedio not，即输出不包含视频
    
5. ffmpeg给音频添加封面：

```powershell
ffmpeg -i 音乐.mp3 -i 封面.jpg -map 0:0 -map 1:0 -c copy -id3v2_version 3 -metadata:s:v title="Album cover" -metadata:s:v comment="Cover (Front)" 音乐.mp3
```

6. ffmpeg将图片序列转成视频

```powershell
ffmpeg -f image2 -framerate 2 -i %3d.png out.mp4
```
    
+ 若图像之间格式不一致，可先转成相同格式再操作，代码参见：[img2mp4byffmpeg.py](https://gist.github.com/Xiang-M-J/89d411e2df0dd23f77f3bba691e5e6a8)

7. 多个视频拼接

```powershell
ffmpeg -f concat -safe 0 -i filelist.txt -c copy out.mp4
```

filelist.txt的格式为："file\t"+"'"+father_path+"\"+file_path+"'\n"

```txt
file 'd:\001.mp4' file 'd:\002.mp4' file 'd:\003.mp4' file 'd:\004.mp4'
```


8. 视频按时长分段
    
```powershell
ffmpeg -i "input_video.mp4" -f segment -segment_time 3600 -vcodec copy -reset_timestamps 1 -map 0:0 -an output_video%d.mp4
```

时长由参数 -segment_time 指定，单位为 s。这个处理似乎没有声音了。


9. m3u8 （多个 ts 文件）转为 mp4

```sh
ffmpeg -i index.m3u8 -c copy out.mp4
```



# SDL

SDL 是一个跨平台的开发库，提供了通过 OpenGL 和 Direct3D 访问音频、视频、键盘等的底层访问。

SDL 的下载地址：https://github.com/libsdl-org/SDL/releases

## 开发环境搭建

### CMake

在 CMakeLists.txt 中添加

```cmake
set(SDL_DEV_ROOT "D:/SDL2-2.30.6")

include_directories(${SDL_DEV_ROOT}/include)  
link_directories(${SDL_DEV_ROOT}/lib/x64)  
link_libraries(SDL2)

file(COPY ${SDL_DEV_ROOT}/lib/x64/SDL2.dll DESTINATION ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})
```


## 使用示例

### 使用 SDL 和 FFmpeg 播放视频

使用 FFmpeg 解码视频的过程不必赘述，下面介绍 SDL2 的用法

引入的头文件为

```c
#include <SDL.h>  
#include <SDL_thread.h>
```

定义的变量

```c
SDL_Texture *texture = NULL;  
SDL_Window *screen = NULL;  
SDL_Renderer *renderer;  
SDL_Event event;
```

首先先初始化 SDL

```c
if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {  
    fprintf(stderr, "Could not initialize SDL - %s\n", SDL_GetError());  
    exit(1);  
}
```

创建一个窗口

```c
screen = SDL_CreateWindow("Player", SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, pCodecCtx->width, pCodecCtx->height, SDL_WINDOW_FULLSCREEN | SDL_WINDOW_OPENGL);  
```


> [!WARNING]
> 在调试时，设置 SDL_WINDOW_FULLSCREEN 时，SDL 窗口会覆盖全屏，可以换成 SDL_WINDOW_HIDDEN


创建渲染器

```c
renderer = SDL_CreateRenderer(screen, -1, 0);
```

创建纹理

```c
texture = SDL_CreateTexture(renderer, SDL_PIXELFORMAT_YV12, SDL_TEXTUREACCESS_STREAMING, pCodecCtx->width, pCodecCtx->height);

int uvPitch = pCodecCtx->width / 2;
```

在 av_read_frame 的循环中，每转换完一帧，就渲染一帧

```c
sws_scale  
(  
    sws_ctx,  
    pFrame->data,  
    pFrame->linesize,  
    0,  
    pCodecCtx->height,  
    cFrame->data,  
    cFrame->linesize  
);  
  
SDL_UpdateYUVTexture(texture, NULL, cFrame->data[0], pCodecCtx->width, cFrame->data[1], uvPitch, cFrame->data[2], uvPitch);  
SDL_RenderClear(renderer);  
SDL_RenderCopy(renderer, texture, NULL, NULL);  
SDL_RenderPresent(renderer);
```

在循环中，还可以监听事件

```c
SDL_PollEvent(&event);  
switch (event.type) {  
    case SDL_QUIT:  
        SDL_DestroyTexture(texture);  
	    SDL_DestroyRenderer(renderer);  
	    SDL_DestroyWindow(screen);  
	    SDL_Quit();  
	    exit(0);  
    default:  
        break;  
}
```



# OpenCV


## OpenCV 编译


### CPU

直接从 [Releases · opencv/opencv (github.com)](https://github.com/opencv/opencv/releases) 下载文件即可



### GPU

从 [opencv](https://github.com/opencv/opencv) 和 [opencv_contrib](https://github.com/opencv/opencv_contrib) 下载源码，使用 cmake 配置opencv。首先添加 `opencv_contrib`模块的引用，在`OPENCV_EXTRA_MODULES_PATH`条目中添加该模块的路径，然后选择`OPENCV_ENABLE_NONFREE`。然后选择 `WITH_CUDA`。，选择`OPENCV_DNN_CUDA`，此处还可以选择`OPENCV_DNN_OPENVINO`等不同的模型部署，最后选择`ENABLE_FAST_MATH`。为了让生成的依赖库文件都集成在一个文件中，还可以选择`BUILD_opencv_world`。

Configure 时可能会遇到一些文件下载超时的问题，可以考虑手动下载，或者多试几次。生成项目后，用 Visual Studio 打开项目，编译 CMakeTargets 的 ALL_BUILD，然后再 build INSTALL（需要在Debug和Release模式下都处理一下）。编译完成后，在编译文件夹的 install 文件夹中能找到需要的文件。




## 开发环境搭建


### 在 visual studio 中的使用

在系统变量中添加 `OPENCV_DIR` --> `D:\opencv\opencv\build\x64\vc15`

在`Path`中添加 `%OPENCV_DIR%\bin`

在visual studio中添加项目配置

1. 在C/C++ `附加包含目录`中添加 `D:\opencv\opencv\build\include`
2. 在链接器`常规` ，`附加库目录`中添加 `$(OPENCV_DIR)\lib`
3. 在链接器`输入` ，`附加依赖项`中添加 `opencv_world454d.lib`

**注意事项**

1. 注意项目为×64位
2. 如果为release，需要添加 `opencv_world454.lib` ， debug模式添加 `opencv_world454d.lib`
3. 附加依赖项的添加需要注意opencv_world后面的版本号


### 在 CMake 中使用

```cmake
set(OpenCV_DIR D:/opencv/install) # xxxx目录包含OpenCVConfig.cmake
find_package(OpenCV REQUIRED)	# 找到opencv库
include_directories(${OpenCV_INCLUDE_DIRS})

add_executable(${PROJECT_NAME} *.cpp)	# *.cpp指要编译的那些源文件
target_link_libraries(${PROJECT_NAME} ${OpenCV_LIBRARIES})
```

>注意需要手动复制依赖的链接库。

# WebRTC

一个实时通信（音视频、文件传输、屏幕共享等）的基于浏览器的 API，WebRTC 可以直接使用 js 编写，通过 html 可视化效果。


## 基本知识

### 协议介绍


#### ICE

[交互式连接创建](https://zh.wikipedia.org/wiki/%E4%BA%92%E5%8B%95%E5%BC%8F%E9%80%A3%E6%8E%A5%E5%BB%BA%E7%AB%8B)（Interactive Connectivity Establishment，ICE）是一个允许浏览器和其它浏览器建立连接的协议框架。在实际的网络当中，有很多原因导致 A 端到 B 端不能简单直连，需要绕过阻止建立连接的防火墙，给设备分配一个唯一可见的地址（通常情况下大部分设备没有一个固定的公网地址），如果路由器不允许主机直连，还得通过一台服务器转发数据。ICE 通过使用 STUN 或者 TURN 等技术完成上述工作。

#### STUN

NAT （网络地址转换，将私有 ip 地址转换为公网地址）的会话穿越功能[Session Traversal Utilities for **NAT** (STU**N**)](http://en.wikipedia.org/wiki/STUN) 是一个允许位于 NAT 后的客户端找出自己的公网地址，判断出路由器阻止直连的限制方法的协议。客户端通过给公网的 STUN 服务器发送请求获得自己的公网地址信息，以及是否能够被（NAT 路由器）访问。

![](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Protocols/webrtc-stun.png)

#### TURN

一些路由器使用一种“对称型 NAT”的 NAT 模型。这意味着路由器只接受和对端先前建立的连接（就是下一次请求建立新的连接映射）。NAT 的中继穿越方式[Traversal Using Relays around NAT (TURN)](http://en.wikipedia.org/wiki/TURN) 通过 TURN 服务器中继所有数据的方式来绕过“对称型 NAT”。需要在 TURN 服务器上创建一个连接，然后告诉所有对端设备发包到服务器上，TURN 服务器再把包转发给你。这种方式开销很大，只有在没得选择的情况下采用。

![](https://developer.mozilla.org/zh-CN/docs/Web/API/WebRTC_API/Protocols/webrtc-turn.png)


#### SDP

会话描述协议[Session Description Protocol (SDP)](http://en.wikipedia.org/wiki/Session_Description_Protocol) 是一个描述多媒体连接内容的协议，例如分辨率，格式，编码，加密算法等。所以在数据传输时两端都能够理解彼此的数据。本质上，这些描述内容的元数据并不是媒体流本身。从技术上讲，SDP 并不是一个真正的协议，而是一种数据格式，用于描述在设备之间共享媒体的连接。

SDP 由一行或多行 UTF-8 文本组成，每行以一个字符的类型开头，后跟等号（“ =”），然后是包含值或描述的结构化文本，其格式取决于类型。以给定字母开头的文本行通常称为“字母行”。例如，提供媒体描述的行的类型为“m”，因此这些行称为“m 行”。



## 编程语言实现


### golang

[pion/webrtc: Pure Go implementation of the WebRTC API (github.com)](https://github.com/pion/webrtc)


### Rust

[webrtc-rs/webrtc: A pure Rust implementation of WebRTC (github.com)](https://github.com/webrtc-rs/webrtc)


### Flutter/C++

[flutter-webrtc/flutter-webrtc: WebRTC plugin for Flutter Mobile/Desktop/Web (github.com)](https://github.com/flutter-webrtc/flutter-webrtc)


### React/Java

[react-native-webrtc/react-native-webrtc: The WebRTC module for React Native (github.com)](https://github.com/react-native-webrtc/react-native-webrtc)


## 基本教程

### 利用 WebRTC 实现实时通信

https://developers.google.cn/codelabs/webrtc-web
#### 通过网络摄像头流式传输视频

创建一个 html 文件，加入以下内容

```html
<!DOCTYPE html>  
<html lang="en">  
  
<head>  
  <title>Realtime communication with WebRTC</title>  
  <link rel="stylesheet" href="css/main.css" />  
</head>  
  
<body>  
  <h1>Realtime communication with WebRTC</h1>  
  <!-- 没有 autoplay 会导致只显示一帧 -->
  <video autoplay playsinline></video>  
  <script src="js/main.js"></script>  
</body>  
  
</html>
```


添加 js 获取媒体流

```js
'use strict';  
  
const mediaStreamConstraints = {  
    video: true,  
    // audio: true   // 只演示 video
}  
  
// 获取 dom 元素  
const localVideo = document.querySelector("video")  
let localStream;  
  
// 获取本地媒体流  
function gotLocalMediaStream(mediaStream) {  
    localStream = mediaStream;  
    localVideo.srcObject = mediaStream;  
}  
  
// 处理错误  
function handleLocalMediaStreamError(error) {  
    console.log("navigator.getUserMedia error: ", error)  
}  
  
// 初始化媒体流  
navigator.mediaDevices.getUserMedia(mediaStreamConstraints).then(  
    gotLocalMediaStream  
).catch(handleLocalMediaStreamError)
```


在 `getUserMedia()` 调用之后，浏览器会请求摄像头使用权限。如果成功，则返回 [`MediaStream`](https://developer.mozilla.org/en/docs/Web/API/MediaStream)，`media` 元素可通过 `srcObject` 属性使用 MediaStream，`constraints` 参数用于指定要获取的媒体，还可以针对其他要求（例如视频分辨率）使用限制条件：

```js
const hdConstraints = {  
    video: {  
        width: {  
            min: 1280  
        },  
        height: {  
            min: 720  
        }  
    }  
}
```

[`MediaTrackConstraints` 规范](https://w3c.github.io/mediacapture-main/getusermedia.html#media-track-constraints)列出了所有可能的限制条件类型，但并非所有浏览器都支持全部选项。

如可以为视频加上 css 过滤器（在 css 文件中添加）：

```css
video {  
  max-width: 100%;  
  width: 640px;  
  filter: blur(4px) invert(1) opacity(0.5);  
}
```

#### 通过 RTCPeerConnection API 流式传输视频


在原来的 `index.html` 文件中，将单个 `video` 元素替换为两个 `video` 元素和三个 `button` 元素：

```html
<video id="localVideo" autoplay playsinline></video>  
<video id="remoteVideo" autoplay playsinline></video>  
  
<div>  
  <button id="startButton">Start</button>  
  <button id="callButton">Call</button>  
  <button id="hangupButton">Hang Up</button>  
</div>
```

一个视频元素显示来自 `getUserMedia()` 的数据流，另一个显示通过 `RTCPeerconnection` 流式传输的同一视频。

引入 [adapter.js](https://webrtc.github.io/adapter/adapter-latest.js)，即在 html 中添加

```html
<!-- 新添加的一行 -->
<script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>  
<script src="js/main.js"></script>
```

将 js 文件替换为

```js
'use strict';  
  
const mediaStreamConstraints = {  
    video: {  
        width: {  
            min: 1280  
        },  
        height: {  
            min: 720  
        },  
    },  
    // audio: true   // 只演示 video}  
  
// 只交换 videoconst offerOptions = {  
    offerToReceiveVideo: 1,  
};  
  
let startTime = null;  
  
// 获取 dom 元素  
const localVideo = document.getElementById('localVideo');  
const remoteVideo = document.getElementById('remoteVideo');  
  
let localStream;  
let remoteStream;  
  
let localPeerConnection;  
let remotePeerConnection;  
  
function trace(text) {  
    text = text.trim();  
    const now = (window.performance.now() / 1000).toFixed(3);  
    console.log(now, text);  
}  
  
// 获取本地媒体流  
function gotLocalMediaStream(mediaStream) {  
    localStream = mediaStream;  
    localVideo.srcObject = mediaStream;  
    trace("Received local stream.");  
    callButton.disabled = false;  
}  
  
// 获取远程媒体流  
function gotRemoteMediaStream(event) {  
    const mediaStream = event.stream;  
    remoteVideo.srcObject = mediaStream;  
    remoteStream = mediaStream;  
    trace('Remote peer connection received remote stream.');  
}  
  
// 处理错误  
function handleLocalMediaStreamError(error) {  
    console.log("navigator.getUserMedia error: ", error);  
}  
  
// 视频加载时打印视频信息  
function logVideoLoaded(event) {  
    const video = event.target;  
    trace(`${video.id} videoWidth: ${video.videoWidth}px, ` +  
        `videoHeight: ${video.videoHeight}px.`);  
}  
  
// 视频 resize 打印视频信息  
function logResizedVideo(event) {  
    logVideoLoaded(event);  
  
    if (startTime) {  
        const elapsedTime = window.performance.now() - startTime;  
        startTime = null;  
        trace(`Setup time: ${elapsedTime.toFixed(3)}ms.`);  
    }  
}  
  
localVideo.addEventListener("loadedmetadata", logVideoLoaded);  
remoteVideo.addEventListener("loadedmetadata", logVideoLoaded);  
remoteVideo.addEventListener("onresize", logResizedVideo);  
  
// 处理连接  
function handleConnection(event) {  
    const peerConnection = event.target;  
    const iceCandidate = event.candidate;  
    if (iceCandidate){  
        const newIceCandidate = new RTCIceCandidate(iceCandidate);  
        const otherPeer = getOtherPeer(peerConnection);  
  
        otherPeer.addIceCandidate(newIceCandidate).then(  
            () => {  
                handleConnectionSuccess(peerConnection);  
            }  
        ).catch((error) => {  
            handleConnectionFailure(peerConnection, error);  
        });  
        trace(`${getPeerName(peerConnection)} ICE candidate:\n` +  
            `${event.candidate.candidate}.`);  
    }  
}  
  
function handleConnectionSuccess(peerConnection) {  
    trace(`${getPeerName(peerConnection)} addIceCandidate success.`);  
}  
  
// Logs that the connection failed.  
function handleConnectionFailure(peerConnection, error) {  
    trace(`${getPeerName(peerConnection)} failed to add ICE Candidate:\n`+  
        `${error.toString()}.`);  
}  
  
function handleConnectionChange(event) {  
    const peerConnection = event.target;  
    console.log('ICE state change event: ', event);  
    trace(`${getPeerName(peerConnection)} ICE state: ` +  
        `${peerConnection.iceConnectionState}.`);  
}  
  
function setSessionDescriptionError(error) {  
    trace(`Failed to create session description: ${error.toString()}.`);  
}  
function setDescriptionSuccess(peerConnection, functionName) {  
    const peerName = getPeerName(peerConnection);  
    trace(`${peerName} ${functionName} complete.`);  
}  
function setLocalDescriptionSuccess(peerConnection) {  
    setDescriptionSuccess(peerConnection, 'setLocalDescription');  
}  
function setRemoteDescriptionSuccess(peerConnection) {  
    setDescriptionSuccess(peerConnection, 'setRemoteDescription');  
}  
  
function createdOffer(description) {  
    trace(`Offer from localPeerConnection:\n${description.sdp}`);  
  
    trace('localPeerConnection setLocalDescription start.');  
    localPeerConnection.setLocalDescription(description)  
        .then(() => {  
            setLocalDescriptionSuccess(localPeerConnection);  
        }).catch(setSessionDescriptionError);  
  
    trace('remotePeerConnection setRemoteDescription start.');  
    remotePeerConnection.setRemoteDescription(description)  
        .then(() => {  
            setRemoteDescriptionSuccess(remotePeerConnection);  
        }).catch(setSessionDescriptionError);  
  
    trace('remotePeerConnection createAnswer start.');  
    remotePeerConnection.createAnswer()  
        .then(createdAnswer)  
        .catch(setSessionDescriptionError);  
}  
  
// Logs answer to offer creation and sets peer connection session descriptions.  
function createdAnswer(description) {  
    trace(`Answer from remotePeerConnection:\n${description.sdp}.`);  
  
    trace('remotePeerConnection setLocalDescription start.');  
    remotePeerConnection.setLocalDescription(description)  
        .then(() => {  
            setLocalDescriptionSuccess(remotePeerConnection);  
        }).catch(setSessionDescriptionError);  
  
    trace('localPeerConnection setRemoteDescription start.');  
    localPeerConnection.setRemoteDescription(description)  
        .then(() => {  
            setRemoteDescriptionSuccess(localPeerConnection);  
        }).catch(setSessionDescriptionError);  
}  
  
  
// Define and add behavior to buttons.  
  
// Define action buttons.  
const startButton = document.getElementById('startButton');  
const callButton = document.getElementById('callButton');  
const hangupButton = document.getElementById('hangupButton');  
  
// Set up initial action buttons status: disable call and hangup.  
callButton.disabled = true;  
hangupButton.disabled = true;  
  
  
// Handles start button action: creates local MediaStream.  
function startAction() {  
    startButton.disabled = true;  
    navigator.mediaDevices.getUserMedia(mediaStreamConstraints)  
        .then(gotLocalMediaStream).catch(handleLocalMediaStreamError);  
    trace('Requesting local stream.');  
}  
  
// Handles call button action: creates peer connection.  
function callAction() {  
    callButton.disabled = true;  
    hangupButton.disabled = false;  
  
    trace('Starting call.');  
    startTime = window.performance.now();  
  
    // Get local media stream tracks.  
    const videoTracks = localStream.getVideoTracks();  
    const audioTracks = localStream.getAudioTracks();  
    if (videoTracks.length > 0) {  
        trace(`Using video device: ${videoTracks[0].label}.`);  
    }  
    if (audioTracks.length > 0) {  
        trace(`Using audio device: ${audioTracks[0].label}.`);  
    }  
  
    const servers = null;  // Allows for RTC server configuration.  
  
    // Create peer connections and add behavior.    localPeerConnection = new RTCPeerConnection(servers);  
    trace('Created local peer connection object localPeerConnection.');  
  
    localPeerConnection.addEventListener('icecandidate', handleConnection);  
    localPeerConnection.addEventListener(  
        'iceconnectionstatechange', handleConnectionChange);  
  
    remotePeerConnection = new RTCPeerConnection(servers);  
    trace('Created remote peer connection object remotePeerConnection.');  
  
    remotePeerConnection.addEventListener('icecandidate', handleConnection);  
    remotePeerConnection.addEventListener(  
        'iceconnectionstatechange', handleConnectionChange);  
    remotePeerConnection.addEventListener('addstream', gotRemoteMediaStream);  
  
    // Add local stream to connection and create offer to connect.  
    localPeerConnection.addStream(localStream);  
    trace('Added local stream to localPeerConnection.');  
  
    trace('localPeerConnection createOffer start.');  
    localPeerConnection.createOffer(offerOptions)  
        .then(createdOffer).catch(setSessionDescriptionError);  
}  
  
// Handles hangup action: ends up call, closes connections and resets peers.  
function hangupAction() {  
    localPeerConnection.close();  
    remotePeerConnection.close();  
    localPeerConnection = null;  
    remotePeerConnection = null;  
    hangupButton.disabled = true;  
    callButton.disabled = false;  
    trace('Ending call.');  
}  
  
// Add click event handlers for buttons.  
startButton.addEventListener('click', startAction);  
callButton.addEventListener('click', callAction);  
hangupButton.addEventListener('click', hangupAction);  
  
  
// Define helper functions.  
  
// Gets the "other" peer connection.  
function getOtherPeer(peerConnection) {  
    return (peerConnection === localPeerConnection) ?  
        remotePeerConnection : localPeerConnection;  
}  
  
// Gets the name of a certain peer connection.  
function getPeerName(peerConnection) {  
    return (peerConnection === localPeerConnection) ?  
        'localPeerConnection' : 'remotePeerConnection';  
}
```


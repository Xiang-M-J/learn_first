
# FFmpeg


## 使用 Visual studio 进行编译

参考链接：[CompilationGuide/MSVC – FFmpeg](https://trac.ffmpeg.org/wiki/CompilationGuide/MSVC)
[Compiling FFmpeg with X264 on Windows 10 using MSVC | ROXLU](https://www.roxlu.com/2019/062/compiling-ffmpeg-with-x264-on-windows-10-using-msvc)


首先需要下载 [MSYS2](https://www.msys2.org/) 和 [Yasm](https://yasm.tortall.net/Download.html)，并且为它们配置环境变量，将 yasm 的可执行文件命名为 yasm.exe。对于 msys2，修改目录下的 msys2_shell.cmd 中的

```bat
rem set MSYS2_PATH_TYPE=inherit
```

为

```bat
set MSYS2_PATH_TYPE=inherit
```


打开visual studio 自带的cmd，`Tools` -> `Command Line` -> `Developer Command Prompt`。为了编译 64 位，需要执行下面的命令来启用 64 位编译器（ vcvarsall.bat 的位置根据安装位置改变，建议直接使用 everything 进行搜索）

```cmd
D:\VisualStudio\Community\VC\Auxiliary\Build\vcvarsall.bat amd64
```

打开 msys2 中的 msys2_shell.cmd，在弹出的窗口进行如下操作
。。。


## 在 Visual studio 中使用 ffmpeg

注意从[Releases · BtbN/FFmpeg-Builds (github.com)](https://github.com/BtbN/FFmpeg-Builds/releases)下载-shared.zip版本，下面是解压后的文件夹

├─bin
├─doc
├─include
└─lib

先设置项目，在 `c/c++` -> `general` -> `Additional Include Directories` 中添加头文件目录 include，同时设置 `SDL checks` 为 No。再在 `Linker` -> `General` -> `Additional Library Directories` 添加 lib 目录，最后在执行时，需要将 bin 文件夹的 dll 文件拷贝至执行文件路径中。

下面是一个简单示例

```c
#include<stdio.h>

// 声明链接库，否则会报错
#pragma comment (lib, "avcodec.lib")
#pragma comment (lib, "avdevice.lib")
#pragma comment (lib, "avfilter.lib")
#pragma comment (lib, "avformat.lib")
#pragma comment (lib, "avutil.lib")
#pragma comment (lib, "swresample.lib")
#pragma comment (lib, "swscale.lib")

#include "libavcodec/avcodec.h"
#include "libavformat/avformat.h"

void main() {
	printf("%s\n", avcodec_configuration());
}
```

注意如果不使用 `#pragma` 声明链接库，需要在 `Linker` -> `Input` -> `Additional Dependencies` 中添加

```
avcodec.lib
avdevice.lib
avfilter.lib
avformat.lib
avutil.lib
postproc.lib
swscale.lib
swresample.lib
```




## ffmpeg使用手册

0. ffmpeg下载：[ffmpeg第一弹:ffmpeg介绍和开发环境搭建 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/336663258)
    
    注意从[Releases · BtbN/FFmpeg-Builds (github.com)](https://github.com/BtbN/FFmpeg-Builds/releases)下载-shared.zip版本
    
1. ffmpeg 录制桌面： 

```powershell
ffmpeg -f gdigrab -i desktop -f mp4 d:/out.mp4
```
    
2. ffmpeg 剪辑视频

```powershell
ffmpeg -ss 00:00:10 -t 30 -i src.mp4 -codec copy cut.mp4
```

这条命令可以从源文件src.mp4中剪切出一个视频片段，并存储成out.mp4
    
+ -ss: 起始时间戳
+ -t: 持续时间，单位为秒
+ -i: 源文件名字，这里是src.mp4
+ -codec copy: 沿用原来的编码格式，cut.mp4为目标文件的文件名
    
3. ffmpeg 视频抽帧： 

```powershell
ffmpeg -i test.mp4 %05d.jpg
```
这条命令可以将test.mp4文件抽帧，抽出来的图片帧在当前目录下，名字为"帧号.jpg",不足5位则补0,如00001.jpg
    
4. ffmpeg提取视频中的音频： 

```powershell
ffmpeg -i test.mp4 -f mp3 -vn test.mp3
```

 + -i 表示input，即输入文件
 + -f 表示format，即输出格式
 + -vn表示vedio not，即输出不包含视频
    
5. ffmpeg给音频添加封面：

```powershell
ffmpeg -i 音乐.mp3 -i 封面.jpg -map 0:0 -map 1:0 -c copy -id3v2_version 3 -metadata:s:v title="Album cover" -metadata:s:v comment="Cover (Front)" 音乐.mp3
```

6. ffmpeg将图片序列转成视频

```powershell
ffmpeg -f image2 -framerate 2 -i %3d.png out.mp4
```
    
+ 若图像之间格式不一致，可先转成相同格式再操作，代码参见：[img2mp4byffmpeg.py](https://gist.github.com/Xiang-M-J/89d411e2df0dd23f77f3bba691e5e6a8)

7. 多个视频拼接

```powershell
ffmpeg -f concat -safe 0 -i filelist.txt -c copy out.mp4
```

filelist.txt的格式为："file\t"+"'"+father_path+"\"+file_path+"'\n"

```txt
file 'd:\001.mp4' file 'd:\002.mp4' file 'd:\003.mp4' file 'd:\004.mp4'
```


8. 视频按时长分段
    
```powershell
ffmpeg -i "input_video.mp4" -f segment -segment_time 3600 -vcodec copy -reset_timestamps 1 -map 0:0 -an output_video%d.mp4
```

时长由参数 -segment_time 指定，单位为 s。这个处理似乎没有声音了。

9. m3u8 （多个 ts 文件）转为 mp4

```sh
ffmpeg -i index.m3u8 -c copy out.mp4
```



# WebRTC

一个实时通信（音视频、文件传输、屏幕共享等）的基于浏览器的 API，一般的 WebRTC 直接使用 js 编写，通过 html 可视化。


## 其它实现


### golang

[pion/webrtc: Pure Go implementation of the WebRTC API (github.com)](https://github.com/pion/webrtc)


### Rust

[webrtc-rs/webrtc: A pure Rust implementation of WebRTC (github.com)](https://github.com/webrtc-rs/webrtc)


### Flutter/C++

[flutter-webrtc/flutter-webrtc: WebRTC plugin for Flutter Mobile/Desktop/Web (github.com)](https://github.com/flutter-webrtc/flutter-webrtc)


### React/Java

[react-native-webrtc/react-native-webrtc: The WebRTC module for React Native (github.com)](https://github.com/react-native-webrtc/react-native-webrtc)


## 基本教程

### 利用 WebRTC 实现实时通信

https://developers.google.cn/codelabs/webrtc-web
#### 通过网络摄像头流式传输视频

创建一个 html 文件，加入以下内容

```html
<!DOCTYPE html>  
<html lang="en">  
  
<head>  
  <title>Realtime communication with WebRTC</title>  
  <link rel="stylesheet" href="css/main.css" />  
</head>  
  
<body>  
  <h1>Realtime communication with WebRTC</h1>  
  <!-- 没有 autoplay 会导致只显示一帧 -->
  <video autoplay playsinline></video>  
  <script src="js/main.js"></script>  
</body>  
  
</html>
```


添加 js 获取媒体流

```js
'use strict';  
  
const mediaStreamConstraints = {  
    video: true,  
    // audio: true   // 只演示 video}  
  
// 获取 dom 元素  
const localVideo = document.querySelector("video")  
let localStream;  
  
// 获取本地媒体流  
function gotLocalMediaStream(mediaStream) {  
    localStream = mediaStream;  
    localVideo.srcObject = mediaStream;  
}  
  
// 处理错误  
function handleLocalMediaStreamError(error) {  
    console.log("navigator.getUserMedia error: ", error)  
}  
  
// 初始化媒体流  
navigator.mediaDevices.getUserMedia(mediaStreamConstraints).then(  
    gotLocalMediaStream  
).catch(handleLocalMediaStreamError)
```


在 `getUserMedia()` 调用之后，浏览器会请求摄像头使用权限。如果成功，则返回 [`MediaStream`](https://developer.mozilla.org/en/docs/Web/API/MediaStream)，`media` 元素可通过 `srcObject` 属性使用 MediaStream，`constraints` 参数用于指定要获取的媒体，还可以针对其他要求（例如视频分辨率）使用限制条件：

```js
const hdConstraints = {  
    video: {  
        width: {  
            min: 1280  
        },  
        height: {  
            min: 720  
        }  
    }  
}
```

[`MediaTrackConstraints` 规范](https://w3c.github.io/mediacapture-main/getusermedia.html#media-track-constraints)列出了所有可能的限制条件类型，但并非所有浏览器都支持全部选项。

如可以为视频加上 css 过滤器（在样式文件中添加）：

```css
video {  
  max-width: 100%;  
  width: 640px;  
  filter: blur(4px) invert(1) opacity(0.5);  
}
```

#### 通过 RTCPeerConnection API 流式传输视频


在原来的 `index.html` 文件中，将单个 `video` 元素替换为两个 `video` 元素和三个 `button` 元素：

```html
<video id="localVideo" autoplay playsinline></video>  
<video id="remoteVideo" autoplay playsinline></video>  
  
<div>  
  <button id="startButton">Start</button>  
  <button id="callButton">Call</button>  
  <button id="hangupButton">Hang Up</button>  
</div>
```

一个视频元素显示来自 `getUserMedia()` 的数据流，另一个显示通过 `RTCPeerconnection` 流式传输的同一视频。

引入 [adapter.js](https://webrtc.github.io/adapter/adapter-latest.js)，即在 html 中添加

```html
<!-- 新添加的一行 -->
<script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>  
<script src="js/main.js"></script>
```

将 js 文件替换为

```js
'use strict';  
  
const mediaStreamConstraints = {  
    video: {  
        width: {  
            min: 1280  
        },  
        height: {  
            min: 720  
        },  
    },  
    // audio: true   // 只演示 video}  
  
// 只交换 videoconst offerOptions = {  
    offerToReceiveVideo: 1,  
};  
  
let startTime = null;  
  
// 获取 dom 元素  
const localVideo = document.getElementById('localVideo');  
const remoteVideo = document.getElementById('remoteVideo');  
  
let localStream;  
let remoteStream;  
  
let localPeerConnection;  
let remotePeerConnection;  
  
function trace(text) {  
    text = text.trim();  
    const now = (window.performance.now() / 1000).toFixed(3);  
    console.log(now, text);  
}  
  
// 获取本地媒体流  
function gotLocalMediaStream(mediaStream) {  
    localStream = mediaStream;  
    localVideo.srcObject = mediaStream;  
    trace("Received local stream.");  
    callButton.disabled = false;  
}  
  
// 获取远程媒体流  
function gotRemoteMediaStream(event) {  
    const mediaStream = event.stream;  
    remoteVideo.srcObject = mediaStream;  
    remoteStream = mediaStream;  
    trace('Remote peer connection received remote stream.');  
}  
  
// 处理错误  
function handleLocalMediaStreamError(error) {  
    console.log("navigator.getUserMedia error: ", error);  
}  
  
// 视频加载时打印视频信息  
function logVideoLoaded(event) {  
    const video = event.target;  
    trace(`${video.id} videoWidth: ${video.videoWidth}px, ` +  
        `videoHeight: ${video.videoHeight}px.`);  
}  
  
// 视频 resize 打印视频信息  
function logResizedVideo(event) {  
    logVideoLoaded(event);  
  
    if (startTime) {  
        const elapsedTime = window.performance.now() - startTime;  
        startTime = null;  
        trace(`Setup time: ${elapsedTime.toFixed(3)}ms.`);  
    }  
}  
  
localVideo.addEventListener("loadedmetadata", logVideoLoaded);  
remoteVideo.addEventListener("loadedmetadata", logVideoLoaded);  
remoteVideo.addEventListener("onresize", logResizedVideo);  
  
// 处理连接  
function handleConnection(event) {  
    const peerConnection = event.target;  
    const iceCandidate = event.candidate;  
    if (iceCandidate){  
        const newIceCandidate = new RTCIceCandidate(iceCandidate);  
        const otherPeer = getOtherPeer(peerConnection);  
  
        otherPeer.addIceCandidate(newIceCandidate).then(  
            () => {  
                handleConnectionSuccess(peerConnection);  
            }  
        ).catch((error) => {  
            handleConnectionFailure(peerConnection, error);  
        });  
        trace(`${getPeerName(peerConnection)} ICE candidate:\n` +  
            `${event.candidate.candidate}.`);  
    }  
}  
  
function handleConnectionSuccess(peerConnection) {  
    trace(`${getPeerName(peerConnection)} addIceCandidate success.`);  
}  
  
// Logs that the connection failed.  
function handleConnectionFailure(peerConnection, error) {  
    trace(`${getPeerName(peerConnection)} failed to add ICE Candidate:\n`+  
        `${error.toString()}.`);  
}  
  
function handleConnectionChange(event) {  
    const peerConnection = event.target;  
    console.log('ICE state change event: ', event);  
    trace(`${getPeerName(peerConnection)} ICE state: ` +  
        `${peerConnection.iceConnectionState}.`);  
}  
  
function setSessionDescriptionError(error) {  
    trace(`Failed to create session description: ${error.toString()}.`);  
}  
function setDescriptionSuccess(peerConnection, functionName) {  
    const peerName = getPeerName(peerConnection);  
    trace(`${peerName} ${functionName} complete.`);  
}  
function setLocalDescriptionSuccess(peerConnection) {  
    setDescriptionSuccess(peerConnection, 'setLocalDescription');  
}  
function setRemoteDescriptionSuccess(peerConnection) {  
    setDescriptionSuccess(peerConnection, 'setRemoteDescription');  
}  
  
function createdOffer(description) {  
    trace(`Offer from localPeerConnection:\n${description.sdp}`);  
  
    trace('localPeerConnection setLocalDescription start.');  
    localPeerConnection.setLocalDescription(description)  
        .then(() => {  
            setLocalDescriptionSuccess(localPeerConnection);  
        }).catch(setSessionDescriptionError);  
  
    trace('remotePeerConnection setRemoteDescription start.');  
    remotePeerConnection.setRemoteDescription(description)  
        .then(() => {  
            setRemoteDescriptionSuccess(remotePeerConnection);  
        }).catch(setSessionDescriptionError);  
  
    trace('remotePeerConnection createAnswer start.');  
    remotePeerConnection.createAnswer()  
        .then(createdAnswer)  
        .catch(setSessionDescriptionError);  
}  
  
// Logs answer to offer creation and sets peer connection session descriptions.  
function createdAnswer(description) {  
    trace(`Answer from remotePeerConnection:\n${description.sdp}.`);  
  
    trace('remotePeerConnection setLocalDescription start.');  
    remotePeerConnection.setLocalDescription(description)  
        .then(() => {  
            setLocalDescriptionSuccess(remotePeerConnection);  
        }).catch(setSessionDescriptionError);  
  
    trace('localPeerConnection setRemoteDescription start.');  
    localPeerConnection.setRemoteDescription(description)  
        .then(() => {  
            setRemoteDescriptionSuccess(localPeerConnection);  
        }).catch(setSessionDescriptionError);  
}  
  
  
// Define and add behavior to buttons.  
  
// Define action buttons.  
const startButton = document.getElementById('startButton');  
const callButton = document.getElementById('callButton');  
const hangupButton = document.getElementById('hangupButton');  
  
// Set up initial action buttons status: disable call and hangup.  
callButton.disabled = true;  
hangupButton.disabled = true;  
  
  
// Handles start button action: creates local MediaStream.  
function startAction() {  
    startButton.disabled = true;  
    navigator.mediaDevices.getUserMedia(mediaStreamConstraints)  
        .then(gotLocalMediaStream).catch(handleLocalMediaStreamError);  
    trace('Requesting local stream.');  
}  
  
// Handles call button action: creates peer connection.  
function callAction() {  
    callButton.disabled = true;  
    hangupButton.disabled = false;  
  
    trace('Starting call.');  
    startTime = window.performance.now();  
  
    // Get local media stream tracks.  
    const videoTracks = localStream.getVideoTracks();  
    const audioTracks = localStream.getAudioTracks();  
    if (videoTracks.length > 0) {  
        trace(`Using video device: ${videoTracks[0].label}.`);  
    }  
    if (audioTracks.length > 0) {  
        trace(`Using audio device: ${audioTracks[0].label}.`);  
    }  
  
    const servers = null;  // Allows for RTC server configuration.  
  
    // Create peer connections and add behavior.    localPeerConnection = new RTCPeerConnection(servers);  
    trace('Created local peer connection object localPeerConnection.');  
  
    localPeerConnection.addEventListener('icecandidate', handleConnection);  
    localPeerConnection.addEventListener(  
        'iceconnectionstatechange', handleConnectionChange);  
  
    remotePeerConnection = new RTCPeerConnection(servers);  
    trace('Created remote peer connection object remotePeerConnection.');  
  
    remotePeerConnection.addEventListener('icecandidate', handleConnection);  
    remotePeerConnection.addEventListener(  
        'iceconnectionstatechange', handleConnectionChange);  
    remotePeerConnection.addEventListener('addstream', gotRemoteMediaStream);  
  
    // Add local stream to connection and create offer to connect.  
    localPeerConnection.addStream(localStream);  
    trace('Added local stream to localPeerConnection.');  
  
    trace('localPeerConnection createOffer start.');  
    localPeerConnection.createOffer(offerOptions)  
        .then(createdOffer).catch(setSessionDescriptionError);  
}  
  
// Handles hangup action: ends up call, closes connections and resets peers.  
function hangupAction() {  
    localPeerConnection.close();  
    remotePeerConnection.close();  
    localPeerConnection = null;  
    remotePeerConnection = null;  
    hangupButton.disabled = true;  
    callButton.disabled = false;  
    trace('Ending call.');  
}  
  
// Add click event handlers for buttons.  
startButton.addEventListener('click', startAction);  
callButton.addEventListener('click', callAction);  
hangupButton.addEventListener('click', hangupAction);  
  
  
// Define helper functions.  
  
// Gets the "other" peer connection.  
function getOtherPeer(peerConnection) {  
    return (peerConnection === localPeerConnection) ?  
        remotePeerConnection : localPeerConnection;  
}  
  
// Gets the name of a certain peer connection.  
function getPeerName(peerConnection) {  
    return (peerConnection === localPeerConnection) ?  
        'localPeerConnection' : 'remotePeerConnection';  
}
```

